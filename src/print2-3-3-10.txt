Initialising fast UCB table... done
Main runs
Starting run 1 with 1 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 27 steps, with total reward -9.46337
Total reward = -8.9902
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -8.9902 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 27 [27, 27]
Total reward: -8.9902 [-8.9902, -8.9902]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -8.9902 (1)
a=7 : 0 (0)
E

# # # # 
# 0X* #
# 2X. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 9.025 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.025 [9.025, 9.025]
Policy after 1 simulations
MCTS Policy:
a=6 : 9.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 9.025 (1)
a=7 : 0 (0)
Check 2

# # # # 
# 0X* #
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 6 attempts
Expanding node: a=1 o=0 a=6 o=2 
Starting simulation

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=7 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 9.5 (1)
Check 3

# # # # 
# 0X* #
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=1 o=0 a=6 o=2 a=7 o=2 
Starting simulation

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X1X#
# 2X* #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=2 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
S

# # # # 
# 0X1X#
# 2X* #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=2 a=7 o=2 a=2 o=0 
Starting simulation

# # # # 
# 0X1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1$#
# 2X* #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# 0X1$#
# 2X* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# 2X* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=7 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 8.57375 (1)
Check 3

# # # # 
# 0X1X#
# 2X* #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=2 a=7 o=2 a=2 o=0 a=7 o=2 
Starting simulation

# # # # 
# 0$1$#
# 2X* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -1.43532
Total reward = -1.36355
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1.36355 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 10 [10, 10]
Total reward: -1.36355 [-1.36355, -1.36355]
Policy after 1 simulations
MCTS Policy:
a=0 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1.36355 (1)
a=7 : 0 (0)
Check 1

# # # # 
# 0X1X#
# 2X* #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=2 a=7 o=2 a=2 o=0 a=7 o=2 a=5 o=2 
Starting simulation

# # # # 
# 0X1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# 2X* #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# 0X1X#
# 2X* #
# # # # 
Reward 10
Terminated
Discounted return = 7.35092, average = 7.35092
Undiscounted return = 10, average = 10
Starting run 2 with 1 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 19 steps, with total reward -5.05286
Total reward = -4.80021
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -4.80021 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 19 [19, 19]
Total reward: -4.80021 [-4.80021, -4.80021]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -4.80021 (1)
a=7 : 0 (0)
Check 3

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=7 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 26 steps, with total reward -3.65254
Total reward = -3.46991
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -3.46991 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 26 [26, 26]
Total reward: -3.46991 [-3.46991, -3.46991]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -3.46991 (1)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# 0$* #
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 9 steps, with total reward -0.716715
Total reward = 9.31912
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 9.31912 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 9 [9, 9]
Total reward: 9.31912 [9.31912, 9.31912]
Policy after 1 simulations
MCTS Policy:
a=4 : 9.31912 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 9.31912 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Sample

# # # # 
# 0$* #
# 2$. #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : 7.35092 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=7 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : 7.35092 (1)
Check 3

# # # # 
# 0$* #
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=1 o=0 a=4 o=0 a=7 o=1 
Starting simulation

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=5 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Check 1

# # # # 
# 0$* #
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=1 o=0 a=4 o=0 a=7 o=1 a=5 o=1 
Starting simulation

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 20 steps, with total reward 16.5281
Total reward = 15.7017
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 15.7017 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 20 [20, 20]
Total reward: 15.7017 [15.7017, 15.7017]
Policy after 1 simulations
MCTS Policy:
a=2 : 15.7017 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 15.7017 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
S

# # # # 
# 0$. #
# 2$* #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=1 o=0 a=4 o=0 a=7 o=1 a=5 o=1 a=2 o=0 
Starting simulation

# # # # 
# 0X. #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 7.73781 [7.73781, 7.73781]
Policy after 1 simulations
MCTS Policy:
a=7 : 7.73781 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : 7.73781 (1)
Check 3

# # # # 
# 0$. #
# 2$* #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=1 o=0 a=4 o=0 a=7 o=1 a=5 o=1 a=2 o=0 a=7 o=1 
Starting simulation

# # # # 
# 0X. #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# 2$* #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
E

# # # # 
# 0$. #
# 2$* #
# # # # 
Reward 10
Terminated
Discounted return = -2.04163, average = 2.65465
Undiscounted return = 0, average = 5
Starting run 3 with 1 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 20 steps, with total reward 3.77354
Total reward = -6.41514
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -6.41514 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 20 [20, 20]
Total reward: -6.41514 [-6.41514, -6.41514]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -6.41514 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Check 1

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 12.6913
Total reward = 12.0568
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 12.0568 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 9 [9, 9]
Total reward: 12.0568 [12.0568, 12.0568]
Policy after 1 simulations
MCTS Policy:
a=6 : 12.0568 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 12.0568 (1)
a=7 : 0 (0)
Check 2

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward -11.5798
Total reward = -11.0008
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -11.0008 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 13 [13, 13]
Total reward: -11.0008 [-11.0008, -11.0008]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -11.0008 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Check 1

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=6 o=1 a=5 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 28 steps, with total reward -11.3978
Total reward = -10.8279
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -10.8279 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 28 [28, 28]
Total reward: -10.8279 [-10.8279, -10.8279]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -10.8279 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Sample

# # # # 
# * 1$#
# 2$. #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=6 o=1 a=5 o=2 a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 19 steps, with total reward 2.80307
Total reward = 2.66292
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 2.66292 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 19 [19, 19]
Total reward: 2.66292 [2.66292, 2.66292]
Policy after 1 simulations
MCTS Policy:
a=2 : 2.66292 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 2.66292 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
S

# # # # 
# . 1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=6 o=1 a=5 o=2 a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : 5.98737 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 10 [10, 10]
Total reward: 5.98737 [5.98737, 5.98737]
Policy after 1 simulations
MCTS Policy:
a=0 : 5.98737 (1)
Values after 1 simulations
MCTS Values:
a=0 : 5.98737 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
N

# # # # 
# * 1$#
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=6 o=1 a=5 o=2 a=4 o=0 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 14.4476
Total reward = 13.7252
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 13.7252 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 10 [10, 10]
Total reward: 13.7252 [13.7252, 13.7252]
Policy after 1 simulations
MCTS Policy:
a=6 : 13.7252 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 13.7252 (1)
a=7 : 0 (0)
Check 2

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=5 o=2 a=6 o=1 a=5 o=2 a=4 o=0 a=2 o=0 a=0 o=0 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 23 steps, with total reward 12.1423
Total reward = 11.5352
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.5352 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 23 [23, 23]
Total reward: 11.5352 [11.5352, 11.5352]
Policy after 1 simulations
MCTS Policy:
a=1 : 11.5352 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.5352 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# . * #
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=6 o=1 a=5 o=2 a=4 o=0 a=2 o=0 a=0 o=0 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# . * #
# 2$. #
# # # # 
Reward 10
Terminated
Discounted return = -1.93955, average = 1.12325
Undiscounted return = 0, average = 3.33333
Starting run 4 with 1 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -11.3635
Total reward = -10.7954
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -10.7954 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 11 [11, 11]
Total reward: -10.7954 [-10.7954, -10.7954]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -10.7954 (1)
a=7 : 0 (0)
Sample

# # # # 
# * 1$#
# 2$. #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : 0 (0)
Check 2

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 5 steps, with total reward 17.1701
Total reward = 16.3116
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 16.3116 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 16.3116 [16.3116, 16.3116]
Policy after 1 simulations
MCTS Policy:
a=6 : 16.3116 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 16.3116 (1)
a=7 : 0 (0)
Check 2

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 20.2804
Total reward = 19.2663
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 19.2663 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 15 [15, 15]
Total reward: 19.2663 [19.2663, 19.2663]
Policy after 1 simulations
MCTS Policy:
a=1 : 19.2663 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 19.2663 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# . * #
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 33 steps, with total reward 8.23961
Total reward = 7.82763
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.82763 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 33 [33, 33]
Total reward: 7.82763 [7.82763, 7.82763]
Policy after 1 simulations
MCTS Policy:
a=3 : 7.82763 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.82763 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
W

# # # # 
# * 1$#
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 14 steps, with total reward 11.1208
Total reward = 10.5648
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 10.5648 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 14 [14, 14]
Total reward: 10.5648 [10.5648, 10.5648]
Policy after 1 simulations
MCTS Policy:
a=2 : 10.5648 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 10.5648 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
S

# # # # 
# . 1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=3 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 18.5738 [18.5738, 18.5738]
Policy after 1 simulations
MCTS Policy:
a=4 : 18.5738 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Sample

# # # # 
# . 1$#
# * . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=3 o=0 a=2 o=0 a=4 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward -2.10662
Total reward = -2.00129
MCTS Values:
a=0 : -2.00129 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 15 [15, 15]
Total reward: -2.00129 [-2.00129, -2.00129]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -2.00129 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=3 o=0 a=2 o=0 a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 5.688 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 11 [11, 11]
Total reward: 5.688 [5.688, 5.688]
Policy after 1 simulations
MCTS Policy:
a=3 : 5.688 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 5.688 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
W

# # # # 
# . 1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=3 o=0 a=2 o=0 a=4 o=0 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 7.73781 [7.73781, 7.73781]
Policy after 1 simulations
MCTS Policy:
a=1 : 7.73781 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=3 o=0 a=2 o=0 a=4 o=0 a=1 o=0 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 23.3383, average = 6.67701
Undiscounted return = 30, average = 10
Starting run 5 with 1 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 7.41825
Total reward = 7.04733
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.04733 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 22 [22, 22]
Total reward: 7.04733 [7.04733, 7.04733]
Policy after 1 simulations
MCTS Policy:
a=5 : 7.04733 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.04733 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Check 1

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# 2$* #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 6.98337 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: 6.98337 [6.98337, 6.98337]
Policy after 1 simulations
MCTS Policy:
a=6 : 6.98337 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 6.98337 (1)
a=7 : 0 (0)
Check 2

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 17.1701
Total reward = 16.3116
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 16.3116 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 16.3116 [16.3116, 16.3116]
Policy after 1 simulations
MCTS Policy:
a=6 : 16.3116 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 16.3116 (1)
a=7 : 0 (0)
Check 2

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 6.84969
Total reward = 6.50721
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 6.50721 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 12 [12, 12]
Total reward: 6.50721 [6.50721, 6.50721]
Policy after 1 simulations
MCTS Policy:
a=6 : 6.50721 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 6.50721 (1)
a=7 : 0 (0)
Check 2

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 12 steps, with total reward -10.6236
Total reward = -10.0924
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -10.0924 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 12 [12, 12]
Total reward: -10.0924 [-10.0924, -10.0924]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -10.0924 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Check 3

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 7 steps, with total reward 8.18686
Total reward = 7.77752
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 7.77752 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: 7.77752 [7.77752, 7.77752]
Policy after 1 simulations
MCTS Policy:
a=6 : 7.77752 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 7.77752 (1)
a=7 : 0 (0)
Check 2

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 31 steps, with total reward -2.48652
Total reward = -2.3622
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.3622 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 31 [31, 31]
Total reward: -2.3622 [-2.3622, -2.3622]
Policy after 1 simulations
MCTS Policy:
a=2 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.3622 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Check 3

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 4 steps, with total reward 17.5987
Total reward = 6.71881
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.71881 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 6.71881 [6.71881, 6.71881]
Policy after 1 simulations
MCTS Policy:
a=4 : 6.71881 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.71881 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Sample

# # # # 
# * 1X#
# 2X. #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward -4.312
Total reward = -4.0964
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -4.0964 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 12 [12, 12]
Total reward: -4.0964 [-4.0964, -4.0964]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -4.0964 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Check 2

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 3 steps, with total reward -0.975
Total reward = -0.92625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: -0.92625 [-0.92625, -0.92625]
Policy after 1 simulations
MCTS Policy:
a=2 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Check 3

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 3 steps, with total reward -0.975
Total reward = -0.92625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: -0.92625 [-0.92625, -0.92625]
Policy after 1 simulations
MCTS Policy:
a=2 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Check 3

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 7.73781 [7.73781, 7.73781]
Policy after 1 simulations
MCTS Policy:
a=7 : 7.73781 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 7.73781 (1)
Check 3

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward -0.716715
Total reward = -0.680879
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.680879 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 9 [9, 9]
Total reward: -0.680879 [-0.680879, -0.680879]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.680879 (1)
a=7 : 0 (0)
Check 3

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 8 steps, with total reward -0.754436
Total reward = -0.716715
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.716715 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 8 [8, 8]
Total reward: -0.716715 [-0.716715, -0.716715]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.716715 (1)
a=7 : 0 (0)
Check 3

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 6 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 7.73781 [7.73781, 7.73781]
Policy after 1 simulations
MCTS Policy:
a=7 : 7.73781 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 7.73781 (1)
Check 3

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 35 steps, with total reward -0.149839
Total reward = -0.142347
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.142347 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 35 [35, 35]
Total reward: -0.142347 [-0.142347, -0.142347]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.142347 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# . * #
# 2X. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=6 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
a=7 : 0 (0)
Check 2

# # # # 
# . * #
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=7 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 8.57375 (1)
Check 3

# # # # 
# . * #
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward -4.17248
Total reward = -13.9639
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -13.9639 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 17 [17, 17]
Total reward: -13.9639 [-13.9639, -13.9639]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -13.9639 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Check 3

# # # # 
# . * #
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 6.6342 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 8 [8, 8]
Total reward: 6.6342 [6.6342, 6.6342]
Policy after 1 simulations
MCTS Policy:
a=3 : 6.6342 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 6.6342 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
W

# # # # 
# * 1X#
# 2X. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 6.56874
Total reward = 6.2403
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 6.2403 (1)
Tree depth: 0 [0, 0]
Rollout depth: 14 [14, 14]
Total reward: 6.2403 [6.2403, 6.2403]
Policy after 1 simulations
MCTS Policy:
a=7 : 6.2403 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 6.2403 (1)
Check 3

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 a=7 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 26 steps, with total reward -6.2511
Total reward = -5.93855
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -5.93855 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 26 [26, 26]
Total reward: -5.93855 [-5.93855, -5.93855]
Policy after 1 simulations
MCTS Policy:
a=2 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -5.93855 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Check 2

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 a=7 o=2 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 15.208
Total reward = 14.4476
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 14.4476 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 9 [9, 9]
Total reward: 14.4476 [14.4476, 14.4476]
Policy after 1 simulations
MCTS Policy:
a=6 : 14.4476 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 14.4476 (1)
a=7 : 0 (0)
Check 2

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 a=7 o=2 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.6342 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 8 [8, 8]
Total reward: 6.6342 [6.6342, 6.6342]
Policy after 1 simulations
MCTS Policy:
a=1 : 6.6342 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.6342 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# . * #
# 2X. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 a=7 o=2 a=6 o=2 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward -8.73735
Total reward = -8.30048
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -8.30048 (1)
Tree depth: 0 [0, 0]
Rollout depth: 21 [21, 21]
Total reward: -8.30048 [-8.30048, -8.30048]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -8.30048 (1)
Sample

# # # # 
# . * #
# 2X. #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 a=7 o=2 a=6 o=2 a=6 o=2 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 13.9851
Total reward = 13.2859
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 13.2859 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9 [9, 9]
Total reward: 13.2859 [13.2859, 13.2859]
Policy after 1 simulations
MCTS Policy:
a=7 : 13.2859 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 13.2859 (1)
Check 3

# # # # 
# . * #
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 a=7 o=2 a=6 o=2 a=6 o=2 a=1 o=0 a=4 o=0 a=7 o=2 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=3 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
W

# # # # 
# * . #
# 2X. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 a=7 o=2 a=6 o=2 a=6 o=2 a=1 o=0 a=4 o=0 a=7 o=2 a=3 o=0 
Starting simulation

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 8 [8, 8]
Total reward: 6.6342 [6.6342, 6.6342]
Policy after 1 simulations
MCTS Policy:
a=2 : 6.6342 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
S

# # # # 
# . . #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 a=7 o=2 a=6 o=2 a=6 o=2 a=1 o=0 a=4 o=0 a=7 o=2 a=3 o=0 a=2 o=0 
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = -3.01663
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -3.01663 (1)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: -3.01663 [-3.01663, -3.01663]
Policy after 1 simulations
MCTS Policy:
a=0 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -3.01663 (1)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
N

# # # # 
# * . #
# 2X. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 a=7 o=2 a=6 o=2 a=6 o=2 a=1 o=0 a=4 o=0 a=7 o=2 a=3 o=0 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 12 steps, with total reward -1.29537
Total reward = -1.2306
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1.2306 (1)
Tree depth: 0 [0, 0]
Rollout depth: 12 [12, 12]
Total reward: -1.2306 [-1.2306, -1.2306]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1.2306 (1)
E

# # # # 
# . * #
# 2X. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 a=7 o=2 a=6 o=2 a=6 o=2 a=1 o=0 a=4 o=0 a=7 o=2 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 11.8601
Total reward = 11.2671
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 11.2671 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 15 [15, 15]
Total reward: 11.2671 [11.2671, 11.2671]
Policy after 1 simulations
MCTS Policy:
a=3 : 11.2671 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 11.2671 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
W

# # # # 
# * . #
# 2X. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 a=7 o=2 a=6 o=2 a=6 o=2 a=1 o=0 a=4 o=0 a=7 o=2 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: 6.98337 [6.98337, 6.98337]
Policy after 1 simulations
MCTS Policy:
a=2 : 6.98337 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
S

# # # # 
# . . #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 a=7 o=2 a=6 o=2 a=6 o=2 a=1 o=0 a=4 o=0 a=7 o=2 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 a=2 o=0 
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 16.3025
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 16.3025 (1)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 9 [9, 9]
Total reward: 16.3025 [16.3025, 16.3025]
Policy after 1 simulations
MCTS Policy:
a=4 : 16.3025 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 16.3025 (1)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Sample

# # # # 
# . . #
# * . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 a=7 o=2 a=6 o=2 a=6 o=2 a=1 o=0 a=4 o=0 a=7 o=2 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=4 o=0 
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 8.14506 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=0 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : 8.14506 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 a=7 o=2 a=6 o=2 a=6 o=2 a=1 o=0 a=4 o=0 a=7 o=2 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=4 o=0 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=1 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 a=7 o=2 a=6 o=2 a=6 o=2 a=1 o=0 a=4 o=0 a=7 o=2 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=4 o=0 a=0 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.30249 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 9 [9, 9]
Total reward: 6.30249 [6.30249, 6.30249]
Policy after 1 simulations
MCTS Policy:
a=2 : 6.30249 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.30249 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
S

# # # # 
# . . #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 a=7 o=2 a=6 o=2 a=6 o=2 a=1 o=0 a=4 o=0 a=7 o=2 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=4 o=0 a=0 o=0 a=1 o=0 a=2 o=0 
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=3 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
W

# # # # 
# . . #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 a=7 o=2 a=6 o=2 a=6 o=2 a=1 o=0 a=4 o=0 a=7 o=2 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=4 o=0 a=0 o=0 a=1 o=0 a=2 o=0 a=3 o=0 
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 8.57375 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=1 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 8.57375 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
E

# # # # 
# . . #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=6 o=2 a=6 o=2 a=7 o=2 a=6 o=2 a=7 o=2 a=4 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=7 o=2 a=1 o=0 a=6 o=2 a=7 o=2 a=7 o=2 a=3 o=0 a=7 o=2 a=6 o=2 a=6 o=2 a=1 o=0 a=4 o=0 a=7 o=2 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=4 o=0 a=0 o=0 a=1 o=0 a=2 o=0 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
E

# # # # 
# . . #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 3.55033, average = 6.05167
Undiscounted return = 0, average = 8
Starting run 6 with 1 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 16 steps, with total reward 3.58449
Total reward = -6.59474
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -6.59474 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 16 [16, 16]
Total reward: -6.59474 [-6.59474, -6.59474]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -6.59474 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Check 1

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=5 o=1 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -3.03763
Total reward = -2.88575
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -2.88575 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 11 [11, 11]
Total reward: -2.88575 [-2.88575, -2.88575]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -2.88575 (1)
a=7 : 0 (0)
Sample

# # # # 
# * 1$#
# 2$. #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 23.3593
Total reward = 22.1913
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 22.1913 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8 [8, 8]
Total reward: 22.1913 [22.1913, 22.1913]
Policy after 1 simulations
MCTS Policy:
a=7 : 22.1913 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 22.1913 (1)
Check 3

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 6 attempts
Expanding node: a=5 o=1 a=4 o=0 a=7 o=2 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 43 steps, with total reward -0.30888
Total reward = -0.293436
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -0.293436 (1)
Tree depth: 0 [0, 0]
Rollout depth: 43 [43, 43]
Total reward: -0.293436 [-0.293436, -0.293436]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -0.293436 (1)
E

# # # # 
# . * #
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=4 o=0 a=7 o=2 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=6 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
a=7 : 0 (0)
Check 2

# # # # 
# . * #
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=4 o=0 a=7 o=2 a=1 o=0 a=6 o=1 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward -5.3188
Total reward = -5.05286
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -5.05286 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 18 [18, 18]
Total reward: -5.05286 [-5.05286, -5.05286]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -5.05286 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Check 2

# # # # 
# . * #
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=4 o=0 a=7 o=2 a=1 o=0 a=6 o=1 a=6 o=1 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=7 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.5 (1)
Check 3

# # # # 
# . * #
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=4 o=0 a=7 o=2 a=1 o=0 a=6 o=1 a=6 o=1 a=7 o=1 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# . * #
# 2$. #
# # # # 
Reward 10
Terminated
Discounted return = 16.4834, average = 7.79029
Undiscounted return = 20, average = 10
Starting run 7 with 1 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 13 steps, with total reward 4.56766
Total reward = 4.33928
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 4.33928 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 13 [13, 13]
Total reward: 4.33928 [4.33928, 4.33928]
Policy after 1 simulations
MCTS Policy:
a=6 : 4.33928 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 4.33928 (1)
a=7 : 0 (0)
Check 2

# # # # 
# * 1$#
# 2X. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.14506 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=5 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.14506 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Check 1

# # # # 
# * 1$#
# 2X. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 7 steps, with total reward 17.3509
Total reward = 16.4834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.4834 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: 16.4834 [16.4834, 16.4834]
Policy after 1 simulations
MCTS Policy:
a=5 : 16.4834 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.4834 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Check 1

# # # # 
# * 1$#
# 2X. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=6 o=1 a=5 o=1 a=5 o=1 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# 2X* #
# # # # 
Ending rollout after 17 steps, with total reward -5.09873
Total reward = -4.8438
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -4.8438 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 17 [17, 17]
Total reward: -4.8438 [-4.8438, -4.8438]
Policy after 1 simulations
MCTS Policy:
a=2 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -4.8438 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Check 1

# # # # 
# * 1$#
# 2X. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=5 o=1 a=5 o=1 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 31 steps, with total reward -0.398513
Total reward = -0.378588
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.378588 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 31 [31, 31]
Total reward: -0.378588 [-0.378588, -0.378588]
Policy after 1 simulations
MCTS Policy:
a=2 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.378588 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Sample

# # # # 
# * 1$#
# 2X. #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 27 steps, with total reward 6.40126
Total reward = 6.0812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.0812 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 27 [27, 27]
Total reward: 6.0812 [6.0812, 6.0812]
Policy after 1 simulations
MCTS Policy:
a=1 : 6.0812 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.0812 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# . * #
# 2X. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 17.3874
Total reward = 16.5181
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 16.5181 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 16 [16, 16]
Total reward: 16.5181 [16.5181, 16.5181]
Policy after 1 simulations
MCTS Policy:
a=3 : 16.5181 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 16.5181 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
W

# # # # 
# * 1$#
# 2X. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=4 o=0 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 4 steps, with total reward 18.0737
Total reward = 17.1701
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.1701 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 17.1701 [17.1701, 17.1701]
Policy after 1 simulations
MCTS Policy:
a=1 : 17.1701 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.1701 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# . * #
# 2X. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=4 o=0 a=1 o=0 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 7 steps, with total reward -0.794144
Total reward = -0.754436
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -0.754436 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: -0.754436 [-0.754436, -0.754436]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -0.754436 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Check 2

# # # # 
# . * #
# 2X. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 6 attempts
Expanding node: a=6 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=4 o=0 a=1 o=0 a=3 o=0 a=1 o=0 a=6 o=1 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 16.9834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 16.9834 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: 16.9834 [16.9834, 16.9834]
Policy after 1 simulations
MCTS Policy:
a=4 : 16.9834 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 16.9834 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Sample

# # # # 
# . * #
# 2X. #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=4 o=0 a=1 o=0 a=3 o=0 a=1 o=0 a=6 o=1 a=4 o=0 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=2 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
S

# # # # 
# . . #
# 2X* #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=4 o=0 a=1 o=0 a=3 o=0 a=1 o=0 a=6 o=1 a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=3 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
W

# # # # 
# . . #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=4 o=0 a=1 o=0 a=3 o=0 a=1 o=0 a=6 o=1 a=4 o=0 a=2 o=0 a=3 o=0 
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 12.0378
Total reward = 11.4359
MCTS Values:
a=0 : 0 (0)
a=1 : 11.4359 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 13 [13, 13]
Total reward: 11.4359 [11.4359, 11.4359]
Policy after 1 simulations
MCTS Policy:
a=1 : 11.4359 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 11.4359 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
E

# # # # 
# . . #
# 2X* #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=4 o=0 a=1 o=0 a=3 o=0 a=1 o=0 a=6 o=1 a=4 o=0 a=2 o=0 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 7.73781 [7.73781, 7.73781]
Policy after 1 simulations
MCTS Policy:
a=0 : 7.73781 (1)
Values after 1 simulations
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
N

# # # # 
# . * #
# 2X. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=4 o=0 a=1 o=0 a=3 o=0 a=1 o=0 a=6 o=1 a=4 o=0 a=2 o=0 a=3 o=0 a=1 o=0 a=0 o=0 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 17.1701
Total reward = 16.3116
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 16.3116 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 16.3116 [16.3116, 16.3116]
Policy after 1 simulations
MCTS Policy:
a=7 : 16.3116 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 16.3116 (1)
Check 3

# # # # 
# . * #
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=4 o=0 a=1 o=0 a=3 o=0 a=1 o=0 a=6 o=1 a=4 o=0 a=2 o=0 a=3 o=0 a=1 o=0 a=0 o=0 a=7 o=2 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 18 steps, with total reward 4.1812
Total reward = 3.97214
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.97214 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 18 [18, 18]
Total reward: 3.97214 [3.97214, 3.97214]
Policy after 1 simulations
MCTS Policy:
a=2 : 3.97214 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.97214 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
S

# # # # 
# . . #
# 2X* #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=4 o=0 a=1 o=0 a=3 o=0 a=1 o=0 a=6 o=1 a=4 o=0 a=2 o=0 a=3 o=0 a=1 o=0 a=0 o=0 a=7 o=2 a=2 o=0 
Starting simulation

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
E

# # # # 
# . . #
# 2X* #
# # # # 
Reward 10
Terminated
Discounted return = 18.8488, average = 9.37008
Undiscounted return = 30, average = 12.8571
Starting run 8 with 1 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# 2$* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 7.73781 [7.73781, 7.73781]
Policy after 1 simulations
MCTS Policy:
a=2 : 7.73781 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
S

# # # # 
# 0$1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Ending rollout after 4 steps, with total reward 17.5987
Total reward = 16.7188
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.7188 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 16.7188 [16.7188, 16.7188]
Policy after 1 simulations
MCTS Policy:
a=5 : 16.7188 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.7188 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Check 1

# # # # 
# 0$1X#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = -2.64908
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: -2.64908 [-2.64908, -2.64908]
Policy after 1 simulations
MCTS Policy:
a=0 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Check 1

# # # # 
# 0$1X#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=2 o=0 a=5 o=1 a=5 o=1 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 4 steps, with total reward 18.0737
Total reward = 17.1701
MCTS Values:
a=0 : 17.1701 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 17.1701 [17.1701, 17.1701]
Policy after 1 simulations
MCTS Policy:
a=0 : 17.1701 (1)
Values after 1 simulations
MCTS Values:
a=0 : 17.1701 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
N

# # # # 
# * 1X#
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=5 o=1 a=0 o=0 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 14 [14, 14]
Total reward: 4.87675 [4.87675, 4.87675]
Policy after 1 simulations
MCTS Policy:
a=2 : 4.87675 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
S

# # # # 
# 0$1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=5 o=1 a=0 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# 2X* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 8.57375 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=1 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 8.57375 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# 0$1X#
# 2$* #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=5 o=1 a=0 o=0 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0$1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 14 steps, with total reward 21.445
Total reward = 20.3727
MCTS Values:
a=0 : 20.3727 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 14 [14, 14]
Total reward: 20.3727 [20.3727, 20.3727]
Policy after 1 simulations
MCTS Policy:
a=0 : 20.3727 (1)
Values after 1 simulations
MCTS Values:
a=0 : 20.3727 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
N

# # # # 
# 0$* #
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=5 o=1 a=0 o=0 a=2 o=0 a=1 o=0 a=0 o=0 
Starting simulation

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 26 steps, with total reward 13.3845
Total reward = 12.7153
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 12.7153 (1)
Tree depth: 0 [0, 0]
Rollout depth: 26 [26, 26]
Total reward: 12.7153 [12.7153, 12.7153]
Policy after 1 simulations
MCTS Policy:
a=7 : 12.7153 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 12.7153 (1)
Check 3

# # # # 
# 0$* #
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=5 o=1 a=0 o=0 a=2 o=0 a=1 o=0 a=0 o=0 a=7 o=1 
Starting simulation

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 20 steps, with total reward 18.4064
Total reward = 17.4861
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 17.4861 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 20 [20, 20]
Total reward: 17.4861 [17.4861, 17.4861]
Policy after 1 simulations
MCTS Policy:
a=6 : 17.4861 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 17.4861 (1)
a=7 : 0 (0)
Check 2

# # # # 
# 0$* #
# 2$. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 7 attempts
Expanding node: a=2 o=0 a=5 o=1 a=5 o=1 a=0 o=0 a=2 o=0 a=1 o=0 a=0 o=0 a=7 o=1 a=6 o=2 
Starting simulation

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# 0$* #
# 2$. #
# # # # 
Reward 10
Terminated
Discounted return = 6.30249, average = 8.98663
Undiscounted return = 10, average = 12.5
Starting run 9 with 1 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Ending rollout after 11 steps, with total reward 12.9707
Total reward = 12.3222
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 12.3222 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 11 [11, 11]
Total reward: 12.3222 [12.3222, 12.3222]
Policy after 1 simulations
MCTS Policy:
a=6 : 12.3222 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 12.3222 (1)
a=7 : 0 (0)
Check 2

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# 2X* #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.35092 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=5 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.35092 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Check 1

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=6 o=2 a=5 o=1 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 18.5738 [18.5738, 18.5738]
Policy after 1 simulations
MCTS Policy:
a=4 : 18.5738 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Sample

# # # # 
# * 1X#
# 2$. #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=5 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 5 steps, with total reward -1.85494
Total reward = -1.76219
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.76219 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: -1.76219 [-1.76219, -1.76219]
Policy after 1 simulations
MCTS Policy:
a=2 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.76219 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Check 2

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=5 o=1 a=4 o=0 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 7.73781 [7.73781, 7.73781]
Policy after 1 simulations
MCTS Policy:
a=7 : 7.73781 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 7.73781 (1)
Check 3

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=5 o=1 a=4 o=0 a=6 o=1 a=7 o=1 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# . * #
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=5 o=1 a=4 o=0 a=6 o=1 a=7 o=1 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 20 steps, with total reward 3.77354
Total reward = 3.58486
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 3.58486 (1)
Tree depth: 0 [0, 0]
Rollout depth: 20 [20, 20]
Total reward: 3.58486 [3.58486, 3.58486]
Policy after 1 simulations
MCTS Policy:
a=7 : 3.58486 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 3.58486 (1)
Check 3

# # # # 
# . * #
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=6 o=2 a=5 o=1 a=4 o=0 a=6 o=1 a=7 o=1 a=1 o=0 a=7 o=1 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 2 steps, with total reward -0.5
Total reward = -0.475
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : -0.475 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: -0.475 [-0.475, -0.475]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : -0.475 (1)
a=7 : 0 (0)
Sample

# # # # 
# . * #
# 2$. #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=5 o=1 a=4 o=0 a=6 o=1 a=7 o=1 a=1 o=0 a=7 o=1 a=4 o=0 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
E

# # # # 
# . * #
# 2$. #
# # # # 
Reward 10
Terminated
Discounted return = 8.67583, average = 8.9521
Undiscounted return = 10, average = 12.2222
Starting run 10 with 1 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 13 steps, with total reward -4.0964
Total reward = -3.89158
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -3.89158 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 13 [13, 13]
Total reward: -3.89158 [-3.89158, -3.89158]
Policy after 1 simulations
MCTS Policy:
a=2 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -3.89158 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Check 2

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 29 steps, with total reward 14.5895
Total reward = 13.86
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 13.86 (1)
Tree depth: 0 [0, 0]
Rollout depth: 29 [29, 29]
Total reward: 13.86 [13.86, 13.86]
Policy after 1 simulations
MCTS Policy:
a=7 : 13.86 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 13.86 (1)
Check 3

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=7 o=1 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 46 steps, with total reward -1.02341
Total reward = -0.97224
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -0.97224 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 46 [46, 46]
Total reward: -0.97224 [-0.97224, -0.97224]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -0.97224 (1)
a=7 : 0 (0)
Check 1

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=7 o=1 a=5 o=1 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 20 steps, with total reward 3.15904
Total reward = 3.00109
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 3.00109 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 20 [20, 20]
Total reward: 3.00109 [3.00109, 3.00109]
Policy after 1 simulations
MCTS Policy:
a=5 : 3.00109 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 3.00109 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Check 1

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=6 o=1 a=7 o=1 a=5 o=1 a=5 o=1 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 28 steps, with total reward 15.56
Total reward = 14.782
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 14.782 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 28 [28, 28]
Total reward: 14.782 [14.782, 14.782]
Policy after 1 simulations
MCTS Policy:
a=5 : 14.782 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 14.782 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Check 1

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=6 o=1 a=7 o=1 a=5 o=1 a=5 o=1 a=5 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 21.0628
Total reward = 20.0097
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 20.0097 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 13 [13, 13]
Total reward: 20.0097 [20.0097, 20.0097]
Policy after 1 simulations
MCTS Policy:
a=5 : 20.0097 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 20.0097 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Check 1

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=7 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=5 o=1 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
a=7 : 0 (0)
Check 2

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=7 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 16.9834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: 16.9834 [16.9834, 16.9834]
Policy after 1 simulations
MCTS Policy:
a=4 : 16.9834 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Sample

# # # # 
# * 1$#
# 2$. #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=7 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=6 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 4 steps, with total reward 18.0737
Total reward = 17.1701
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.1701 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 17.1701 [17.1701, 17.1701]
Policy after 1 simulations
MCTS Policy:
a=1 : 17.1701 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.1701 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# . * #
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=7 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=6 o=1 a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 7.9932
Total reward = 7.59354
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.59354 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 18 [18, 18]
Total reward: 7.59354 [7.59354, 7.59354]
Policy after 1 simulations
MCTS Policy:
a=3 : 7.59354 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.59354 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
W

# # # # 
# * 1$#
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=7 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=6 o=1 a=4 o=0 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward -2.04981
Total reward = -1.94732
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.94732 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 12 [12, 12]
Total reward: -1.94732 [-1.94732, -1.94732]
Policy after 1 simulations
MCTS Policy:
a=2 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.94732 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Check 3

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=6 o=1 a=7 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=6 o=1 a=4 o=0 a=1 o=0 a=3 o=0 a=7 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 4 steps, with total reward 18.5737
Total reward = 17.6451
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.6451 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 17.6451 [17.6451, 17.6451]
Policy after 1 simulations
MCTS Policy:
a=1 : 17.6451 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.6451 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# . * #
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=7 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=5 o=1 a=6 o=1 a=4 o=0 a=1 o=0 a=3 o=0 a=7 o=1 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# . * #
# 2$. #
# # # # 
Reward 10
Terminated
Discounted return = 12.387, average = 9.29559
Undiscounted return = 20, average = 13
Simulations = 1
Runs = 10
Undiscounted return = 13 +- 3.47851
Discounted return = 9.29559 +- 2.55064
Time = 0.0040115
Starting run 1 with 2 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 14 steps, with total reward -0.853949
Total reward = -0.811251
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -0.811251 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward -10.3885
Total reward = -9.86912
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -9.86912 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -0.811251 (1)
Tree depth: 0 [0, 0]
Rollout depth: 12.5 [11, 14]
Total reward: -5.34019 [-9.86912, -0.811251]
Policy after 2 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -9.86912 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -0.811251 (1)
E

# # # # 
# 0X* #
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 29 steps, with total reward 13.5217
Total reward = 12.8456
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 12.8456 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# 2X* #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 12.8456 (1)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 15.5 [2, 29]
Total reward: 10.9353 [9.025, 12.8456]
Policy after 2 simulations
MCTS Policy:
a=3 : 12.8456 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 12.8456 (1)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 0 (0)
a=7 : 0 (0)
W

# # # # 
# * 1X#
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# 2$* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 16.9834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [3, 7]
Total reward: 12.7786 [8.57375, 16.9834]
Policy after 2 simulations
MCTS Policy:
a=4 : 16.9834 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Sample

# # # # 
# * 1X#
# 2$. #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=3 o=0 a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [1, 5]
Total reward: 8.6189 [7.73781, 9.5]
Policy after 2 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# . * #
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=3 o=0 a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 9.5848
Total reward = 9.10556
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.10556 (1)
Tree depth: 0 [0, 0]
Rollout depth: 10 [2, 18]
Total reward: 9.06528 [9.025, 9.10556]
Policy after 2 simulations
MCTS Policy:
a=7 : 9.10556 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.10556 (1)
Check 3

# # # # 
# . * #
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=1 o=0 a=3 o=0 a=4 o=0 a=1 o=0 a=7 o=1 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = -0.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 16 steps, with total reward 12.778
Total reward = 12.1391
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 12.1391 (1)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 8.5 [1, 16]
Total reward: 5.81954 [-0.5, 12.1391]
Policy after 2 simulations
MCTS Policy:
a=2 : 12.1391 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 12.1391 (1)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
S

# # # # 
# . 1X#
# 2$* #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=3 o=0 a=4 o=0 a=1 o=0 a=7 o=1 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 8.57375 (1)
Starting simulation

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 9.28688 [8.57375, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 8.57375 (1)
E

# # # # 
# . 1X#
# 2$* #
# # # # 
Reward 10
Terminated
Discounted return = -1.67408, average = -1.67408
Undiscounted return = 0, average = 0
Starting run 2 with 2 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 20 steps, with total reward 18.7859
Total reward = 17.8466
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 17.8466 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0$* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Ending rollout after 2 steps, with total reward -0.5
Total reward = -0.475
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.475 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 17.8466 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11 [2, 20]
Total reward: 8.68581 [-0.475, 17.8466]
Policy after 2 simulations
MCTS Policy:
a=7 : 17.8466 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.475 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 17.8466 (1)
Check 3

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# 2$* #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 12 [10, 14]
Total reward: 5.43206 [4.87675, 5.98737]
Policy after 2 simulations
MCTS Policy:
a=1 : 5.98737 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# 0$* #
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 17 steps, with total reward 12.1391
Total reward = 11.5321
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 11.5321 (1)
Starting simulation

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# 2$* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 7.73781 (1)
a=7 : 11.5321 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11 [5, 17]
Total reward: 9.63497 [7.73781, 11.5321]
Policy after 2 simulations
MCTS Policy:
a=7 : 11.5321 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 7.73781 (1)
a=7 : 11.5321 (1)
Check 3

# # # # 
# 0$* #
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=7 o=1 a=1 o=0 a=7 o=1 
Starting simulation

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 19 steps, with total reward 21.5792
Total reward = 20.5002
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 20.5002 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 11.8601
Total reward = 11.2671
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 20.5002 (1)
a=6 : 0 (0)
a=7 : 11.2671 (1)
Tree depth: 0 [0, 0]
Rollout depth: 17 [15, 19]
Total reward: 15.8837 [11.2671, 20.5002]
Policy after 2 simulations
MCTS Policy:
a=5 : 20.5002 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 20.5002 (1)
a=6 : 0 (0)
a=7 : 11.2671 (1)
Check 1

# # # # 
# 0$* #
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=7 o=1 a=1 o=0 a=7 o=1 a=5 o=1 
Starting simulation

# # # # 
# 0$* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 18 steps, with total reward 17.4597
Total reward = 16.5867
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 16.5867 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 18 [18, 18]
Total reward: 13.2934 [10, 16.5867]
Policy after 2 simulations
MCTS Policy:
a=2 : 16.5867 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 16.5867 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
S

# # # # 
# 0$1X#
# 2$* #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=1 o=0 a=7 o=1 a=5 o=1 a=2 o=0 
Starting simulation

# # # # 
# 0$1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 19 steps, with total reward 8.59588
Total reward = 8.16608
MCTS Values:
a=0 : 8.16608 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# 2X* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 8.16608 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11 [3, 19]
Total reward: 8.36992 [8.16608, 8.57375]
Policy after 2 simulations
MCTS Policy:
a=7 : 8.57375 (1)
Values after 2 simulations
MCTS Values:
a=0 : 8.16608 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 8.57375 (1)
Check 3

# # # # 
# 0$1X#
# 2$* #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=7 o=1 a=1 o=0 a=7 o=1 a=5 o=1 a=2 o=0 a=7 o=1 
Starting simulation

# # # # 
# 0$1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$1X#
# 2$* #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 9.5 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 17.7378
Total reward = 16.8509
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 16.8509 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 9.5 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3.5 [1, 6]
Total reward: 13.1755 [9.5, 16.8509]
Policy after 2 simulations
MCTS Policy:
a=3 : 16.8509 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 16.8509 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 9.5 (1)
a=7 : 0 (0)
W

# # # # 
# 0$1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=1 o=0 a=7 o=1 a=5 o=1 a=2 o=0 a=7 o=1 a=3 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 9.025 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 9.025 (1)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3.5 [2, 5]
Total reward: 8.3814 [7.73781, 9.025]
Policy after 2 simulations
MCTS Policy:
a=0 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : 9.025 (1)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
N

# # # # 
# * 1X#
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=1 o=0 a=7 o=1 a=5 o=1 a=2 o=0 a=7 o=1 a=3 o=0 a=0 o=0 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 15.6592
Total reward = 14.8762
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 14.8762 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 14.8762 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5.5 [2, 9]
Total reward: 16.9506 [14.8762, 19.025]
Policy after 2 simulations
MCTS Policy:
a=4 : 19.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 14.8762 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Sample

# # # # 
# * 1X#
# 2$. #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=1 o=0 a=7 o=1 a=5 o=1 a=2 o=0 a=7 o=1 a=3 o=0 a=0 o=0 a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 19 steps, with total reward -0.660769
Total reward = -0.627731
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -0.627731 (1)
Tree depth: 0 [0, 0]
Rollout depth: 12 [5, 19]
Total reward: 3.55504 [-0.627731, 7.73781]
Policy after 2 simulations
MCTS Policy:
a=2 : 7.73781 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -0.627731 (1)
S

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=1 o=0 a=7 o=1 a=5 o=1 a=2 o=0 a=7 o=1 a=3 o=0 a=0 o=0 a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -1.67408
Total reward = -1.59038
MCTS Values:
a=0 : 0 (0)
a=1 : -1.59038 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [3, 7]
Total reward: 3.49169 [-1.59038, 8.57375]
Policy after 2 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : -1.59038 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : 0 (0)
Check 2

# # # # 
# . 1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=7 o=1 a=1 o=0 a=7 o=1 a=5 o=1 a=2 o=0 a=7 o=1 a=3 o=0 a=0 o=0 a=4 o=0 a=2 o=0 a=6 o=2 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 15.4036
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 11 steps, with total reward -0.646835
Total reward = -0.614493
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -0.614493 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11.5 [11, 12]
Total reward: 7.39455 [-0.614493, 15.4036]
Policy after 2 simulations
MCTS Policy:
a=4 : 15.4036 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -0.614493 (1)
Sample

# # # # 
# . 1X#
# * . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=1 o=0 a=7 o=1 a=5 o=1 a=2 o=0 a=7 o=1 a=3 o=0 a=0 o=0 a=4 o=0 a=2 o=0 a=6 o=2 a=4 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 6 [5, 7]
Total reward: 7.36059 [6.98337, 7.73781]
Policy after 2 simulations
MCTS Policy:
a=0 : 7.73781 (1)
Values after 2 simulations
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
a=7 : -1e+10 (1000000)
N

# # # # 
# * 1X#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=1 o=0 a=7 o=1 a=5 o=1 a=2 o=0 a=7 o=1 a=3 o=0 a=0 o=0 a=4 o=0 a=2 o=0 a=6 o=2 a=4 o=0 a=0 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward -1.54856
Total reward = -1.47113
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.47113 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward -0.475
Total reward = -0.45125
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.47113 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.45125 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 12 [3, 21]
Total reward: -0.961192 [-1.47113, -0.45125]
Policy after 2 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.47113 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.45125 (1)
a=7 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=1 o=0 a=7 o=1 a=5 o=1 a=2 o=0 a=7 o=1 a=3 o=0 a=0 o=0 a=4 o=0 a=2 o=0 a=6 o=2 a=4 o=0 a=0 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 6.30249 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 9 [9, 9]
Total reward: 8.15125 [6.30249, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 6.30249 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 17.199, average = 7.76244
Undiscounted return = 30, average = 15
Starting run 3 with 2 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 8 steps, with total reward -0.367546
Total reward = -0.349169
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.349169 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 11 steps, with total reward -1.75044
Total reward = 8.33708
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 8.33708 (1)
a=5 : -0.349169 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 9.5 [8, 11]
Total reward: 3.99396 [-0.349169, 8.33708]
Policy after 2 simulations
MCTS Policy:
a=4 : 8.33708 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 8.33708 (1)
a=5 : -0.349169 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Sample

# # # # 
# * 1$#
# 2X. #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 13 steps, with total reward 15.4036
Total reward = 14.6334
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.6334 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.6334 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9 [5, 13]
Total reward: 11.1856 [7.73781, 14.6334]
Policy after 2 simulations
MCTS Policy:
a=1 : 14.6334 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.6334 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 7.73781 (1)
E

# # # # 
# . * #
# 2X. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 19.025 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 14.5125 [10, 19.025]
Policy after 2 simulations
MCTS Policy:
a=4 : 19.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 19.025 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Sample

# # # # 
# . * #
# 2X. #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 6.30249 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 6.30249 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 9 [9, 9]
Total reward: 8.15125 [6.30249, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 6.30249 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
E

# # # # 
# . * #
# 2X. #
# # # # 
Reward 10
Terminated
Discounted return = 27.5987, average = 14.3745
Undiscounted return = 30, average = 20
Starting run 4 with 2 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward -2.2175
Total reward = -2.10662
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -2.10662 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 26 steps, with total reward -0.810963
Total reward = -0.770415
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -2.10662 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.770415 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 20 [14, 26]
Total reward: -1.43852 [-2.10662, -0.770415]
Policy after 2 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -2.10662 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.770415 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Sample

# # # # 
# * 1$#
# 2$. #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 6 steps, with total reward -0.407253
Total reward = -0.38689
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.38689 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7.5 [6, 9]
Total reward: 2.9578 [-0.38689, 6.30249]
Policy after 2 simulations
MCTS Policy:
a=2 : 6.30249 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.38689 (1)
a=7 : 0 (0)
S

# # # # 
# . 1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 37 steps, with total reward 1.57779
Total reward = -8.5011
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -8.5011 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -8.5011 (1)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 21.5 [6, 37]
Total reward: -0.575089 [-8.5011, 7.35092]
Policy after 2 simulations
MCTS Policy:
a=6 : 7.35092 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -8.5011 (1)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : 0 (0)
Check 2

# # # # 
# . 1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 6 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=1 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward -1.42625
Total reward = -1.35494
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1.35494 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = -3.69751
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -3.69751 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1.35494 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6.5 [4, 9]
Total reward: -2.52622 [-3.69751, -1.35494]
Policy after 2 simulations
MCTS Policy:
a=0 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -3.69751 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1.35494 (1)
N

# # # # 
# * 1$#
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=1 a=0 o=0 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 28 steps, with total reward -1.67776
Total reward = -1.59387
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1.59387 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 11 steps, with total reward -1.36355
Total reward = -1.29537
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.29537 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1.59387 (1)
Tree depth: 0 [0, 0]
Rollout depth: 19.5 [11, 28]
Total reward: -1.44462 [-1.59387, -1.29537]
Policy after 2 simulations
MCTS Policy:
a=2 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.29537 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1.59387 (1)
S

# # # # 
# . 1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=1 a=0 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 16.6342
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 26 steps, with total reward 6.00923
Total reward = 5.70877
MCTS Values:
a=0 : 5.70877 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 17 [8, 26]
Total reward: 11.1715 [5.70877, 16.6342]
Policy after 2 simulations
MCTS Policy:
a=4 : 16.6342 (1)
Values after 2 simulations
MCTS Values:
a=0 : 5.70877 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Sample

# # # # 
# . 1$#
# * . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=1 a=0 o=0 a=2 o=0 a=4 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 14 steps, with total reward -0.55458
Total reward = -0.526851
MCTS Values:
a=0 : -0.526851 (1)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 9.5 [5, 14]
Total reward: 3.60548 [-0.526851, 7.73781]
Policy after 2 simulations
MCTS Policy:
a=1 : 7.73781 (1)
Values after 2 simulations
MCTS Values:
a=0 : -0.526851 (1)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=1 a=0 o=0 a=2 o=0 a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 9.07253 [8.14506, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
a=7 : -1e+10 (1000000)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 4.72118, average = 11.9612
Undiscounted return = 10, average = 17.5
Starting run 5 with 2 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 16.0084
Total reward = 15.208
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 15.208 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 15.208 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [2, 8]
Total reward: 12.1165 [9.025, 15.208]
Policy after 2 simulations
MCTS Policy:
a=2 : 15.208 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 15.208 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
S

# # # # 
# 0$1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# 2$* #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.40127
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 4.40127 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 39 steps, with total reward 6.84908
Total reward = 6.50662
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 4.40127 (1)
a=6 : 0 (0)
a=7 : 6.50662 (1)
Tree depth: 0 [0, 0]
Rollout depth: 27.5 [16, 39]
Total reward: 5.45394 [4.40127, 6.50662]
Policy after 2 simulations
MCTS Policy:
a=7 : 6.50662 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 4.40127 (1)
a=6 : 0 (0)
a=7 : 6.50662 (1)
Check 3

# # # # 
# 0$1X#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=7 o=1 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 16 steps, with total reward -3.94084
Total reward = -3.7438
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -3.7438 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 17.2378
Total reward = 16.3759
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -3.7438 (1)
a=6 : 16.3759 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 11 [6, 16]
Total reward: 6.31606 [-3.7438, 16.3759]
Policy after 2 simulations
MCTS Policy:
a=6 : 16.3759 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -3.7438 (1)
a=6 : 16.3759 (1)
a=7 : 0 (0)
Check 2

# # # # 
# 0$1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=2 o=0 a=7 o=1 a=6 o=2 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 19 steps, with total reward -10.0052
Total reward = 0.495053
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0.495053 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# 2$* #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0.495053 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 12.5 [6, 19]
Total reward: 3.92299 [0.495053, 7.35092]
Policy after 2 simulations
MCTS Policy:
a=0 : 7.35092 (1)
Values after 2 simulations
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0.495053 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
N

# # # # 
# * 1X#
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=7 o=1 a=6 o=2 a=0 o=0 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 14.8767
Total reward = 14.1329
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 14.1329 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 14.1329 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 8.5 [2, 15]
Total reward: 11.579 [9.025, 14.1329]
Policy after 2 simulations
MCTS Policy:
a=2 : 14.1329 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 14.1329 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 0 (0)
a=7 : 0 (0)
S

# # # # 
# 0$1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=7 o=1 a=6 o=2 a=0 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 12 steps, with total reward -3.812
Total reward = -3.6214
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -3.6214 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = -2.26219
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -3.6214 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8.5 [5, 12]
Total reward: -2.94179 [-3.6214, -2.26219]
Policy after 2 simulations
MCTS Policy:
a=0 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -3.6214 (1)
Check 1

# # # # 
# 0$1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=7 o=1 a=6 o=2 a=0 o=0 a=2 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 33 steps, with total reward -2.17847
Total reward = -2.06955
MCTS Values:
a=0 : -2.06955 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward -2.04163
Total reward = 8.06045
MCTS Values:
a=0 : -2.06955 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 8.06045 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 20.5 [8, 33]
Total reward: 2.99545 [-2.06955, 8.06045]
Policy after 2 simulations
MCTS Policy:
a=4 : 8.06045 (1)
Values after 2 simulations
MCTS Values:
a=0 : -2.06955 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 8.06045 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Sample

# # # # 
# 0$1X#
# * . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=7 o=1 a=6 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=4 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 16.8509
Total reward = 16.0084
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 16.0084 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 14.7793
Total reward = 14.0403
MCTS Values:
a=0 : 0 (0)
a=1 : 14.0403 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 16.0084 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 8 [7, 9]
Total reward: 15.0243 [14.0403, 16.0084]
Policy after 2 simulations
MCTS Policy:
a=5 : 16.0084 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 14.0403 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 16.0084 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Check 1

# # # # 
# 0$1X#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=7 o=1 a=6 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=4 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 12.387
Total reward = 11.7676
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 11.7676 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 11.7676 (1)
a=6 : 6.6342 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 10.5 [8, 13]
Total reward: 9.20091 [6.6342, 11.7676]
Policy after 2 simulations
MCTS Policy:
a=5 : 11.7676 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 11.7676 (1)
a=6 : 6.6342 (1)
a=7 : -1e+10 (1000000)
Check 1

# # # # 
# 0$1X#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=7 o=1 a=6 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=4 o=0 a=5 o=1 a=5 o=1 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward -1.43532
Total reward = -1.36355
MCTS Values:
a=0 : -1.36355 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 15.496
Total reward = 14.7212
MCTS Values:
a=0 : -1.36355 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 14.7212 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 8.5 [7, 10]
Total reward: 6.67882 [-1.36355, 14.7212]
Policy after 2 simulations
MCTS Policy:
a=6 : 14.7212 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1.36355 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 14.7212 (1)
a=7 : -1e+10 (1000000)
Check 2

# # # # 
# 0$1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=2 o=0 a=7 o=1 a=6 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=4 o=0 a=5 o=1 a=5 o=1 a=6 o=2 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 5.98737 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 8 [6, 10]
Total reward: 6.66914 [5.98737, 7.35092]
Policy after 2 simulations
MCTS Policy:
a=1 : 7.35092 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 5.98737 (1)
a=7 : -1e+10 (1000000)
E

# # # # 
# 0$1X#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=7 o=1 a=6 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=4 o=0 a=5 o=1 a=5 o=1 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward -1.2306
Total reward = -1.16907
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : -1.16907 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 13 [13, 13]
Total reward: 4.41546 [-1.16907, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : -1.16907 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
E

# # # # 
# 0$1X#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 13.0389, average = 12.1767
Undiscounted return = 20, average = 18
Starting run 6 with 2 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 20 steps, with total reward 8.14012
Total reward = 7.73311
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 7.73311 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# 2X* #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.6342 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 7.73311 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 14 [8, 20]
Total reward: 7.18366 [6.6342, 7.73311]
Policy after 2 simulations
MCTS Policy:
a=6 : 7.73311 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.6342 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 7.73311 (1)
a=7 : 0 (0)
Check 2

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward -0.55458
Total reward = -0.526851
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.526851 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 7.59875
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 7.59875 (1)
a=5 : -0.526851 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 8.5 [3, 14]
Total reward: 3.53595 [-0.526851, 7.59875]
Policy after 2 simulations
MCTS Policy:
a=4 : 7.59875 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 7.59875 (1)
a=5 : -0.526851 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Sample

# # # # 
# * 1$#
# 2$. #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 17 steps, with total reward -2.58211
Total reward = -2.453
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -2.453 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 26 steps, with total reward -4.57702
Total reward = -4.34817
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -4.34817 (1)
a=7 : -2.453 (1)
Tree depth: 0 [0, 0]
Rollout depth: 21.5 [17, 26]
Total reward: -3.40059 [-4.34817, -2.453]
Policy after 2 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -4.34817 (1)
a=7 : -2.453 (1)
E

# # # # 
# . * #
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward -1.28172
Total reward = -1.21763
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1.21763 (1)
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1.21763 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11.5 [2, 21]
Total reward: 3.90368 [-1.21763, 9.025]
Policy after 2 simulations
MCTS Policy:
a=2 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1.21763 (1)
S

# # # # 
# . 1$#
# 2$* #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=1 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 8 steps, with total reward -3.01663
Total reward = -2.8658
MCTS Values:
a=0 : -2.8658 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 8 [8, 8]
Total reward: 1.8842 [-2.8658, 6.6342]
Policy after 2 simulations
MCTS Policy:
a=6 : 6.6342 (1)
Values after 2 simulations
MCTS Values:
a=0 : -2.8658 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
a=7 : 0 (0)
Check 2

# # # # 
# . 1$#
# 2$* #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=6 o=1 a=4 o=0 a=1 o=0 a=2 o=0 a=6 o=1 
Starting simulation

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 7 steps, with total reward 16.8509
Total reward = 16.0084
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 16.0084 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 27 steps, with total reward 15.6741
Total reward = 14.8904
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 14.8904 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 16.0084 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 17 [7, 27]
Total reward: 15.4494 [14.8904, 16.0084]
Policy after 2 simulations
MCTS Policy:
a=6 : 16.0084 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 14.8904 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 16.0084 (1)
a=7 : 0 (0)
Check 2

# # # # 
# . 1$#
# 2$* #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=1 o=0 a=2 o=0 a=6 o=1 a=6 o=1 
Starting simulation

# # # # 
# . 1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 7.73781 [7.73781, 7.73781]
Policy after 2 simulations
MCTS Policy:
a=0 : 7.73781 (1)
Values after 2 simulations
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : 0 (0)
Check 2

# # # # 
# . 1$#
# 2$* #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=1 o=0 a=2 o=0 a=6 o=1 a=6 o=1 a=6 o=1 
Starting simulation

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 15.5571
Total reward = 14.7793
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 14.7793 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 5 steps, with total reward 16.7188
Total reward = 15.8829
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 14.7793 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 15.8829 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6.5 [5, 8]
Total reward: 15.3311 [14.7793, 15.8829]
Policy after 2 simulations
MCTS Policy:
a=6 : 15.8829 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 14.7793 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 15.8829 (1)
a=7 : 0 (0)
Check 2

# # # # 
# . 1$#
# 2$* #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=1 o=0 a=2 o=0 a=6 o=1 a=6 o=1 a=6 o=1 a=6 o=1 
Starting simulation

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 9.28688 [8.57375, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# . 1$#
# 2$* #
# # # # 
Reward 10
Terminated
Discounted return = -2.8658, average = 9.66966
Undiscounted return = 0, average = 15
Starting run 7 with 2 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 20.6792
Total reward = 19.6453
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 19.6453 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 16.3116
Total reward = 25.496
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 25.496 (1)
a=5 : 0 (0)
a=6 : 19.6453 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 10.5 [6, 15]
Total reward: 22.5706 [19.6453, 25.496]
Policy after 2 simulations
MCTS Policy:
a=4 : 25.496 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 25.496 (1)
a=5 : 0 (0)
a=6 : 19.6453 (1)
a=7 : 0 (0)
Sample

# # # # 
# * 1$#
# 2X. #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 10 steps, with total reward -1.04842
Total reward = -0.996004
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.996004 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.996004 (1)
a=7 : 8.14506 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7 [4, 10]
Total reward: 3.57453 [-0.996004, 8.14506]
Policy after 2 simulations
MCTS Policy:
a=7 : 8.14506 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.996004 (1)
a=7 : 8.14506 (1)
Check 3

# # # # 
# * 1$#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 27 steps, with total reward 2.6352
Total reward = 2.50344
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 11 steps, with total reward -1.75044
Total reward = -1.66292
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.66292 (1)
a=2 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 19 [11, 27]
Total reward: 0.420261 [-1.66292, 2.50344]
Policy after 2 simulations
MCTS Policy:
a=2 : 2.50344 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.66292 (1)
a=2 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
S

# # # # 
# . 1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=2 a=2 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -9.67183
Total reward = -9.18824
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -9.18824 (1)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 7 steps, with total reward -0.38689
Total reward = -0.367546
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : -0.367546 (1)
a=7 : -9.18824 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9 [7, 11]
Total reward: -4.77789 [-9.18824, -0.367546]
Policy after 2 simulations
MCTS Policy:
a=0 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : -0.367546 (1)
a=7 : -9.18824 (1)
Sample

# # # # 
# . 1$#
# * . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=2 a=2 o=0 a=4 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.63291
MCTS Values:
a=0 : 4.63291 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 24 steps, with total reward 6.3089
Total reward = 5.99346
MCTS Values:
a=0 : 4.63291 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.99346 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 19.5 [15, 24]
Total reward: 5.31319 [4.63291, 5.99346]
Policy after 2 simulations
MCTS Policy:
a=6 : 5.99346 (1)
Values after 2 simulations
MCTS Values:
a=0 : 4.63291 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.99346 (1)
a=7 : -1e+10 (1000000)
Check 2

# # # # 
# . 1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=2 a=2 o=0 a=4 o=0 a=6 o=1 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.4036 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 6.5 [1, 12]
Total reward: 7.4518 [5.4036, 9.5]
Policy after 2 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.4036 (1)
a=7 : -1e+10 (1000000)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=2 a=2 o=0 a=4 o=0 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward -4.86658
Total reward = -4.62325
MCTS Values:
a=0 : -4.62325 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 14 [14, 14]
Total reward: 2.68837 [-4.62325, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : -4.62325 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 8.77717, average = 9.54216
Undiscounted return = 10, average = 14.2857
Starting run 8 with 2 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 16 steps, with total reward 17.9188
Total reward = 17.0228
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 17.0228 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 21.0155
Total reward = 19.9647
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 19.9647 (1)
a=6 : 17.0228 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 14 [12, 16]
Total reward: 18.4938 [17.0228, 19.9647]
Policy after 2 simulations
MCTS Policy:
a=5 : 19.9647 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 19.9647 (1)
a=6 : 17.0228 (1)
a=7 : 0 (0)
Check 1

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 19.7613
Total reward = 18.7733
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 18.7733 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 12 steps, with total reward 15.688
Total reward = 14.9036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 14.9036 (1)
a=6 : 18.7733 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 14 [12, 16]
Total reward: 16.8384 [14.9036, 18.7733]
Policy after 2 simulations
MCTS Policy:
a=6 : 18.7733 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 14.9036 (1)
a=6 : 18.7733 (1)
a=7 : 0 (0)
Check 2

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 14.4286
Total reward = 13.7072
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 13.7072 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 20 steps, with total reward 18.9615
Total reward = 18.0135
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 18.0135 (1)
a=2 : 13.7072 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 16.5 [13, 20]
Total reward: 15.8603 [13.7072, 18.0135]
Policy after 2 simulations
MCTS Policy:
a=1 : 18.0135 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 18.0135 (1)
a=2 : 13.7072 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# 0$* #
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 19 steps, with total reward -3.76567
Total reward = -3.57738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -3.57738 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -3.57738 (1)
a=3 : 0 (0)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 10.5 [2, 19]
Total reward: 7.72381 [-3.57738, 19.025]
Policy after 2 simulations
MCTS Policy:
a=4 : 19.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -3.57738 (1)
a=3 : 0 (0)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Sample

# # # # 
# 0$* #
# 2$. #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 14.4286
Total reward = 13.7072
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 13.7072 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 36 steps, with total reward 4.89617
Total reward = 4.65136
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 13.7072 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 4.65136 (1)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 24.5 [13, 36]
Total reward: 9.17927 [4.65136, 13.7072]
Policy after 2 simulations
MCTS Policy:
a=2 : 13.7072 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 13.7072 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 4.65136 (1)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
S

# # # # 
# 0$. #
# 2$* #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=1 o=0 a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0X. #
# 2$* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 15.208
Total reward = 14.4476
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 14.4476 (1)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X. #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X. #
# 2$* #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 14.4476 (1)
a=6 : -1e+10 (1000000)
a=7 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5 [1, 9]
Total reward: 11.9738 [9.5, 14.4476]
Policy after 2 simulations
MCTS Policy:
a=5 : 14.4476 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 14.4476 (1)
a=6 : -1e+10 (1000000)
a=7 : 9.5 (1)
Check 1

# # # # 
# 0$. #
# 2$* #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=5 o=1 a=6 o=1 a=1 o=0 a=4 o=0 a=2 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$. #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 30 steps, with total reward 9.66616
Total reward = 9.18286
MCTS Values:
a=0 : 9.18286 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$. #
# 2$* #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 9.18286 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 15.5 [1, 30]
Total reward: 9.34143 [9.18286, 9.5]
Policy after 2 simulations
MCTS Policy:
a=5 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : 9.18286 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Check 1

# # # # 
# 0$. #
# 2$* #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 a=6 o=1 a=1 o=0 a=4 o=0 a=2 o=0 a=5 o=1 a=5 o=1 
Starting simulation

# # # # 
# 0$. #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 10.4837
Total reward = 9.95951
MCTS Values:
a=0 : 9.95951 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$. #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# 2$* #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 9.95951 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 18 [18, 18]
Total reward: 9.97976 [9.95951, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 9.95951 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
E

# # # # 
# 0$. #
# 2$* #
# # # # 
Reward 10
Terminated
Discounted return = 15.5571, average = 10.294
Undiscounted return = 20, average = 15
Starting run 9 with 2 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = -2.26219
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 15 steps, with total reward -4.62325
Total reward = -4.39209
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -4.39209 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 10 [5, 15]
Total reward: -3.32714 [-4.39209, -2.26219]
Policy after 2 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -4.39209 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# 0$* #
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# 2X* #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 5.688 (1)
Starting simulation

# # # # 
# 0X* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 5.688 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11 [11, 11]
Total reward: 7.844 [5.688, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 5.688 (1)
E

# # # # 
# 0$* #
# 2$. #
# # # # 
Reward 10
Terminated
Discounted return = 9.5, average = 10.2058
Undiscounted return = 10, average = 14.4444
Starting run 10 with 2 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 26 steps, with total reward 9.97463
Total reward = 9.4759
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 9.4759 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# 2$* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 9.4759 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 14.5 [3, 26]
Total reward: 9.02482 [8.57375, 9.4759]
Policy after 2 simulations
MCTS Policy:
a=5 : 9.4759 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 9.4759 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Check 1

# # # # 
# * 1$#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 7.49823
Total reward = 7.12332
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.12332 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 9 steps, with total reward -1.51086
Total reward = -11.4353
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -11.4353 (1)
a=5 : 7.12332 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 10 [9, 11]
Total reward: -2.156 [-11.4353, 7.12332]
Policy after 2 simulations
MCTS Policy:
a=5 : 7.12332 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -11.4353 (1)
a=5 : 7.12332 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Check 1

# # # # 
# * 1$#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=5 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = -1.42625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 9 steps, with total reward -2.3908
Total reward = -2.27126
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : -2.27126 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [3, 9]
Total reward: -1.84875 [-2.27126, -1.42625]
Policy after 2 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : -2.27126 (1)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# 0X* #
# 2X. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=5 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 25 steps, with total reward -1.49682
Total reward = -1.42198
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.42198 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# 2$. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# 2$* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = -2.26219
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.42198 (1)
a=3 : 0 (0)
a=4 : -2.26219 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 15 [5, 25]
Total reward: -1.84208 [-2.26219, -1.42198]
Policy after 2 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.42198 (1)
a=3 : 0 (0)
a=4 : -2.26219 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
E

# # # # 
# 0X* #
# 2X. #
# # # # 
Reward 10
Terminated
Discounted return = 8.57375, average = 10.0426
Undiscounted return = 10, average = 14
Simulations = 2
Runs = 10
Undiscounted return = 14 +- 3.2249
Discounted return = 10.0426 +- 2.71127
Time = 0.0037944
Starting run 1 with 4 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 9 steps, with total reward -1.51086
Total reward = 8.56468
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 8.56468 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 23 steps, with total reward -3.39887
Total reward = -3.22893
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -3.22893 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 8.56468 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# 2$* #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -3.22893 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 8.56468 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 4.88496
Total reward = 4.64072
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -3.22893 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 8.56468 (1)
a=5 : 4.64072 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 13.5 [4, 23]
Total reward: 4.53038 [-3.22893, 8.56468]
Policy after 4 simulations
MCTS Policy:
a=4 : 8.56468 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -3.22893 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 8.56468 (1)
a=5 : 4.64072 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Sample

# # # # 
# * 1X#
# 2$. #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 14.1812
Total reward = 13.4721
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 13.4721 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 23 steps, with total reward 12.2603
Total reward = 11.6473
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 13.4721 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 11.6473 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 13.4721 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.13342 (1)
a=7 : 11.6473 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 13.4721 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.13342 (1)
a=7 : 11.6473 (1)
Tree depth: 0 [0, 0]
Rollout depth: 15.75 [9, 23]
Total reward: 9.13884 [5.13342, 13.4721]
Policy after 4 simulations
MCTS Policy:
a=2 : 13.4721 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 13.4721 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.13342 (1)
a=7 : 11.6473 (1)
S

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 13.6176
Total reward = 22.9367
MCTS Values:
a=0 : 0 (0)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 22.9367 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 19.5097
Total reward = 18.5342
MCTS Values:
a=0 : 0 (0)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 22.9367 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 18.5342 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : 0 (0)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 22.9367 (1)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
a=7 : 18.5342 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9.75 [5, 16]
Total reward: 13.8778 [6.30249, 22.9367]
Policy after 4 simulations
MCTS Policy:
a=4 : 22.9367 (1)
Values after 4 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 22.9367 (1)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
a=7 : 18.5342 (1)
Sample

# # # # 
# . 1X#
# * . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=4 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 4.87675 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 4.87675 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 8.3814 (2)
a=1 o=0 a=0 : 9.5 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 4.87675 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0.25 [0, 1]
Rollout depth: 6.5 [1, 14]
Total reward: 7.24762 [4.87675, 9.025]
Policy after 4 simulations
MCTS Policy:
a=1 : 8.3814 (2)
a=1 o=0 a=0 : 9.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 8.3814 (2)
a=1 o=0 a=0 : 9.5 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 4.87675 (1)
a=7 : -1e+10 (1000000)
E

# # # # 
# . 1X#
# . * #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 13.9774
Total reward = 13.2785
MCTS Values:
a=0 : 13.2785 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : 13.2785 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 6.30249 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 13.2785 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 6.30249 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 13.2785 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 6.30249 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 10 [8, 13]
Total reward: 9.0538 [6.30249, 13.2785]
Policy after 4 simulations
MCTS Policy:
a=0 : 13.2785 (1)
Values after 4 simulations
MCTS Values:
a=0 : 13.2785 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 6.30249 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
a=7 : -1e+10 (1000000)
N

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=4 o=0 a=1 o=0 a=0 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 19.025 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : 19.025 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 36 steps, with total reward 1.66083
Total reward = 1.57779
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 1.57779 (1)
a=4 : 19.025 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 1.57779 (1)
a=4 : 19.025 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 13 [1, 36]
Total reward: 10.0257 [1.57779, 19.025]
Policy after 4 simulations
MCTS Policy:
a=4 : 19.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 1.57779 (1)
a=4 : 19.025 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=4 o=0 a=1 o=0 a=0 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.35092 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.35092 (1)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 7.35092 (1)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 5 [4, 6]
Total reward: 8.874 [7.35092, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 7.35092 (1)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 18.6381, average = 18.6381
Undiscounted return = 20, average = 20
Starting run 2 with 4 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 27.1313
Total reward = 25.7747
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 25.7747 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 16.9834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 25.7747 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 23 steps, with total reward 0.973145
Total reward = 0.924488
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0.924488 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 25.7747 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 32 steps, with total reward -9.77002
Total reward = -9.28152
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0.924488 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : -9.28152 (1)
a=6 : 0 (0)
a=7 : 25.7747 (1)
Tree depth: 0 [0, 0]
Rollout depth: 19.25 [7, 32]
Total reward: 8.60026 [-9.28152, 25.7747]
Policy after 4 simulations
MCTS Policy:
a=7 : 25.7747 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0.924488 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : -9.28152 (1)
a=6 : 0 (0)
a=7 : 25.7747 (1)
Check 3

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=7 o=1 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 10.2804
Total reward = 9.76633
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.76633 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 9.84851
Total reward = 9.35608
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.76633 (1)
a=2 : 9.35608 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 16.3025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.76633 (1)
a=2 : 9.35608 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.3025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 12 steps, with total reward 8.5538
Total reward = 8.12611
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.76633 (1)
a=2 : 9.35608 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.3025 (1)
a=5 : 0 (0)
a=6 : 8.12611 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 14.5 [9, 22]
Total reward: 10.8878 [8.12611, 16.3025]
Policy after 4 simulations
MCTS Policy:
a=4 : 16.3025 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.76633 (1)
a=2 : 9.35608 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.3025 (1)
a=5 : 0 (0)
a=6 : 8.12611 (1)
a=7 : 0 (0)
Sample

# # # # 
# * 1X#
# 2$. #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 39 steps, with total reward 7.11196
Total reward = 6.75636
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.75636 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.75636 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 11.7061
Total reward = 11.1208
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.75636 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.1208 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 11.919
Total reward = 11.3231
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.75636 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.1208 (1)
a=7 : 11.3231 (1)
Tree depth: 0 [0, 0]
Rollout depth: 18 [2, 39]
Total reward: 9.5563 [6.75636, 11.3231]
Policy after 4 simulations
MCTS Policy:
a=7 : 11.3231 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.75636 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.1208 (1)
a=7 : 11.3231 (1)
Check 3

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=7 o=1 a=4 o=0 a=7 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 13.9851
Total reward = 13.2859
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 13.2859 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 32 steps, with total reward 6.91582
Total reward = 6.57003
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 13.2859 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.57003 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 13.2859 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.57003 (1)
a=7 : 6.30249 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 13.2859 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.57003 (1)
a=7 : 6.30249 (1)
Tree depth: 0 [0, 0]
Rollout depth: 12.75 [1, 32]
Total reward: 8.9146 [6.30249, 13.2859]
Policy after 4 simulations
MCTS Policy:
a=2 : 13.2859 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 13.2859 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.57003 (1)
a=7 : 6.30249 (1)
S

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=4 o=0 a=7 o=1 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 1.991
Total reward = 1.89145
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 1.89145 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward -1.84995
Total reward = -1.75745
MCTS Values:
a=0 : 0 (0)
a=1 : -1.75745 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 1.89145 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 9.025 (1)
a=1 : -1.75745 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 1.89145 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 15.208
Total reward = 4.44756
MCTS Values:
a=0 : 9.025 (1)
a=1 : -1.75745 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 4.44756 (1)
a=5 : -1e+10 (1000000)
a=6 : 1.89145 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 10 [2, 15]
Total reward: 3.40164 [-1.75745, 9.025]
Policy after 4 simulations
MCTS Policy:
a=0 : 9.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : 9.025 (1)
a=1 : -1.75745 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 4.44756 (1)
a=5 : -1e+10 (1000000)
a=6 : 1.89145 (1)
a=7 : 0 (0)
N

# # # # 
# * 1X#
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=4 o=0 a=7 o=1 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 21.5445
Total reward = 20.4673
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 20.4673 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 20.4673 (1)
a=7 : 7.35092 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 20.4673 (1)
a=7 : 7.35092 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 15.6592
Total reward = 14.8762
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.8762 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 20.4673 (1)
a=7 : 7.35092 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7.25 [3, 11]
Total reward: 12.817 [7.35092, 20.4673]
Policy after 4 simulations
MCTS Policy:
a=6 : 20.4673 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.8762 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 20.4673 (1)
a=7 : 7.35092 (1)
Check 2

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=7 o=1 a=4 o=0 a=7 o=1 a=2 o=0 a=0 o=0 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 17.2378
Total reward = 16.3759
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 16.3759 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 30 steps, with total reward 10.8331
Total reward = 10.2915
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.2915 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 16.3759 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 18.1451
Total reward = 17.2378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.2915 (1)
a=2 : 17.2378 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 16.3759 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.2915 (1)
a=2 : 17.2378 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : 16.3759 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11.5 [5, 30]
Total reward: 12.9107 [7.73781, 17.2378]
Policy after 4 simulations
MCTS Policy:
a=2 : 17.2378 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.2915 (1)
a=2 : 17.2378 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : 16.3759 (1)
S

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=4 o=0 a=7 o=1 a=2 o=0 a=0 o=0 a=6 o=2 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 22.1146
Total reward = 21.0088
MCTS Values:
a=0 : 21.0088 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.40127
MCTS Values:
a=0 : 21.0088 (1)
a=1 : 4.40127 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 21 steps, with total reward 9.88735
Total reward = 9.39299
MCTS Values:
a=0 : 21.0088 (1)
a=1 : 4.40127 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : -1e+10 (1000000)
a=6 : 9.39299 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 13.5 [2, 21]
Total reward: 13.457 [4.40127, 21.0088]
Policy after 4 simulations
MCTS Policy:
a=0 : 21.0088 (1)
Values after 4 simulations
MCTS Values:
a=0 : 21.0088 (1)
a=1 : 4.40127 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : -1e+10 (1000000)
a=6 : 9.39299 (1)
a=7 : 0 (0)
N

# # # # 
# * 1X#
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=4 o=0 a=7 o=1 a=2 o=0 a=0 o=0 a=6 o=2 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 7.35092 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.40127
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.40127 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 7.35092 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.40127 (1)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 7.35092 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.40127 (1)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
a=7 : 7.35092 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9.25 [2, 16]
Total reward: 6.47765 [4.40127, 9.025]
Policy after 4 simulations
MCTS Policy:
a=6 : 9.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.40127 (1)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
a=7 : 7.35092 (1)
Check 2

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=7 o=1 a=4 o=0 a=7 o=1 a=2 o=0 a=0 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.688 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward -2.60439
Total reward = -2.47417
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.47417 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.688 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward -2.60439
Total reward = -2.47417
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.47417 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.688 (1)
a=7 : -2.47417 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward -2.51663
Total reward = -2.3908
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.47417 (1)
a=2 : -2.3908 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.688 (1)
a=7 : -2.47417 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11.75 [8, 14]
Total reward: -0.412783 [-2.47417, 5.688]
Policy after 4 simulations
MCTS Policy:
a=6 : 5.688 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.47417 (1)
a=2 : -2.3908 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.688 (1)
a=7 : -2.47417 (1)
Check 2

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=4 o=0 a=7 o=1 a=2 o=0 a=0 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 11 steps, with total reward -2.58638
Total reward = -2.45706
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -2.45706 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -2.45706 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
a=7 : -2.45706 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 15 steps, with total reward -0.256671
Total reward = -0.243837
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : -0.243837 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
a=7 : -2.45706 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7.75 [2, 15]
Total reward: 3.72446 [-2.45706, 9.025]
Policy after 4 simulations
MCTS Policy:
a=6 : 9.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : -0.243837 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
a=7 : -2.45706 (1)
Check 2

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=7 o=1 a=4 o=0 a=7 o=1 a=2 o=0 a=0 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 2.71007
Total reward = 2.57457
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 2.57457 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 2.57457 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 2.57457 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 2.57457 (1)
a=7 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7.25 [2, 22]
Total reward: 7.29958 [2.57457, 9.025]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 2.57457 (1)
a=7 : 8.57375 (1)
S

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=4 o=0 a=7 o=1 a=2 o=0 a=0 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 a=6 o=2 a=6 o=2 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 16 steps, with total reward 20.6413
Total reward = 19.6092
MCTS Values:
a=0 : 19.6092 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 19.6092 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 6.6342 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 19.6092 (1)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 6.6342 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 15.1334
MCTS Values:
a=0 : 19.6092 (1)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 15.1334 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 6.6342 (1)
Tree depth: 0 [0, 0]
Rollout depth: 10.25 [4, 16]
Total reward: 12.3805 [6.6342, 19.6092]
Policy after 4 simulations
MCTS Policy:
a=0 : 19.6092 (1)
Values after 4 simulations
MCTS Values:
a=0 : 19.6092 (1)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 15.1334 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 6.6342 (1)
N

# # # # 
# * 1X#
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=4 o=0 a=7 o=1 a=2 o=0 a=0 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 a=6 o=2 a=6 o=2 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 7.10708
Total reward = 6.75173
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 6.75173 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
a=7 : 6.75173 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 15.9874
Total reward = 15.188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 15.188 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
a=7 : 6.75173 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 8.80922
Total reward = 8.36876
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.36876 (1)
a=2 : 15.188 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
a=7 : 6.75173 (1)
Tree depth: 0 [0, 0]
Rollout depth: 14.5 [9, 22]
Total reward: 9.15274 [6.30249, 15.188]
Policy after 4 simulations
MCTS Policy:
a=2 : 15.188 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.36876 (1)
a=2 : 15.188 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
a=7 : 6.75173 (1)
S

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=4 o=0 a=7 o=1 a=2 o=0 a=0 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 a=6 o=2 a=6 o=2 a=2 o=0 a=0 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 19 steps, with total reward -0.660769
Total reward = -0.627731
MCTS Values:
a=0 : -0.627731 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 17.2378
Total reward = 16.3759
MCTS Values:
a=0 : -0.627731 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 16.3759 (1)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 15.6592
Total reward = 14.8762
MCTS Values:
a=0 : -0.627731 (1)
a=1 : 14.8762 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 16.3759 (1)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -0.627731 (1)
a=1 : 14.8762 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 16.3759 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9 [2, 19]
Total reward: 12.4124 [-0.627731, 19.025]
Policy after 4 simulations
MCTS Policy:
a=4 : 19.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : -0.627731 (1)
a=1 : 14.8762 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 16.3759 (1)
Sample

# # # # 
# . 1X#
# * . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=4 o=0 a=7 o=1 a=2 o=0 a=0 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 a=6 o=2 a=6 o=2 a=2 o=0 a=0 o=0 a=2 o=0 a=4 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward -3.89158
Total reward = -3.697
MCTS Values:
a=0 : 0 (0)
a=1 : -3.697 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : 6.98337 (1)
a=1 : -3.697 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : 6.98337 (1)
a=1 : -3.697 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=7 o=1 a=4 o=0 a=7 o=1 a=2 o=0 a=0 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 a=6 o=2 a=6 o=2 a=2 o=0 a=0 o=0 a=2 o=0 a=4 o=0 a=0 o=0 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : 7.36059 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 8.14506 (1)
a=0 o=0 a=7 : -1e+10 (1000000)
a=1 : -3.697 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0.25 [0, 1]
Rollout depth: 8.5 [4, 14]
Total reward: 4.33167 [-3.697, 7.73781]
Policy after 4 simulations
MCTS Policy:
a=0 : 7.36059 (2)
a=0 o=0 a=6 : 8.14506 (1)
Values after 4 simulations
MCTS Values:
a=0 : 7.36059 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 8.14506 (1)
a=0 o=0 a=7 : -1e+10 (1000000)
a=1 : -3.697 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
a=7 : -1e+10 (1000000)
N

# # # # 
# * 1X#
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=4 o=0 a=7 o=1 a=2 o=0 a=0 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 a=6 o=2 a=6 o=2 a=2 o=0 a=0 o=0 a=2 o=0 a=4 o=0 a=0 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.35092 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.35092 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 7.35092 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=7 o=1 a=4 o=0 a=7 o=1 a=2 o=0 a=0 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 a=6 o=2 a=6 o=2 a=2 o=0 a=0 o=0 a=2 o=0 a=4 o=0 a=0 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 9.5 (1)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : 7.35092 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0.25 [0, 1]
Rollout depth: 2.5 [1, 6]
Total reward: 8.72523 [7.35092, 9.5]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.2625 (2)
a=1 o=0 a=6 : 9.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 9.5 (1)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : 7.35092 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
a=7 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=4 o=0 a=7 o=1 a=2 o=0 a=0 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 a=6 o=2 a=6 o=2 a=2 o=0 a=0 o=0 a=2 o=0 a=4 o=0 a=0 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -0.428688
Total reward = -0.407253
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.407253 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 22 steps, with total reward 3.40562
Total reward = 3.23534
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.407253 (1)
a=3 : 3.23534 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.407253 (1)
a=3 : 3.23534 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = -0.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.407253 (1)
a=3 : 3.23534 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 8.25 [1, 22]
Total reward: 2.51647 [-0.5, 7.73781]
Policy after 4 simulations
MCTS Policy:
a=6 : 7.73781 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.407253 (1)
a=3 : 3.23534 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : -1e+10 (1000000)
Check 2

# # # # 
# . * #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=4 o=0 a=7 o=1 a=2 o=0 a=0 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 a=6 o=2 a=6 o=2 a=2 o=0 a=0 o=0 a=2 o=0 a=4 o=0 a=0 o=0 a=1 o=0 a=6 o=2 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -2.14908
Total reward = -2.04163
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -2.04163 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -2.04163 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward -0.5
Total reward = -0.475
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -2.04163 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : -0.475 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = -3.3658
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -2.04163 (1)
a=4 : -3.3658 (1)
a=5 : -1e+10 (1000000)
a=6 : -0.475 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 5.66667 [2, 8]
Total reward: 1.02939 [-3.3658, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -2.04163 (1)
a=4 : -3.3658 (1)
a=5 : -1e+10 (1000000)
a=6 : -0.475 (1)
a=7 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 18.3489, average = 18.4935
Undiscounted return = 30, average = 25
Starting run 3 with 4 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 21 steps, with total reward -8.96103
Total reward = -8.51298
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -8.51298 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 3 steps, with total reward -0.475
Total reward = -10.4512
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.4512 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -8.51298 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Ending rollout after 20 steps, with total reward 3.77354
Total reward = 3.58486
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.4512 (1)
a=5 : 0 (0)
a=6 : 3.58486 (1)
a=7 : -8.51298 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 17 steps, with total reward -8.27011
Total reward = -7.8566
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -7.8566 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.4512 (1)
a=5 : 0 (0)
a=6 : 3.58486 (1)
a=7 : -8.51298 (1)
Tree depth: 0 [0, 0]
Rollout depth: 15.25 [3, 21]
Total reward: -5.80899 [-10.4512, 3.58486]
Policy after 4 simulations
MCTS Policy:
a=6 : 3.58486 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -7.8566 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.4512 (1)
a=5 : 0 (0)
a=6 : 3.58486 (1)
a=7 : -8.51298 (1)
Check 2

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# 2X* #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.13342 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# 2X* #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.13342 (1)
a=6 : 0 (0)
a=7 : 5.98737 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# 2X* #
# # # # 
Ending rollout after 12 steps, with total reward -2.45706
Total reward = -2.33421
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -2.33421 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.13342 (1)
a=6 : 0 (0)
a=7 : 5.98737 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward -3.337
Total reward = -13.1701
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -2.33421 (1)
a=3 : -1e+10 (1000000)
a=4 : -13.1701 (1)
a=5 : 5.13342 (1)
a=6 : 0 (0)
a=7 : 5.98737 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11.75 [10, 13]
Total reward: -1.09589 [-13.1701, 5.98737]
Policy after 4 simulations
MCTS Policy:
a=7 : 5.98737 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -2.33421 (1)
a=3 : -1e+10 (1000000)
a=4 : -13.1701 (1)
a=5 : 5.13342 (1)
a=6 : 0 (0)
a=7 : 5.98737 (1)
Check 3

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=6 o=2 a=7 o=1 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Ending rollout after 9 steps, with total reward -1.10361
Total reward = -1.04842
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1.04842 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 31 steps, with total reward -1.08895
Total reward = -11.0345
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -11.0345 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1.04842 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 11 steps, with total reward -13.0376
Total reward = -12.3857
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -12.3857 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -11.0345 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1.04842 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 30 steps, with total reward -5.53527
Total reward = -5.25851
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -12.3857 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -11.0345 (1)
a=5 : -5.25851 (1)
a=6 : 0 (0)
a=7 : -1.04842 (1)
Tree depth: 0 [0, 0]
Rollout depth: 20.25 [9, 31]
Total reward: -7.43179 [-12.3857, -1.04842]
Policy after 4 simulations
MCTS Policy:
a=2 : 0 (0)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -12.3857 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -11.0345 (1)
a=5 : -5.25851 (1)
a=6 : 0 (0)
a=7 : -1.04842 (1)
S

# # # # 
# 0$1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=7 o=1 a=2 o=0 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 16.9834
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 34 steps, with total reward 5.19791
Total reward = 4.93801
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 4.93801 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 4 steps, with total reward -0.92625
Total reward = -0.879938
MCTS Values:
a=0 : 0 (0)
a=1 : -0.879938 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 4.93801 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# 2$* #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : 0 (0)
a=1 : -0.879938 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 4.93801 (1)
a=6 : 5.98737 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 13.75 [4, 34]
Total reward: 6.7572 [-0.879938, 16.9834]
Policy after 4 simulations
MCTS Policy:
a=4 : 16.9834 (1)
Values after 4 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : -0.879938 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 4.93801 (1)
a=6 : 5.98737 (1)
a=7 : 0 (0)
Sample

# # # # 
# 0$1X#
# * . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=7 o=1 a=2 o=0 a=4 o=0 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 7.4061
Total reward = 7.03579
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.03579 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 0 (0)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.03579 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.03579 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 19 steps, with total reward 10.9555
Total reward = 10.4077
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.03579 (1)
a=6 : 10.4077 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 8.5 [2, 19]
Total reward: 8.76057 [7.03579, 10.4077]
Policy after 4 simulations
MCTS Policy:
a=6 : 10.4077 (1)
Values after 4 simulations
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.03579 (1)
a=6 : 10.4077 (1)
a=7 : -1e+10 (1000000)
Check 2

# # # # 
# 0$1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=7 o=1 a=2 o=0 a=4 o=0 a=6 o=2 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.87675 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.87675 (1)
a=6 : 8.14506 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward -2.60439
Total reward = -2.47417
MCTS Values:
a=0 : 0 (0)
a=1 : -2.47417 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.87675 (1)
a=6 : 8.14506 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward -1.42625
Total reward = -1.35494
MCTS Values:
a=0 : -1.35494 (1)
a=1 : -2.47417 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.87675 (1)
a=6 : 8.14506 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 9 [4, 14]
Total reward: 2.29818 [-2.47417, 8.14506]
Policy after 4 simulations
MCTS Policy:
a=6 : 8.14506 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1.35494 (1)
a=1 : -2.47417 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.87675 (1)
a=6 : 8.14506 (1)
a=7 : -1e+10 (1000000)
Check 2

# # # # 
# 0$1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=7 o=1 a=2 o=0 a=4 o=0 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 13.2785
Total reward = 12.6146
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 12.6146 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 23 steps, with total reward 5.51772
Total reward = 5.24183
MCTS Values:
a=0 : 5.24183 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 12.6146 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 7.38864
Total reward = 7.01921
MCTS Values:
a=0 : 5.24183 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.01921 (1)
a=6 : 12.6146 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward -0.315125
Total reward = -0.299368
MCTS Values:
a=0 : 5.24183 (1)
a=1 : -0.299368 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.01921 (1)
a=6 : 12.6146 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 14.25 [9, 23]
Total reward: 6.14406 [-0.299368, 12.6146]
Policy after 4 simulations
MCTS Policy:
a=6 : 12.6146 (1)
Values after 4 simulations
MCTS Values:
a=0 : 5.24183 (1)
a=1 : -0.299368 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.01921 (1)
a=6 : 12.6146 (1)
a=7 : -1e+10 (1000000)
Check 2

# # # # 
# 0$1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=6 o=2 a=7 o=1 a=2 o=0 a=4 o=0 a=6 o=2 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 25 steps, with total reward 5.25024
Total reward = 4.98773
MCTS Values:
a=0 : 4.98773 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 4.98773 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 8.63811
Total reward = 8.2062
MCTS Values:
a=0 : 4.98773 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 8.2062 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward -0.45125
Total reward = -0.428688
MCTS Values:
a=0 : 4.98773 (1)
a=1 : -0.428688 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 8.2062 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 10.25 [4, 25]
Total reward: 5.12576 [-0.428688, 8.2062]
Policy after 4 simulations
MCTS Policy:
a=6 : 8.2062 (1)
Values after 4 simulations
MCTS Values:
a=0 : 4.98773 (1)
a=1 : -0.428688 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 8.2062 (1)
a=7 : -1e+10 (1000000)
Check 2

# # # # 
# 0$1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=6 o=2 a=7 o=1 a=2 o=0 a=4 o=0 a=6 o=2 a=6 o=2 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 15.1284
Total reward = 14.372
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 14.372 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 39 steps, with total reward 1.86233
Total reward = 1.76921
MCTS Values:
a=0 : 0 (0)
a=1 : 1.76921 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 14.372 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 13.6579
Total reward = 12.975
MCTS Values:
a=0 : 12.975 (1)
a=1 : 1.76921 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 14.372 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 23 steps, with total reward 8.63894
Total reward = 8.20699
MCTS Values:
a=0 : 12.975 (1)
a=1 : 1.76921 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.20699 (1)
a=6 : 14.372 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 21.5 [8, 39]
Total reward: 9.33081 [1.76921, 14.372]
Policy after 4 simulations
MCTS Policy:
a=6 : 14.372 (1)
Values after 4 simulations
MCTS Values:
a=0 : 12.975 (1)
a=1 : 1.76921 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.20699 (1)
a=6 : 14.372 (1)
a=7 : -1e+10 (1000000)
Check 2

# # # # 
# 0$1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=7 o=1 a=2 o=0 a=4 o=0 a=6 o=2 a=6 o=2 a=6 o=2 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 19 steps, with total reward 3.97214
Total reward = 3.77354
MCTS Values:
a=0 : 3.77354 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 3.77354 (1)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 3.77354 (1)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.35092 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : 3.77354 (1)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 5.4036 (1)
a=6 : 7.35092 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 10.75 [6, 19]
Total reward: 5.96974 [3.77354, 7.35092]
Policy after 4 simulations
MCTS Policy:
a=1 : 7.35092 (1)
Values after 4 simulations
MCTS Values:
a=0 : 3.77354 (1)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 5.4036 (1)
a=6 : 7.35092 (1)
a=7 : -1e+10 (1000000)
E

# # # # 
# 0$1X#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=7 o=1 a=2 o=0 a=4 o=0 a=6 o=2 a=6 o=2 a=6 o=2 a=6 o=2 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 5.50835
Total reward = 5.23293
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 5.23293 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 5.23293 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 19 steps, with total reward 2.99714
Total reward = 2.84729
MCTS Values:
a=0 : 2.84729 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 5.23293 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward -0.583769
Total reward = -0.55458
MCTS Values:
a=0 : 2.84729 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : -0.55458 (1)
a=4 : -1e+10 (1000000)
a=5 : 5.23293 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 14 [10, 19]
Total reward: 4.38141 [-0.55458, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : 2.84729 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : -0.55458 (1)
a=4 : -1e+10 (1000000)
a=5 : 5.23293 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
E

# # # # 
# 0$1X#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 14.5611, average = 17.1827
Undiscounted return = 20, average = 23.3333
Starting run 4 with 4 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 6 steps, with total reward -10.3359
Total reward = -9.81914
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -9.81914 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# 2X* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -9.81914 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 13.5487
Total reward = 12.8712
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 12.8712 (1)
a=6 : 0 (0)
a=7 : -9.81914 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6.75 [3, 13]
Total reward: 4.84091 [-9.81914, 12.8712]
Policy after 4 simulations
MCTS Policy:
a=5 : 12.8712 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 12.8712 (1)
a=6 : 0 (0)
a=7 : -9.81914 (1)
Check 1

# # # # 
# * 1$#
# 2X. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# 2$* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 7.73781 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 24.372
Total reward = 23.1534
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 7.73781 (1)
a=7 : 23.1534 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 7.73781 (1)
a=7 : 23.1534 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 17.7378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 0 (0)
a=6 : 7.73781 (1)
a=7 : 23.1534 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5 [1, 9]
Total reward: 14.5323 [7.73781, 23.1534]
Policy after 4 simulations
MCTS Policy:
a=7 : 23.1534 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 0 (0)
a=6 : 7.73781 (1)
a=7 : 23.1534 (1)
Check 3

# # # # 
# * 1$#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=7 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward -6.89088
Total reward = 3.45367
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 3.45367 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 23 steps, with total reward 1.57242
Total reward = 1.4938
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 1.4938 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 3.45367 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -1.75044
Total reward = -1.66292
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 1.4938 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 3.45367 (1)
a=5 : -1.66292 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Ending rollout after 12 steps, with total reward -2.88575
Total reward = -2.74146
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 1.4938 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 3.45367 (1)
a=5 : -1.66292 (1)
a=6 : -2.74146 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 15.25 [11, 23]
Total reward: 0.135771 [-2.74146, 3.45367]
Policy after 4 simulations
MCTS Policy:
a=4 : 3.45367 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 1.4938 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 3.45367 (1)
a=5 : -1.66292 (1)
a=6 : -2.74146 (1)
a=7 : 0 (0)
Sample

# # # # 
# * 1$#
# 2X. #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=7 o=2 a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 7 steps, with total reward 15.9247
Total reward = 15.1284
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.1284 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.1284 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 28 steps, with total reward -2.83497
Total reward = -2.69322
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.1284 (1)
a=2 : -2.69322 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 1.03702
Total reward = 0.985171
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.1284 (1)
a=2 : -2.69322 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
a=7 : 0.985171 (1)
Tree depth: 0 [0, 0]
Rollout depth: 13.25 [4, 28]
Total reward: 5.39136 [-2.69322, 15.1284]
Policy after 4 simulations
MCTS Policy:
a=1 : 15.1284 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.1284 (1)
a=2 : -2.69322 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
a=7 : 0.985171 (1)
E

# # # # 
# . * #
# 2X. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=7 o=2 a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = -2.64908
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -2.64908 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 2 steps, with total reward -0.5
Total reward = -0.475
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -2.64908 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -0.475 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 24 steps, with total reward -4.66424
Total reward = -4.43103
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -4.43103 (1)
a=4 : -2.64908 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -0.475 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -4.43103 (1)
a=4 : -2.64908 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -0.475 (1)
Tree depth: 0 [0, 0]
Rollout depth: 10.6667 [2, 24]
Total reward: 0.611223 [-4.43103, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -4.43103 (1)
a=4 : -2.64908 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -0.475 (1)
E

# # # # 
# . * #
# 2X. #
# # # # 
Reward 10
Terminated
Discounted return = 17.1701, average = 17.1795
Undiscounted return = 20, average = 22.5
Starting run 5 with 4 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 10 steps, with total reward 21.0237
Total reward = 19.9725
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 19.9725 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 12 steps, with total reward 21.1754
Total reward = 20.1166
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 19.9725 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 20.1166 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 16 steps, with total reward 11.2671
Total reward = 10.7038
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.7038 (1)
a=2 : 19.9725 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 20.1166 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 30 steps, with total reward 8.94669
Total reward = 8.49936
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.7038 (1)
a=2 : 19.9725 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 20.1166 (1)
a=6 : 8.49936 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 17 [10, 30]
Total reward: 14.8231 [8.49936, 20.1166]
Policy after 4 simulations
MCTS Policy:
a=5 : 20.1166 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.7038 (1)
a=2 : 19.9725 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 20.1166 (1)
a=6 : 8.49936 (1)
a=7 : 0 (0)
Check 1

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.30249 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 16.9834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 6.30249 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 6.30249 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 24.5611
Total reward = 23.3331
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 6.30249 (1)
a=6 : 23.3331 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [1, 11]
Total reward: 14.0297 [6.30249, 23.3331]
Policy after 4 simulations
MCTS Policy:
a=6 : 23.3331 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 6.30249 (1)
a=6 : 23.3331 (1)
a=7 : 0 (0)
Check 2

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# 2X* #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.35092 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Ending rollout after 5 steps, with total reward 17.1701
Total reward = 16.3116
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 16.3116 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.35092 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 5.67687
Total reward = 5.39303
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 16.3116 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.35092 (1)
a=6 : 5.39303 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 34 steps, with total reward 10.3245
Total reward = 9.8083
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 16.3116 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.35092 (1)
a=6 : 5.39303 (1)
a=7 : 9.8083 (1)
Tree depth: 0 [0, 0]
Rollout depth: 16.75 [5, 34]
Total reward: 9.71595 [5.39303, 16.3116]
Policy after 4 simulations
MCTS Policy:
a=2 : 16.3116 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 16.3116 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.35092 (1)
a=6 : 5.39303 (1)
a=7 : 9.8083 (1)
S

# # # # 
# 0$1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 4 steps, with total reward 17.5987
Total reward = 16.7188
MCTS Values:
a=0 : 0 (0)
a=1 : 16.7188 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 28 steps, with total reward 22.3244
Total reward = 21.2081
MCTS Values:
a=0 : 0 (0)
a=1 : 16.7188 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 21.2081 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 7 steps, with total reward 16.8509
Total reward = 16.0084
MCTS Values:
a=0 : 0 (0)
a=1 : 16.7188 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 16.0084 (1)
a=7 : 21.2081 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 20 steps, with total reward 17.3222
Total reward = 16.4561
MCTS Values:
a=0 : 16.4561 (1)
a=1 : 16.7188 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 16.0084 (1)
a=7 : 21.2081 (1)
Tree depth: 0 [0, 0]
Rollout depth: 14.75 [4, 28]
Total reward: 17.5979 [16.0084, 21.2081]
Policy after 4 simulations
MCTS Policy:
a=7 : 21.2081 (1)
Values after 4 simulations
MCTS Values:
a=0 : 16.4561 (1)
a=1 : 16.7188 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 16.0084 (1)
a=7 : 21.2081 (1)
Check 3

# # # # 
# 0$1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 a=7 o=1 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 7 steps, with total reward -0.38689
Total reward = -0.367546
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -0.367546 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 25 steps, with total reward 5.06943
Total reward = 4.81596
MCTS Values:
a=0 : 4.81596 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -0.367546 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 20.2042
Total reward = 19.194
MCTS Values:
a=0 : 4.81596 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 19.194 (1)
a=6 : 0 (0)
a=7 : -0.367546 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 13.9774
Total reward = 13.2785
MCTS Values:
a=0 : 4.81596 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 19.194 (1)
a=6 : 13.2785 (1)
a=7 : -0.367546 (1)
Tree depth: 0 [0, 0]
Rollout depth: 15 [7, 25]
Total reward: 9.23023 [-0.367546, 19.194]
Policy after 4 simulations
MCTS Policy:
a=5 : 19.194 (1)
Values after 4 simulations
MCTS Values:
a=0 : 4.81596 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 19.194 (1)
a=6 : 13.2785 (1)
a=7 : -0.367546 (1)
Check 1

# # # # 
# 0$1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 a=7 o=1 a=5 o=1 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 15.688
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 15.688 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 14 steps, with total reward 12.4843
Total reward = 11.8601
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 15.688 (1)
a=5 : 0 (0)
a=6 : 11.8601 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 30 steps, with total reward 5.66497
Total reward = 5.38172
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 15.688 (1)
a=5 : 5.38172 (1)
a=6 : 11.8601 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 20.6993
Total reward = 19.6643
MCTS Values:
a=0 : 19.6643 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 15.688 (1)
a=5 : 5.38172 (1)
a=6 : 11.8601 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 18.25 [11, 30]
Total reward: 13.1485 [5.38172, 19.6643]
Policy after 4 simulations
MCTS Policy:
a=0 : 19.6643 (1)
Values after 4 simulations
MCTS Values:
a=0 : 19.6643 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 15.688 (1)
a=5 : 5.38172 (1)
a=6 : 11.8601 (1)
a=7 : 0 (0)
N

# # # # 
# * 1$#
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 a=7 o=1 a=5 o=1 a=0 o=0 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 25.6592
Total reward = 24.3762
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 24.3762 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 24.3762 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 28 steps, with total reward 2.76435
Total reward = 2.62613
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 2.62613 (1)
a=2 : 24.3762 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 27 steps, with total reward 11.0162
Total reward = 10.4653
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 2.62613 (1)
a=2 : 24.3762 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 10.4653 (1)
Tree depth: 0 [0, 0]
Rollout depth: 16.5 [2, 28]
Total reward: 14.1232 [2.62613, 24.3762]
Policy after 4 simulations
MCTS Policy:
a=2 : 24.3762 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 2.62613 (1)
a=2 : 24.3762 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 10.4653 (1)
S

# # # # 
# 0$1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 a=7 o=1 a=5 o=1 a=0 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 31 steps, with total reward -1.27942
Total reward = -1.21545
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1.21545 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 7 steps, with total reward -0.794144
Total reward = -0.754436
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.754436 (1)
a=6 : -1.21545 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 29 steps, with total reward -7.49127
Total reward = 2.88329
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 2.88329 (1)
a=5 : -0.754436 (1)
a=6 : -1.21545 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 16.9834
Total reward = 16.1342
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 2.88329 (1)
a=5 : -0.754436 (1)
a=6 : -1.21545 (1)
a=7 : 16.1342 (1)
Tree depth: 0 [0, 0]
Rollout depth: 18.75 [7, 31]
Total reward: 4.2619 [-1.21545, 16.1342]
Policy after 4 simulations
MCTS Policy:
a=7 : 16.1342 (1)
Values after 4 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 2.88329 (1)
a=5 : -0.754436 (1)
a=6 : -1.21545 (1)
a=7 : 16.1342 (1)
Check 3

# # # # 
# 0$1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 a=7 o=1 a=5 o=1 a=0 o=0 a=2 o=0 a=7 o=1 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# 2$* #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = 4.1812
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 4.1812 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 30 steps, with total reward 8.56185
Total reward = 8.13376
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.13376 (1)
a=6 : 4.1812 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 25 steps, with total reward 1.55634
Total reward = 1.47852
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.13376 (1)
a=6 : 4.1812 (1)
a=7 : 1.47852 (1)
Tree depth: 0 [0, 0]
Rollout depth: 19.25 [5, 30]
Total reward: 5.38282 [1.47852, 8.13376]
Policy after 4 simulations
MCTS Policy:
a=5 : 8.13376 (1)
Values after 4 simulations
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.13376 (1)
a=6 : 4.1812 (1)
a=7 : 1.47852 (1)
Check 1

# # # # 
# 0$1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 a=7 o=1 a=5 o=1 a=0 o=0 a=2 o=0 a=7 o=1 a=5 o=1 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 7.42268
Total reward = 7.05155
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 7.05155 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# 2$* #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 7.05155 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.14506 (1)
a=7 : 7.05155 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -1.22283
Total reward = 8.83831
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 8.83831 (1)
a=5 : 0 (0)
a=6 : 8.14506 (1)
a=7 : 7.05155 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7 [4, 11]
Total reward: 7.84646 [7.05155, 8.83831]
Policy after 4 simulations
MCTS Policy:
a=4 : 8.83831 (1)
Values after 4 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 8.83831 (1)
a=5 : 0 (0)
a=6 : 8.14506 (1)
a=7 : 7.05155 (1)
Sample

# # # # 
# 0$1$#
# * . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 a=7 o=1 a=5 o=1 a=0 o=0 a=2 o=0 a=7 o=1 a=5 o=1 a=4 o=0 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.35092 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : 5.688 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.35092 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 12.2277
Total reward = 11.6163
MCTS Values:
a=0 : 5.688 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 11.6163 (1)
a=6 : 7.35092 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 32 steps, with total reward 4.95896
Total reward = 4.71101
MCTS Values:
a=0 : 5.688 (1)
a=1 : 4.71101 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 11.6163 (1)
a=6 : 7.35092 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 16 [6, 32]
Total reward: 7.34155 [4.71101, 11.6163]
Policy after 4 simulations
MCTS Policy:
a=5 : 11.6163 (1)
Values after 4 simulations
MCTS Values:
a=0 : 5.688 (1)
a=1 : 4.71101 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 11.6163 (1)
a=6 : 7.35092 (1)
a=7 : -1e+10 (1000000)
Check 1

# # # # 
# 0$1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 a=7 o=1 a=5 o=1 a=0 o=0 a=2 o=0 a=7 o=1 a=5 o=1 a=4 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 8.14506 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 17.5427
Total reward = 16.6655
MCTS Values:
a=0 : 8.14506 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 16.6655 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = 4.1812
MCTS Values:
a=0 : 8.14506 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 16.6655 (1)
a=6 : 4.1812 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 21.6396
Total reward = 20.5576
MCTS Values:
a=0 : 8.14506 (1)
a=1 : 20.5576 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 16.6655 (1)
a=6 : 4.1812 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 13.25 [4, 17]
Total reward: 12.3873 [4.1812, 20.5576]
Policy after 4 simulations
MCTS Policy:
a=1 : 20.5576 (1)
Values after 4 simulations
MCTS Values:
a=0 : 8.14506 (1)
a=1 : 20.5576 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 16.6655 (1)
a=6 : 4.1812 (1)
a=7 : -1e+10 (1000000)
E

# # # # 
# 0$1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 a=7 o=1 a=5 o=1 a=0 o=0 a=2 o=0 a=7 o=1 a=5 o=1 a=4 o=0 a=5 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 10.0893
Total reward = 9.5848
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5848 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 23.1176
Total reward = 21.9617
MCTS Values:
a=0 : 21.9617 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5848 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 15.1284
Total reward = 14.372
MCTS Values:
a=0 : 21.9617 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5848 (1)
a=6 : 14.372 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 21.9617 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5848 (1)
a=6 : 14.372 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 11.3333 [8, 17]
Total reward: 13.9796 [9.5848, 21.9617]
Policy after 4 simulations
MCTS Policy:
a=0 : 21.9617 (1)
Values after 4 simulations
MCTS Values:
a=0 : 21.9617 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5848 (1)
a=6 : 14.372 (1)
a=7 : -1e+10 (1000000)
N

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 a=7 o=1 a=5 o=1 a=0 o=0 a=2 o=0 a=7 o=1 a=5 o=1 a=4 o=0 a=5 o=1 a=1 o=0 a=0 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 19.5 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 19.5 (1)
a=5 : 8.57375 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 13.9851
Total reward = 13.2859
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 13.2859 (1)
a=4 : 19.5 (1)
a=5 : 8.57375 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 13.2859 (1)
a=4 : 19.5 (1)
a=5 : 8.57375 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 3.5 [1, 9]
Total reward: 12.7149 [8.57375, 19.5]
Policy after 4 simulations
MCTS Policy:
a=4 : 19.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 13.2859 (1)
a=4 : 19.5 (1)
a=5 : 8.57375 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Sample

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 a=7 o=1 a=5 o=1 a=0 o=0 a=2 o=0 a=7 o=1 a=5 o=1 a=4 o=0 a=5 o=1 a=1 o=0 a=0 o=0 a=4 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.14506 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 14.4013
Total reward = 13.6812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.14506 (1)
a=3 : 13.6812 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 7.33333 [1, 17]
Total reward: 10.3316 [8.14506, 13.6812]
Policy after 4 simulations
MCTS Policy:
a=3 : 13.6812 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.14506 (1)
a=3 : 13.6812 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
W

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 a=7 o=1 a=5 o=1 a=0 o=0 a=2 o=0 a=7 o=1 a=5 o=1 a=4 o=0 a=5 o=1 a=1 o=0 a=0 o=0 a=4 o=0 a=3 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 15.4874
Total reward = 14.713
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 14.713 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 14.713 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 14.713 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 6.6342 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 16.7628
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.7628 (1)
a=2 : 14.713 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 6.6342 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 6.75 [3, 11]
Total reward: 14.1709 [6.6342, 18.5738]
Policy after 4 simulations
MCTS Policy:
a=4 : 18.5738 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.7628 (1)
a=2 : 14.713 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 6.6342 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 a=7 o=1 a=5 o=1 a=0 o=0 a=2 o=0 a=7 o=1 a=5 o=1 a=4 o=0 a=5 o=1 a=1 o=0 a=0 o=0 a=4 o=0 a=3 o=0 a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 a=7 o=1 a=5 o=1 a=0 o=0 a=2 o=0 a=7 o=1 a=5 o=1 a=4 o=0 a=5 o=1 a=1 o=0 a=0 o=0 a=4 o=0 a=3 o=0 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 8.58503 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=2 o=0 a=7 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 a=7 o=1 a=5 o=1 a=0 o=0 a=2 o=0 a=7 o=1 a=5 o=1 a=4 o=0 a=5 o=1 a=1 o=0 a=0 o=0 a=4 o=0 a=3 o=0 a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.6189 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : -1e+10 (1000000)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : 8.58503 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=2 o=0 a=7 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0.5 [0, 1]
Rollout depth: 3.33333 [1, 5]
Total reward: 8.60197 [7.73781, 9.5]
Policy after 4 simulations
MCTS Policy:
a=1 : 8.6189 (2)
a=1 o=0 a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.6189 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : -1e+10 (1000000)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : 8.58503 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=2 o=0 a=7 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 a=7 o=1 a=5 o=1 a=0 o=0 a=2 o=0 a=7 o=1 a=5 o=1 a=4 o=0 a=5 o=1 a=1 o=0 a=0 o=0 a=4 o=0 a=3 o=0 a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 2.5 [1, 4]
Total reward: 9.41127 [8.14506, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 20.25, average = 17.7936
Undiscounted return = 40, average = 26
Starting run 6 with 4 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Ending rollout after 3 steps, with total reward 19.025
Total reward = 18.0737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 18.0737 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 15 steps, with total reward -1.42574
Total reward = -1.35446
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 18.0737 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1.35446 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 30 steps, with total reward -0.402706
Total reward = -0.38257
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 18.0737 (1)
a=2 : -0.38257 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1.35446 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 22.0639
Total reward = 20.9607
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 18.0737 (1)
a=2 : -0.38257 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 20.9607 (1)
a=6 : 0 (0)
a=7 : -1.35446 (1)
Tree depth: 0 [0, 0]
Rollout depth: 15 [3, 30]
Total reward: 9.32436 [-1.35446, 20.9607]
Policy after 4 simulations
MCTS Policy:
a=5 : 20.9607 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 18.0737 (1)
a=2 : -0.38257 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 20.9607 (1)
a=6 : 0 (0)
a=7 : -1.35446 (1)
Check 1

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 23.2072
Total reward = 22.0468
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 22.0468 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 5 steps, with total reward 27.1701
Total reward = 25.8116
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 25.8116 (1)
a=7 : 22.0468 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 10 steps, with total reward 13.6534
Total reward = 12.9707
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 12.9707 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 25.8116 (1)
a=7 : 22.0468 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 12.9707 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 25.8116 (1)
a=7 : 22.0468 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8.75 [5, 14]
Total reward: 17.045 [7.35092, 25.8116]
Policy after 4 simulations
MCTS Policy:
a=6 : 25.8116 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 12.9707 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 25.8116 (1)
a=7 : 22.0468 (1)
Check 2

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=5 o=1 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 14 steps, with total reward -3.01164
Total reward = 7.13894
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 7.13894 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 19 steps, with total reward 12.9971
Total reward = 12.3473
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 12.3473 (1)
a=3 : -1e+10 (1000000)
a=4 : 7.13894 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 12.0024
Total reward = 11.4023
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 12.3473 (1)
a=3 : -1e+10 (1000000)
a=4 : 7.13894 (1)
a=5 : 11.4023 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 38 steps, with total reward 8.82832
Total reward = 8.3869
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 12.3473 (1)
a=3 : -1e+10 (1000000)
a=4 : 7.13894 (1)
a=5 : 11.4023 (1)
a=6 : 0 (0)
a=7 : 8.3869 (1)
Tree depth: 0 [0, 0]
Rollout depth: 23.25 [14, 38]
Total reward: 9.81885 [7.13894, 12.3473]
Policy after 4 simulations
MCTS Policy:
a=2 : 12.3473 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 12.3473 (1)
a=3 : -1e+10 (1000000)
a=4 : 7.13894 (1)
a=5 : 11.4023 (1)
a=6 : 0 (0)
a=7 : 8.3869 (1)
S

# # # # 
# 0$1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=2 o=0 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 19 steps, with total reward 5.20275
Total reward = 4.94261
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 4.94261 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 7 steps, with total reward -10.7228
Total reward = -10.1867
MCTS Values:
a=0 : -10.1867 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 4.94261 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 11.5321
Total reward = 10.9555
MCTS Values:
a=0 : -10.1867 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 4.94261 (1)
a=6 : 0 (0)
a=7 : 10.9555 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = 14.1812
MCTS Values:
a=0 : -10.1867 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 14.1812 (1)
a=5 : 4.94261 (1)
a=6 : 0 (0)
a=7 : 10.9555 (1)
Tree depth: 0 [0, 0]
Rollout depth: 15.25 [7, 19]
Total reward: 4.97316 [-10.1867, 14.1812]
Policy after 4 simulations
MCTS Policy:
a=4 : 14.1812 (1)
Values after 4 simulations
MCTS Values:
a=0 : -10.1867 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 14.1812 (1)
a=5 : 4.94261 (1)
a=6 : 0 (0)
a=7 : 10.9555 (1)
Sample

# # # # 
# 0$1X#
# * . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=2 o=0 a=4 o=0 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 30 steps, with total reward -1.14626
Total reward = -1.08895
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1.08895 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 0 (0)
a=1 : 6.6342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1.08895 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -9.67736
Total reward = -9.19349
MCTS Values:
a=0 : -9.19349 (1)
a=1 : 6.6342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1.08895 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -9.19349 (1)
a=1 : 6.6342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : -1.08895 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 12.25 [2, 30]
Total reward: 1.34419 [-9.19349, 9.025]
Policy after 4 simulations
MCTS Policy:
a=5 : 9.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : -9.19349 (1)
a=1 : 6.6342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : -1.08895 (1)
a=7 : -1e+10 (1000000)
Check 1

# # # # 
# 0$1X#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=2 o=0 a=4 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 15.9247
Total reward = 15.1284
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 15.1284 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 15.1284
Total reward = 14.372
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 15.1284 (1)
a=6 : 14.372 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 49 steps, with total reward 6.45131
Total reward = 6.12874
MCTS Values:
a=0 : 6.12874 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 15.1284 (1)
a=6 : 14.372 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 6.12874 (1)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 15.1284 (1)
a=6 : 14.372 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 17.25 [5, 49]
Total reward: 10.8418 [6.12874, 15.1284]
Policy after 4 simulations
MCTS Policy:
a=5 : 15.1284 (1)
Values after 4 simulations
MCTS Values:
a=0 : 6.12874 (1)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 15.1284 (1)
a=6 : 14.372 (1)
a=7 : -1e+10 (1000000)
Check 1

# # # # 
# 0$1X#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=2 o=0 a=4 o=0 a=5 o=1 a=5 o=1 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 2.41091
Total reward = 2.29037
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 2.29037 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 33 steps, with total reward -1.67199
Total reward = -1.58839
MCTS Values:
a=0 : -1.58839 (1)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 2.29037 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 14.25 [4, 33]
Total reward: 4.04949 [-1.58839, 8.14506]
Policy after 4 simulations
MCTS Policy:
a=5 : 8.14506 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1.58839 (1)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 2.29037 (1)
a=7 : -1e+10 (1000000)
Check 1

# # # # 
# 0$1X#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=2 o=0 a=4 o=0 a=5 o=1 a=5 o=1 a=5 o=1 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward 22.1586
Total reward = 21.0507
MCTS Values:
a=0 : 21.0507 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 21.0507 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 8.14506 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 21.0507 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 8.14506 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 7.75 [1, 21]
Total reward: 11.6084 [7.73781, 21.0507]
Policy after 4 simulations
MCTS Policy:
a=0 : 21.0507 (1)
Values after 4 simulations
MCTS Values:
a=0 : 21.0507 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 8.14506 (1)
a=7 : -1e+10 (1000000)
N

# # # # 
# * 1X#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=2 o=0 a=4 o=0 a=5 o=1 a=5 o=1 a=5 o=1 a=0 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 18.6732
Total reward = 17.7396
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 17.7396 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 14.0403
Total reward = 13.3383
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 13.3383 (1)
a=6 : 17.7396 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 13.3383 (1)
a=6 : 17.7396 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 17.7378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 13.3383 (1)
a=6 : 17.7396 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 8 [1, 16]
Total reward: 14.5789 [9.5, 17.7396]
Policy after 4 simulations
MCTS Policy:
a=6 : 17.7396 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 13.3383 (1)
a=6 : 17.7396 (1)
a=7 : -1e+10 (1000000)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 9 attempts
Expanding node: a=5 o=1 a=6 o=2 a=2 o=0 a=4 o=0 a=5 o=1 a=5 o=1 a=5 o=1 a=0 o=0 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 8.57375 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 8.57375 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 2.75 [2, 3]
Total reward: 8.68656 [8.57375, 9.025]
Policy after 4 simulations
MCTS Policy:
a=5 : 9.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 8.57375 (1)
a=7 : -1e+10 (1000000)
Check 1

# # # # 
# * 1X#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 a=6 o=2 a=2 o=0 a=4 o=0 a=5 o=1 a=5 o=1 a=5 o=1 a=0 o=0 a=6 o=2 a=5 o=1 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 14.4013
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 14.4013 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 14.4013 (1)
a=5 : 8.14506 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 9.16829
Total reward = 8.70988
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 14.4013 (1)
a=5 : 8.14506 (1)
a=6 : 8.70988 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -0.33171
Total reward = -0.315125
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.315125 (1)
a=3 : -1e+10 (1000000)
a=4 : 14.4013 (1)
a=5 : 8.14506 (1)
a=6 : 8.70988 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 10 [4, 16]
Total reward: 7.73527 [-0.315125, 14.4013]
Policy after 4 simulations
MCTS Policy:
a=4 : 14.4013 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.315125 (1)
a=3 : -1e+10 (1000000)
a=4 : 14.4013 (1)
a=5 : 8.14506 (1)
a=6 : 8.70988 (1)
a=7 : -1e+10 (1000000)
Sample

# # # # 
# * 1X#
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=2 o=0 a=4 o=0 a=5 o=1 a=5 o=1 a=5 o=1 a=0 o=0 a=6 o=2 a=5 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.4036 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.4036 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=5 o=1 a=6 o=2 a=2 o=0 a=4 o=0 a=5 o=1 a=5 o=1 a=5 o=1 a=0 o=0 a=6 o=2 a=5 o=1 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 20 steps, with total reward 3.77354
Total reward = 3.40562
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.4036 (1)
a=2 : 6.21531 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 3.58486 (1)
a=2 o=0 a=7 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0.25 [0, 1]
Rollout depth: 9.25 [2, 20]
Total reward: 6.60199 [3.40562, 9.025]
Policy after 4 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.4036 (1)
a=2 : 6.21531 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 3.58486 (1)
a=2 o=0 a=7 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : -1e+10 (1000000)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 7 attempts
Expanding node: a=5 o=1 a=6 o=2 a=2 o=0 a=4 o=0 a=5 o=1 a=5 o=1 a=5 o=1 a=0 o=0 a=6 o=2 a=5 o=1 a=4 o=0 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -3.03763
Total reward = -2.88575
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -2.88575 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -2.88575 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : -2.88575 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=2 a=2 o=0 a=4 o=0 a=5 o=1 a=5 o=1 a=5 o=1 a=0 o=0 a=6 o=2 a=5 o=1 a=4 o=0 a=6 o=2 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -2.88575 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0.25 [0, 1]
Rollout depth: 5.75 [1, 11]
Total reward: 5.36669 [-2.88575, 9.025]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.025 (2)
a=1 o=0 a=2 : 9.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -2.88575 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
a=7 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=2 o=0 a=4 o=0 a=5 o=1 a=5 o=1 a=5 o=1 a=0 o=0 a=6 o=2 a=5 o=1 a=4 o=0 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 14.6329
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 14.6329 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 6.6342 (1)
a=4 : 14.6329 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.025 (1)
a=3 : 6.6342 (1)
a=4 : 14.6329 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 8.33333 [2, 15]
Total reward: 10.073 [6.6342, 14.6329]
Policy after 4 simulations
MCTS Policy:
a=4 : 14.6329 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.025 (1)
a=3 : 6.6342 (1)
a=4 : 14.6329 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=2 o=0 a=4 o=0 a=5 o=1 a=5 o=1 a=5 o=1 a=0 o=0 a=6 o=2 a=5 o=1 a=4 o=0 a=6 o=2 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.14506 (1)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.14506 (1)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 9.07253 [8.14506, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.14506 (1)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 14.3044, average = 17.2121
Undiscounted return = 20, average = 25
Starting run 7 with 4 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 24 steps, with total reward 12.2507
Total reward = 1.63817
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 1.63817 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# 2X* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# 2X* #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.6342 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 1.63817 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 11 steps, with total reward 14.5611
Total reward = 13.8331
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.6342 (1)
a=2 : 13.8331 (1)
a=3 : -1e+10 (1000000)
a=4 : 1.63817 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# 2$* #
# # # # 
Ending rollout after 6 steps, with total reward 16.7628
Total reward = 15.9247
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.6342 (1)
a=2 : 13.8331 (1)
a=3 : -1e+10 (1000000)
a=4 : 1.63817 (1)
a=5 : 0 (0)
a=6 : 15.9247 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 12.25 [6, 24]
Total reward: 9.50753 [1.63817, 15.9247]
Policy after 4 simulations
MCTS Policy:
a=6 : 15.9247 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.6342 (1)
a=2 : 13.8331 (1)
a=3 : -1e+10 (1000000)
a=4 : 1.63817 (1)
a=5 : 0 (0)
a=6 : 15.9247 (1)
a=7 : 0 (0)
Check 2

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 11 steps, with total reward -1.36355
Total reward = -1.29537
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1.29537 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 18.0737
Total reward = 17.1701
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 17.1701 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1.29537 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Ending rollout after 7 steps, with total reward 15.9247
Total reward = 15.1284
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.1284 (1)
a=2 : 17.1701 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1.29537 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 10 steps, with total reward -3.19751
Total reward = -3.03763
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.1284 (1)
a=2 : 17.1701 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1.29537 (1)
a=7 : -3.03763 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8 [4, 11]
Total reward: 6.99137 [-3.03763, 17.1701]
Policy after 4 simulations
MCTS Policy:
a=2 : 17.1701 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.1284 (1)
a=2 : 17.1701 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1.29537 (1)
a=7 : -3.03763 (1)
S

# # # # 
# 0X1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 22 steps, with total reward 17.7776
Total reward = 26.8887
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 26.8887 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 21.912
Total reward = 20.8164
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 26.8887 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 20.8164 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 17 steps, with total reward 19.8886
Total reward = 18.8942
MCTS Values:
a=0 : 18.8942 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 26.8887 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 20.8164 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 22 steps, with total reward 24.8961
Total reward = 23.6513
MCTS Values:
a=0 : 18.8942 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 26.8887 (1)
a=5 : 23.6513 (1)
a=6 : 0 (0)
a=7 : 20.8164 (1)
Tree depth: 0 [0, 0]
Rollout depth: 18 [11, 22]
Total reward: 22.5627 [18.8942, 26.8887]
Policy after 4 simulations
MCTS Policy:
a=4 : 26.8887 (1)
Values after 4 simulations
MCTS Values:
a=0 : 18.8942 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 26.8887 (1)
a=5 : 23.6513 (1)
a=6 : 0 (0)
a=7 : 20.8164 (1)
Sample

# # # # 
# 0X1$#
# * . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=2 o=0 a=4 o=0 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 6.6342 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 14.0403
Total reward = 13.3383
MCTS Values:
a=0 : 6.6342 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 13.3383 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 6.6342 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 13.3383 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 6.6342 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 13.3383 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 5.5 [1, 10]
Total reward: 9.51156 [6.6342, 13.3383]
Policy after 4 simulations
MCTS Policy:
a=6 : 13.3383 (1)
Values after 4 simulations
MCTS Values:
a=0 : 6.6342 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 13.3383 (1)
a=7 : -1e+10 (1000000)
Check 2

# # # # 
# 0X1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=2 o=0 a=4 o=0 a=6 o=1 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 18.0737
Total reward = 17.1701
MCTS Values:
a=0 : 17.1701 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 13.4505
Total reward = 12.778
MCTS Values:
a=0 : 17.1701 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 12.778 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 22.8581
Total reward = 21.7152
MCTS Values:
a=0 : 17.1701 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 21.7152 (1)
a=6 : 12.778 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 13.2859
Total reward = 12.6216
MCTS Values:
a=0 : 17.1701 (1)
a=1 : 12.6216 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 21.7152 (1)
a=6 : 12.778 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 10.25 [4, 15]
Total reward: 16.0712 [12.6216, 21.7152]
Policy after 4 simulations
MCTS Policy:
a=5 : 21.7152 (1)
Values after 4 simulations
MCTS Values:
a=0 : 17.1701 (1)
a=1 : 12.6216 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 21.7152 (1)
a=6 : 12.778 (1)
a=7 : -1e+10 (1000000)
Check 1

# # # # 
# 0X1$#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 7 attempts
Expanding node: a=6 o=1 a=2 o=0 a=4 o=0 a=6 o=1 a=5 o=2 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 8.57375 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 8.57375 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward -2.3908
Total reward = -2.27126
MCTS Values:
a=0 : -2.27126 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 8.57375 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 4 [1, 9]
Total reward: 6.09406 [-2.27126, 9.5]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -2.27126 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 8.57375 (1)
a=7 : -1e+10 (1000000)
E

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=2 o=0 a=4 o=0 a=6 o=1 a=5 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 9.025 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -0.879938
Total reward = -0.835941
MCTS Values:
a=0 : 9.025 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : -0.835941 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 3.5 [2, 5]
Total reward: 6.23797 [-0.835941, 9.025]
Policy after 4 simulations
MCTS Policy:
a=0 : 9.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : 9.025 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : -0.835941 (1)
a=7 : -1e+10 (1000000)
N

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=2 o=0 a=4 o=0 a=6 o=1 a=5 o=2 a=1 o=0 a=0 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 19.5
Total reward = 18.525
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 18.525 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 18.525 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : 18.525 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.73781 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : 18.525 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 2.66667 [1, 5]
Total reward: 11.4407 [7.73781, 18.525]
Policy after 4 simulations
MCTS Policy:
a=6 : 18.525 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.73781 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : 18.525 (1)
a=7 : -1e+10 (1000000)
Check 2

# # # # 
# 0X* #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=2 o=0 a=4 o=0 a=6 o=1 a=5 o=2 a=1 o=0 a=0 o=0 a=6 o=1 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 19.5 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 19.5 (1)
a=5 : 0 (0)
a=6 : 9.5 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 5.96936
Total reward = 5.67089
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 19.5 (1)
a=5 : 5.67089 (1)
a=6 : 9.5 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 19.5 (1)
a=5 : 5.67089 (1)
a=6 : 9.5 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 5.33333 [1, 14]
Total reward: 11.1677 [5.67089, 19.5]
Policy after 4 simulations
MCTS Policy:
a=4 : 19.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 19.5 (1)
a=5 : 5.67089 (1)
a=6 : 9.5 (1)
a=7 : -1e+10 (1000000)
Sample

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=2 o=0 a=4 o=0 a=6 o=1 a=5 o=2 a=1 o=0 a=0 o=0 a=6 o=1 a=4 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.4036 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 5.4036 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 16.3759
Total reward = 15.5571
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 5.4036 (1)
a=3 : 15.5571 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 5.4036 (1)
a=3 : 15.5571 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 6.66667 [1, 12]
Total reward: 10.1152 [5.4036, 15.5571]
Policy after 4 simulations
MCTS Policy:
a=3 : 15.5571 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 5.4036 (1)
a=3 : 15.5571 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
W

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=2 o=0 a=4 o=0 a=6 o=1 a=5 o=2 a=1 o=0 a=0 o=0 a=6 o=1 a=4 o=0 a=3 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 34 steps, with total reward 1.84026
Total reward = 11.7482
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 11.7482 (1)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 11.7482 (1)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 13.3383
Total reward = 12.6714
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.6714 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 11.7482 (1)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 18.1451
Total reward = 17.2378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.6714 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 11.7482 (1)
a=5 : 17.2378 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 13 [2, 34]
Total reward: 12.6706 [9.025, 17.2378]
Policy after 4 simulations
MCTS Policy:
a=5 : 17.2378 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.6714 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 11.7482 (1)
a=5 : 17.2378 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Check 1

# # # # 
# * . #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=2 o=0 a=4 o=0 a=6 o=1 a=5 o=2 a=1 o=0 a=0 o=0 a=6 o=1 a=4 o=0 a=3 o=0 a=5 o=2 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = -2.64908
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 9.025 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 9.025 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 33 steps, with total reward -6.20795
Total reward = -5.89755
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : -5.89755 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 9.025 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 11.75 [2, 33]
Total reward: 1.95732 [-5.89755, 9.025]
Policy after 4 simulations
MCTS Policy:
a=5 : 9.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : -5.89755 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 9.025 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Check 1

# # # # 
# * . #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=2 o=0 a=4 o=0 a=6 o=1 a=5 o=2 a=1 o=0 a=0 o=0 a=6 o=1 a=4 o=0 a=3 o=0 a=5 o=2 a=5 o=2 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.6342 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.6342 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = -3.01663
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.01663 (1)
a=5 : 6.6342 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.01663 (1)
a=5 : 6.6342 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 5.75 [3, 8]
Total reward: 4.98228 [-3.01663, 8.57375]
Policy after 4 simulations
MCTS Policy:
a=2 : 8.57375 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.01663 (1)
a=5 : 6.6342 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
S

# # # # 
# 0X. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=2 o=0 a=4 o=0 a=6 o=1 a=5 o=2 a=1 o=0 a=0 o=0 a=6 o=1 a=4 o=0 a=3 o=0 a=5 o=2 a=5 o=2 a=2 o=0 
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 8.14506 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 14 steps, with total reward -1.50078
Total reward = -1.42574
MCTS Values:
a=0 : 8.14506 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1.42574 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 8.14506 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1.42574 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Expanding node: a=6 o=1 a=2 o=0 a=4 o=0 a=6 o=1 a=5 o=2 a=1 o=0 a=0 o=0 a=6 o=1 a=4 o=0 a=3 o=0 a=5 o=2 a=5 o=2 a=2 o=0 a=1 o=0 
Adding sample:

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : 8.14506 (1)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : -1e+10 (1000000)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1.42574 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0.25 [0, 1]
Rollout depth: 6.66667 [2, 14]
Total reward: 6.31108 [-1.42574, 9.5]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.2625 (2)
a=1 o=0 a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : 8.14506 (1)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : -1e+10 (1000000)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1.42574 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
E

# # # # 
# 0X. #
# . * #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=2 o=0 a=4 o=0 a=6 o=1 a=5 o=2 a=1 o=0 a=0 o=0 a=6 o=1 a=4 o=0 a=3 o=0 a=5 o=2 a=5 o=2 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 5.4036 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 5.4036 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 5.4036 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = 4.1812
MCTS Values:
a=0 : 4.1812 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 5.4036 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 10 [1, 17]
Total reward: 7.2712 [4.1812, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : 4.1812 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 5.4036 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
E

# # # # 
# 0X. #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 20.536, average = 17.6869
Undiscounted return = 30, average = 25.7143
Starting run 8 with 4 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -2.14908
Total reward = -2.04163
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -2.04163 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 27 steps, with total reward -6.17891
Total reward = -5.86997
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -5.86997 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -2.04163 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 26 steps, with total reward -9.71044
Total reward = -9.22492
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -5.86997 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -9.22492 (1)
a=7 : -2.04163 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 17.4556
Total reward = 6.58284
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -5.86997 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.58284 (1)
a=5 : 0 (0)
a=6 : -9.22492 (1)
a=7 : -2.04163 (1)
Tree depth: 0 [0, 0]
Rollout depth: 18.5 [7, 27]
Total reward: -2.63842 [-9.22492, 6.58284]
Policy after 4 simulations
MCTS Policy:
a=4 : 6.58284 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -5.86997 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.58284 (1)
a=5 : 0 (0)
a=6 : -9.22492 (1)
a=7 : -2.04163 (1)
Sample

# # # # 
# * 1X#
# 2$. #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 17.2378
Total reward = 16.3759
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : 16.3759 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 10.5648
Total reward = 10.0365
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 10.0365 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : 16.3759 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7.25 [3, 15]
Total reward: 10.681 [7.73781, 16.3759]
Policy after 4 simulations
MCTS Policy:
a=7 : 16.3759 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 10.0365 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : 16.3759 (1)
Check 3

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 10 steps, with total reward 15.8025
Total reward = 15.0124
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 15.0124 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 10.2804
Total reward = 9.76633
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 15.0124 (1)
a=7 : 9.76633 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 15.0124 (1)
a=7 : 9.76633 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9 [3, 15]
Total reward: 9.99666 [6.6342, 15.0124]
Policy after 4 simulations
MCTS Policy:
a=6 : 15.0124 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 15.0124 (1)
a=7 : 9.76633 (1)
Check 2

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 32 steps, with total reward -3.40107
Total reward = -3.23102
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -3.23102 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 11.1208
Total reward = 10.5648
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.5648 (1)
a=7 : -3.23102 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 16 steps, with total reward -1.35446
Total reward = -1.28673
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.28673 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.5648 (1)
a=7 : -3.23102 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 17.3509
Total reward = 16.4834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.28673 (1)
a=2 : 16.4834 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.5648 (1)
a=7 : -3.23102 (1)
Tree depth: 0 [0, 0]
Rollout depth: 17.25 [7, 32]
Total reward: 5.63259 [-3.23102, 16.4834]
Policy after 4 simulations
MCTS Policy:
a=2 : 16.4834 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.28673 (1)
a=2 : 16.4834 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.5648 (1)
a=7 : -3.23102 (1)
S

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 a=6 o=2 a=2 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 17.7378
Total reward = 16.8509
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 16.8509 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 16.8509 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 9.025 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 16.8509 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 12.0378
Total reward = 11.4359
MCTS Values:
a=0 : 9.025 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 16.8509 (1)
a=7 : 11.4359 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5.5 [1, 13]
Total reward: 11.703 [9.025, 16.8509]
Policy after 4 simulations
MCTS Policy:
a=6 : 16.8509 (1)
Values after 4 simulations
MCTS Values:
a=0 : 9.025 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 16.8509 (1)
a=7 : 11.4359 (1)
Check 2

# # # # 
# . 1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 a=6 o=2 a=2 o=0 a=6 o=2 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 8.57375 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -0.33171
Total reward = -10.3151
MCTS Values:
a=0 : 0 (0)
a=1 : 8.57375 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -10.3151 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 17 steps, with total reward -4.17248
Total reward = -3.96386
MCTS Values:
a=0 : 0 (0)
a=1 : 8.57375 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -10.3151 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -3.96386 (1)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 20 steps, with total reward -2.21383
Total reward = -2.10314
MCTS Values:
a=0 : 0 (0)
a=1 : 8.57375 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -10.3151 (1)
a=5 : -1e+10 (1000000)
a=6 : -2.10314 (1)
a=7 : -3.96386 (1)
Tree depth: 0 [0, 0]
Rollout depth: 12.5 [3, 20]
Total reward: -1.95209 [-10.3151, 8.57375]
Policy after 4 simulations
MCTS Policy:
a=1 : 8.57375 (1)
Values after 4 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 8.57375 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -10.3151 (1)
a=5 : -1e+10 (1000000)
a=6 : -2.10314 (1)
a=7 : -3.96386 (1)
E

# # # # 
# . 1X#
# 2$* #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 a=6 o=2 a=2 o=0 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.5 (1)
Starting simulation

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 17.2378
Total reward = 16.3759
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 16.3759 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.5 (1)
Starting simulation

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 16.3759 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.5 (1)
Starting simulation

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 3.65341
Total reward = 3.47074
MCTS Values:
a=0 : 3.47074 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 16.3759 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5.66667 [1, 10]
Total reward: 9.83667 [3.47074, 16.3759]
Policy after 4 simulations
MCTS Policy:
a=3 : 16.3759 (1)
Values after 4 simulations
MCTS Values:
a=0 : 3.47074 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 16.3759 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.5 (1)
W

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 a=6 o=2 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 19 steps, with total reward -0.429124
Total reward = -0.407667
MCTS Values:
a=0 : -0.407667 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -0.407667 (1)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 17.3509
MCTS Values:
a=0 : -0.407667 (1)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 17.3509 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 25 steps, with total reward 5.13739
Total reward = 4.88052
MCTS Values:
a=0 : -0.407667 (1)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 17.3509 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 4.88052 (1)
Tree depth: 0 [0, 0]
Rollout depth: 13.75 [5, 25]
Total reward: 7.39039 [-0.407667, 17.3509]
Policy after 4 simulations
MCTS Policy:
a=4 : 17.3509 (1)
Values after 4 simulations
MCTS Values:
a=0 : -0.407667 (1)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 17.3509 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 4.88052 (1)
Sample

# # # # 
# . 1X#
# * . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 a=6 o=2 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 a=4 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 9.025 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 9.025 (1)
a=1 : 6.6342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=4 o=0 a=7 o=1 a=6 o=2 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 a=4 o=0 a=0 o=0 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward -0.367546
Total reward = -0.33171
MCTS Values:
a=0 : 4.34664 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : -0.349169 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 0 (0)
a=0 o=0 a=7 : -1e+10 (1000000)
a=1 : 6.6342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0.25 [0, 1]
Rollout depth: 5.75 [2, 8]
Total reward: 5.76633 [-0.33171, 9.025]
Policy after 4 simulations
MCTS Policy:
a=6 : 7.73781 (1)
Values after 4 simulations
MCTS Values:
a=0 : 4.34664 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : -0.349169 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 0 (0)
a=0 o=0 a=7 : -1e+10 (1000000)
a=1 : 6.6342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : -1e+10 (1000000)
Check 2

# # # # 
# . 1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 a=6 o=2 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 a=4 o=0 a=6 o=2 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 0 (0)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Expanding node: a=4 o=0 a=7 o=1 a=6 o=2 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 a=4 o=0 a=6 o=2 a=1 o=0 
Adding sample:

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0.25 [0, 1]
Rollout depth: 3.66667 [2, 6]
Total reward: 8.61242 [7.35092, 9.5]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.2625 (2)
a=1 o=0 a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : -1e+10 (1000000)
E

# # # # 
# . 1X#
# . * #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 a=6 o=2 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 a=4 o=0 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward -0.975
Total reward = -0.92625
MCTS Values:
a=0 : -0.92625 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -0.92625 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -0.92625 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -0.92625 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 5.66667 [3, 7]
Total reward: 5.76012 [-0.92625, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -0.92625 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
a=7 : -1e+10 (1000000)
E

# # # # 
# . 1X#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 22.9707, average = 18.3474
Undiscounted return = 30, average = 26.25
Starting run 9 with 4 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 4 steps, with total reward -0.45125
Total reward = -10.4287
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# 2X* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 22 steps, with total reward -3.22859
Total reward = -3.06716
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : -3.06716 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 16 steps, with total reward -10.0005
Total reward = -9.50048
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : -3.06716 (1)
a=6 : -9.50048 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 11.25 [3, 22]
Total reward: -3.60564 [-10.4287, 8.57375]
Policy after 4 simulations
MCTS Policy:
a=1 : 8.57375 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : -3.06716 (1)
a=6 : -9.50048 (1)
a=7 : 0 (0)
E

# # # # 
# 0X* #
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward -3.51263
Total reward = -3.337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -3.337 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = -0.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -3.337 (1)
a=4 : -0.5 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -3.337 (1)
a=4 : -0.5 (1)
a=5 : 0 (0)
a=6 : 9.5 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4.33333 [1, 11]
Total reward: 3.91575 [-3.337, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -3.337 (1)
a=4 : -0.5 (1)
a=5 : 0 (0)
a=6 : 9.5 (1)
a=7 : 0 (0)
E

# # # # 
# 0X* #
# 2$. #
# # # # 
Reward 10
Terminated
Discounted return = 9.5, average = 17.3644
Undiscounted return = 10, average = 24.4444
Starting run 10 with 4 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 14 steps, with total reward 1.96327
Total reward = 1.86511
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 1.86511 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 6 steps, with total reward 17.7378
Total reward = 16.8509
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 1.86511 (1)
a=6 : 16.8509 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 16.9834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 1.86511 (1)
a=6 : 16.8509 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.40127
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.40127 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 1.86511 (1)
a=6 : 16.8509 (1)
a=7 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 10.75 [6, 16]
Total reward: 10.0252 [1.86511, 16.9834]
Policy after 4 simulations
MCTS Policy:
a=4 : 16.9834 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.40127 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 1.86511 (1)
a=6 : 16.8509 (1)
a=7 : 0 (0)
Sample

# # # # 
# * 1$#
# 2$. #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 2 steps, with total reward -0.5
Total reward = -0.475
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.475 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 19 steps, with total reward 7.98477
Total reward = 7.58554
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.475 (1)
a=2 : 7.58554 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 18 steps, with total reward 4.1812
Total reward = 3.97214
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.475 (1)
a=2 : 7.58554 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 3.97214 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 13.5487
Total reward = 12.8712
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.475 (1)
a=2 : 7.58554 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 3.97214 (1)
a=7 : 12.8712 (1)
Tree depth: 0 [0, 0]
Rollout depth: 13 [2, 19]
Total reward: 5.98848 [-0.475, 12.8712]
Policy after 4 simulations
MCTS Policy:
a=7 : 12.8712 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.475 (1)
a=2 : 7.58554 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 3.97214 (1)
a=7 : 12.8712 (1)
Check 3

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 11.9838
Total reward = 11.3846
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 11.3846 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.35092 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 11.3846 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 14.8762
Total reward = 14.1324
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.35092 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 14.1324 (1)
a=7 : 11.3846 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 17.5987
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 7.35092 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 14.1324 (1)
a=7 : 11.3846 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8.75 [3, 16]
Total reward: 12.6167 [7.35092, 17.5987]
Policy after 4 simulations
MCTS Policy:
a=1 : 17.5987 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 7.35092 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 14.1324 (1)
a=7 : 11.3846 (1)
E

# # # # 
# . * #
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 11 steps, with total reward 15.9874
Total reward = 15.188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : 15.188 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : 15.188 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward -2.04163
Total reward = -1.93955
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.93955 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : 15.188 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8.33333 [6, 11]
Total reward: 7.64984 [-1.93955, 15.188]
Policy after 4 simulations
MCTS Policy:
a=7 : 15.188 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.93955 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : 15.188 (1)
Check 3

# # # # 
# . * #
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 a=1 o=0 a=7 o=1 
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 26 steps, with total reward 16.0474
Total reward = 15.2451
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 15.2451 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 15.2451 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : 15.2451 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : 15.2451 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9 [1, 26]
Total reward: 10.1674 [7.35092, 15.2451]
Policy after 4 simulations
MCTS Policy:
a=3 : 15.2451 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : 15.2451 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : 9.5 (1)
W

# # # # 
# * 1$#
# 2$. #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 a=1 o=0 a=7 o=1 a=3 o=0 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 1.3646
Total reward = 1.29637
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 1.29637 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 7 steps, with total reward 15.496
Total reward = 14.7212
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 1.29637 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 14.7212 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 17 steps, with total reward -3.33654
Total reward = -3.16972
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 1.29637 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -3.16972 (1)
a=7 : 14.7212 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 4.83429
Total reward = 4.59258
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 1.29637 (1)
a=2 : 4.59258 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -3.16972 (1)
a=7 : 14.7212 (1)
Tree depth: 0 [0, 0]
Rollout depth: 12 [7, 17]
Total reward: 4.3601 [-3.16972, 14.7212]
Policy after 4 simulations
MCTS Policy:
a=7 : 14.7212 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 1.29637 (1)
a=2 : 4.59258 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -3.16972 (1)
a=7 : 14.7212 (1)
Check 3

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=4 o=0 a=7 o=1 a=1 o=0 a=7 o=1 a=3 o=0 a=7 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 25.0887
Total reward = 23.8343
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 23.8343 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 21.6466
Total reward = 20.5642
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 23.8343 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : 20.5642 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 23.8343 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : 20.5642 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5.75 [2, 11]
Total reward: 15.4993 [8.57375, 23.8343]
Policy after 4 simulations
MCTS Policy:
a=2 : 23.8343 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 23.8343 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
a=7 : 20.5642 (1)
S

# # # # 
# . 1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 a=1 o=0 a=7 o=1 a=3 o=0 a=7 o=1 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 21.7676
Total reward = 20.6792
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 20.6792 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 20.6792 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 20 steps, with total reward 9.76091
Total reward = 9.27286
MCTS Values:
a=0 : 0 (0)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 20.6792 (1)
a=7 : 9.27286 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 16.1342
Total reward = 15.3275
MCTS Values:
a=0 : 15.3275 (1)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 20.6792 (1)
a=7 : 9.27286 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11.75 [4, 20]
Total reward: 13.3562 [8.14506, 20.6792]
Policy after 4 simulations
MCTS Policy:
a=6 : 20.6792 (1)
Values after 4 simulations
MCTS Values:
a=0 : 15.3275 (1)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 20.6792 (1)
a=7 : 9.27286 (1)
Check 2

# # # # 
# . 1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 a=1 o=0 a=7 o=1 a=3 o=0 a=7 o=1 a=2 o=0 a=6 o=1 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 14.3343
Total reward = 3.61758
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 3.61758 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward -4.14825
Total reward = -3.94084
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 3.61758 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -3.94084 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 26 steps, with total reward -7.2261
Total reward = -6.8648
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 3.61758 (1)
a=5 : -1e+10 (1000000)
a=6 : -6.8648 (1)
a=7 : -3.94084 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward -1.67408
Total reward = -1.59038
MCTS Values:
a=0 : -1.59038 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 3.61758 (1)
a=5 : -1e+10 (1000000)
a=6 : -6.8648 (1)
a=7 : -3.94084 (1)
Tree depth: 0 [0, 0]
Rollout depth: 14 [7, 26]
Total reward: -2.19461 [-6.8648, 3.61758]
Policy after 4 simulations
MCTS Policy:
a=4 : 3.61758 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1.59038 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 3.61758 (1)
a=5 : -1e+10 (1000000)
a=6 : -6.8648 (1)
a=7 : -3.94084 (1)
Sample

# # # # 
# . 1$#
# * . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 a=1 o=0 a=7 o=1 a=3 o=0 a=7 o=1 a=2 o=0 a=6 o=1 a=4 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Expanding node: a=4 o=0 a=7 o=1 a=1 o=0 a=7 o=1 a=3 o=0 a=7 o=1 a=2 o=0 a=6 o=1 a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 4.87675
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 7.18837 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 5.13342 (1)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0.25 [0, 1]
Rollout depth: 7.25 [1, 13]
Total reward: 7.00754 [4.87675, 9.5]
Policy after 4 simulations
MCTS Policy:
a=0 : 7.35092 (1)
Values after 4 simulations
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 7.18837 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 5.13342 (1)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
a=7 : -1e+10 (1000000)
N

# # # # 
# * 1$#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 a=1 o=0 a=7 o=1 a=3 o=0 a=7 o=1 a=2 o=0 a=6 o=1 a=4 o=0 a=0 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 4.1812
Total reward = 3.97214
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 3.97214 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=7 o=1 a=1 o=0 a=7 o=1 a=3 o=0 a=7 o=1 a=2 o=0 a=6 o=1 a=4 o=0 a=0 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = -3.812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 2.844 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -4.01263 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 3.97214 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0.25 [0, 1]
Rollout depth: 8.5 [1, 18]
Total reward: 4.34949 [-3.812, 9.5]
Policy after 4 simulations
MCTS Policy:
a=2 : 7.73781 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 2.844 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -4.01263 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 3.97214 (1)
a=7 : -1e+10 (1000000)
S

# # # # 
# . 1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 a=1 o=0 a=7 o=1 a=3 o=0 a=7 o=1 a=2 o=0 a=6 o=1 a=4 o=0 a=0 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -2.15769
Total reward = -2.04981
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -2.04981 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 22 steps, with total reward 3.40562
Total reward = 3.23534
MCTS Values:
a=0 : 3.23534 (1)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -2.04981 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Expanding node: a=4 o=0 a=7 o=1 a=1 o=0 a=7 o=1 a=3 o=0 a=7 o=1 a=2 o=0 a=6 o=1 a=4 o=0 a=0 o=0 a=2 o=0 a=1 o=0 
Adding sample:

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : 3.23534 (1)
a=1 : 8.42546 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -2.04981 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0.25 [0, 1]
Rollout depth: 13 [6, 22]
Total reward: 4.50911 [-2.04981, 9.5]
Policy after 4 simulations
MCTS Policy:
a=1 : 8.42546 (2)
a=1 o=0 a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : 3.23534 (1)
a=1 : 8.42546 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -2.04981 (1)
a=7 : -1e+10 (1000000)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=7 o=1 a=1 o=0 a=7 o=1 a=3 o=0 a=7 o=1 a=2 o=0 a=6 o=1 a=4 o=0 a=0 o=0 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward -0.475
Total reward = -0.45125
MCTS Values:
a=0 : -0.45125 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -0.45125 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -0.45125 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
a=7 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 2.33333 [1, 3]
Total reward: 6.90563 [-0.45125, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -0.45125 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
a=7 : -1e+10 (1000000)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 22.0378, average = 17.8317
Undiscounted return = 30, average = 25
Simulations = 4
Runs = 10
Undiscounted return = 25 +- 2.54951
Discounted return = 17.8317 +- 1.22953
Time = 0.0107214
Starting run 1 with 8 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 7.27456
Total reward = 6.91083
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.91083 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 15.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.91083 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# 2X* #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 6.91083 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# 2X* #
# # # # 
Ending rollout after 12 steps, with total reward -3.812
Total reward = -3.6214
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 6.91083 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : -3.6214 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 26 steps, with total reward 8.4619
Total reward = 8.0388
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 6.91083 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : -3.6214 (1)
a=6 : 8.0388 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 15.1284
Total reward = 14.372
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 6.91083 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : -3.6214 (1)
a=6 : 8.0388 (1)
a=7 : 14.372 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2$. #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 14.3767
Total reward = 2.97502
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 6.91083 (1)
a=3 : -1e+10 (1000000)
a=4 : 9.18931 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 13.6579 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 0 (0)
a=5 : -3.6214 (1)
a=6 : 8.0388 (1)
a=7 : 14.372 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Expanding node: a=7 o=2 
Adding sample:

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward -4.0964
Total reward = -3.697
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 6.91083 (1)
a=3 : -1e+10 (1000000)
a=4 : 9.18931 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 13.6579 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 0 (0)
a=5 : -3.6214 (1)
a=6 : 8.0388 (1)
a=7 : 5.33751 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : -3.89158 (1)
a=7 o=2 a=3 : -1e+10 (1000000)
a=7 o=2 a=4 : 0 (0)
a=7 o=2 a=5 : 0 (0)
a=7 o=2 a=6 : 0 (0)
a=7 o=2 a=7 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 12.625 [4, 26]
Total reward: 6.06587 [-3.697, 15.4036]
Policy after 8 simulations
MCTS Policy:
a=4 : 9.18931 (2)
a=4 o=0 a=2 : 13.6579 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 6.91083 (1)
a=3 : -1e+10 (1000000)
a=4 : 9.18931 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 13.6579 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 0 (0)
a=5 : -3.6214 (1)
a=6 : 8.0388 (1)
a=7 : 5.33751 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : -3.89158 (1)
a=7 o=2 a=3 : -1e+10 (1000000)
a=7 o=2 a=4 : 0 (0)
a=7 o=2 a=5 : 0 (0)
a=7 o=2 a=6 : 0 (0)
a=7 o=2 a=7 : 0 (0)
Sample

# # # # 
# * 1$#
# 2X. #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 4 steps, with total reward -0.45125
Total reward = -0.428688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -0.428688 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -0.428688 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 22 steps, with total reward 6.99048
Total reward = 6.64095
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.64095 (1)
a=7 : -0.428688 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.64095 (1)
a=7 : -0.428688 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.6189 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.64095 (1)
a=7 : -0.428688 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 11 steps, with total reward 14.5611
Total reward = 13.1414
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.6189 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.89118 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 13.8331 (1)
a=7 : -0.428688 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.6189 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 5.71796 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 5.4036 (1)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.89118 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 13.8331 (1)
a=7 : -0.428688 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 13.0218
Total reward = 11.7522
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.6189 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 5.71796 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 5.4036 (1)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.5115 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 13.8331 (1)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 12.3707 (1)
a=6 o=2 a=7 : 0 (0)
a=7 : -0.428688 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 11.1429 [4, 22]
Total reward: 7.47245 [-0.428688, 13.1414]
Policy after 8 simulations
MCTS Policy:
a=6 : 10.5115 (3)
a=6 o=1 a=7 : 13.8331 (1)
a=6 o=2 a=6 : 12.3707 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.6189 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 5.71796 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 5.4036 (1)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.5115 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 13.8331 (1)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 12.3707 (1)
a=6 o=2 a=7 : 0 (0)
a=7 : -0.428688 (1)
Check 2

# # # # 
# * 1$#
# 2X. #
# # # # 
Observed good
Reward 0
Matched 1 states
Created 1 local transformations out of 3 attempts
Expanding node: a=4 o=0 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 17.1701
Total reward = 16.3116
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 16.3116 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
a=7 : 16.3116 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 14.5611
Total reward = 13.8331
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.8331 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
a=7 : 16.3116 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 13.5487
Total reward = 12.8712
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.8331 (1)
a=2 : 12.8712 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
a=7 : 16.3116 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=7 o=1 
Adding sample:

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 23.9851
Total reward = 21.6466
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.8331 (1)
a=2 : 12.8712 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
a=7 : 18.9791 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 22.7859 (1)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=1 o=0 
Adding sample:

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.2336 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.98337 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 12.8712 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
a=7 : 18.9791 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 22.7859 (1)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.2336 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.98337 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 10.9481 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
a=7 : 18.9791 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 22.7859 (1)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Adding sample:

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.2336 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.98337 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 10.9481 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
a=7 : 15.5106 (3)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 22.7859 (1)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 9.025 (1)
a=7 o=1 a=7 : 0 (0)
Tree depth: 0.5 [0, 1]
Rollout depth: 6.875 [1, 13]
Total reward: 11.9848 [6.6342, 21.6466]
Policy after 8 simulations
MCTS Policy:
a=7 : 15.5106 (3)
a=7 o=1 a=2 : 22.7859 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.2336 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.98337 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 10.9481 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
a=7 : 15.5106 (3)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 22.7859 (1)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 9.025 (1)
a=7 o=1 a=7 : 0 (0)
Check 3

# # # # 
# * 1$#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=4 o=0 a=6 o=1 a=7 o=2 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 32 steps, with total reward -1.73447
Total reward = -1.64774
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1.64774 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward 4.88023
Total reward = 4.63622
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.63622 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1.64774 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 3.5132
Total reward = 3.33754
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.33754 (1)
a=2 : 4.63622 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1.64774 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 7 steps, with total reward 16.8509
Total reward = 16.0084
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.33754 (1)
a=2 : 4.63622 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 16.0084 (1)
a=7 : -1.64774 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=7 o=2 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.33754 (1)
a=2 : 4.63622 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.1554 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 6.6342 (1)
a=6 o=1 a=7 : 0 (0)
a=7 : -1.64774 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=7 o=2 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 14.0403
Total reward = 3.17137
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.33754 (1)
a=2 : 3.9038 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 3.33829 (1)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.1554 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 6.6342 (1)
a=6 o=1 a=7 : 0 (0)
a=7 : -1.64774 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=7 o=2 a=1 o=0 
Adding sample:

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.7413 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 3.9038 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 3.33829 (1)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.1554 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 6.6342 (1)
a=6 o=1 a=7 : 0 (0)
a=7 : -1.64774 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Adding sample:

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 10 steps, with total reward 14.0403
Total reward = 12.6714
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.7413 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 3.9038 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 3.33829 (1)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.6607 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 6.6342 (1)
a=6 o=1 a=7 : 13.3383 (1)
a=7 : -1.64774 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 13.25 [3, 32]
Total reward: 6.57809 [-1.64774, 16.0084]
Policy after 8 simulations
MCTS Policy:
a=6 : 11.6607 (3)
a=6 o=1 a=7 : 13.3383 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.7413 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 3.9038 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 3.33829 (1)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.6607 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 6.6342 (1)
a=6 o=1 a=7 : 13.3383 (1)
a=7 : -1.64774 (1)
Check 2

# # # # 
# * 1$#
# 2X. #
# # # # 
Observed good
Reward 0
Matched 2 states
Created 1 local transformations out of 2 attempts
Expanding node: a=4 o=0 a=6 o=1 a=7 o=2 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 25 steps, with total reward -3.71431
Total reward = -3.5286
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -3.5286 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 24 steps, with total reward -1.29796
Total reward = -1.23306
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.23306 (1)
a=7 : -3.5286 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward -4.5964
Total reward = -4.36658
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : -4.36658 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.23306 (1)
a=7 : -3.5286 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=7 o=2 a=6 o=1 a=1 o=0 
Adding sample:

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 18.525
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.335 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 19.5 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -4.36658 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.23306 (1)
a=7 : -3.5286 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Adding sample:

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.0567 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 19.5 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -4.36658 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.23306 (1)
a=7 : -3.5286 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Adding sample:

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 5.0801
Total reward = 4.58479
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.1887 (4)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 4.82609 (1)
a=1 o=0 a=4 : 19.5 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -4.36658 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.23306 (1)
a=7 : -3.5286 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=7 o=2 a=6 o=1 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.1887 (4)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 4.82609 (1)
a=1 o=0 a=4 : 19.5 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -4.36658 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 2.22747 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 5.98737 (1)
a=6 o=1 a=7 : 0 (0)
a=7 : -3.5286 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 13.5714 [1, 25]
Total reward: 4.66433 [-4.36658, 18.525]
Policy after 8 simulations
MCTS Policy:
a=1 : 10.1887 (4)
a=1 o=0 a=4 : 19.5 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.1887 (4)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 4.82609 (1)
a=1 o=0 a=4 : 19.5 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -4.36658 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 2.22747 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 5.98737 (1)
a=6 o=1 a=7 : 0 (0)
a=7 : -3.5286 (1)
E

# # # # 
# . * #
# 2X. #
# # # # 
Reward 0
Matched 3 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=7 o=2 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 18.5738 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 11 steps, with total reward 12.2899
Total reward = 11.6754
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 18.5738 (1)
a=5 : -1e+10 (1000000)
a=6 : 11.6754 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 27 steps, with total reward 7.02775
Total reward = 6.67636
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 6.67636 (1)
a=4 : 18.5738 (1)
a=5 : -1e+10 (1000000)
a=6 : 11.6754 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 6.67636 (1)
a=4 : 18.5738 (1)
a=5 : -1e+10 (1000000)
a=6 : 11.6754 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 6.67636 (1)
a=4 : 18.5738 (1)
a=5 : -1e+10 (1000000)
a=6 : 11.6754 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 6.67636 (1)
a=4 : 18.5738 (1)
a=5 : -1e+10 (1000000)
a=6 : 11.6754 (1)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=7 o=2 a=6 o=1 a=1 o=0 a=4 o=0 
Adding sample:

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 19.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 6.67636 (1)
a=4 : 19.0369 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=4 o=0 a=7 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 11.6754 (1)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=7 o=2 a=6 o=1 a=1 o=0 a=6 o=1 
Adding sample:

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 5 steps, with total reward 16.7188
Total reward = 15.0887
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 6.67636 (1)
a=4 : 19.0369 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=4 o=0 a=7 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 13.382 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 15.8829 (1)
a=7 : 9.025 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 8.16667 [1, 27]
Total reward: 12.5049 [6.67636, 19.5]
Policy after 8 simulations
MCTS Policy:
a=4 : 19.0369 (2)
a=4 o=0 a=1 : 10 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 6.67636 (1)
a=4 : 19.0369 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=4 o=0 a=7 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 13.382 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 15.8829 (1)
a=7 : 9.025 (1)
Sample

# # # # 
# . * #
# 2X. #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=7 o=2 a=6 o=1 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 16 steps, with total reward -4.86709
Total reward = -4.62373
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -4.62373 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.14506 (1)
a=3 : -4.62373 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.14506 (1)
a=3 : -4.62373 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=7 o=2 a=6 o=1 a=1 o=0 a=4 o=0 a=7 o=2 
Adding sample:

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.14506 (1)
a=3 : -4.62373 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 9.2625 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 9.5 (1)
a=7 o=2 a=3 : 0 (0)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : -1e+10 (1000000)
a=7 o=2 a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=7 o=2 a=6 o=1 a=1 o=0 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 7.94144 (2)
a=2 o=0 a=0 : 8.14506 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=2 o=0 a=7 : 0 (0)
a=3 : -4.62373 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 9.2625 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 9.5 (1)
a=7 o=2 a=3 : 0 (0)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : -1e+10 (1000000)
a=7 o=2 a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 7.94144 (2)
a=2 o=0 a=0 : 8.14506 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=2 o=0 a=7 : 0 (0)
a=3 : -4.62373 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 9.2625 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 9.5 (1)
a=7 o=2 a=3 : 0 (0)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : -1e+10 (1000000)
a=7 o=2 a=7 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 5.2 [1, 16]
Total reward: 7.47302 [-4.62373, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 7.94144 (2)
a=2 o=0 a=0 : 8.14506 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=2 o=0 a=7 : 0 (0)
a=3 : -4.62373 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 9.2625 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 9.5 (1)
a=7 o=2 a=3 : 0 (0)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : -1e+10 (1000000)
a=7 o=2 a=7 : 0 (0)
E

# # # # 
# . * #
# 2X. #
# # # # 
Reward 10
Terminated
Discounted return = 25.0887, average = 25.0887
Undiscounted return = 30, average = 30
Starting run 2 with 8 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 9.025 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = -4.01263
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : 0 (0)
a=6 : 9.025 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 11 steps, with total reward -2.15769
Total reward = -2.04981
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.04981 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : 0 (0)
a=6 : 9.025 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 28 steps, with total reward -8.25309
Total reward = -7.84044
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.04981 (1)
a=2 : -7.84044 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : 0 (0)
a=6 : 9.025 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.04981 (1)
a=2 : -7.84044 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : 5.688 (1)
a=6 : 9.025 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 4 steps, with total reward 7.59875
Total reward = 7.21881
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.04981 (1)
a=2 : -7.84044 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : 5.688 (1)
a=6 : 9.025 (1)
a=7 : 7.21881 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Expanding node: a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 21 steps, with total reward -1.81874
Total reward = -1.64141
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.04981 (1)
a=2 : -7.84044 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : 5.688 (1)
a=6 : 3.69179 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : -1.7278 (1)
a=7 : 7.21881 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Expanding node: a=7 o=1 
Adding sample:

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 8 steps, with total reward 9.13245
Total reward = 8.24204
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.04981 (1)
a=2 : -7.84044 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : 5.688 (1)
a=6 : 3.69179 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : -1.7278 (1)
a=7 : 7.73043 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 0 (0)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : 0 (0)
a=7 o=1 a=5 : 0 (0)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 8.67583 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 11.875 [2, 28]
Total reward: 1.8287 [-7.84044, 9.025]
Policy after 8 simulations
MCTS Policy:
a=7 : 7.73043 (2)
a=7 o=1 a=7 : 8.67583 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.04981 (1)
a=2 : -7.84044 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : 5.688 (1)
a=6 : 3.69179 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : -1.7278 (1)
a=7 : 7.73043 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 0 (0)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : 0 (0)
a=7 o=1 a=5 : 0 (0)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 8.67583 (1)
Check 3

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed good
Reward 0
Matched 1 states
Created 1 local transformations out of 2 attempts
Expanding node: a=7 o=1 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 19.5347
Total reward = 18.558
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 18.558 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 18.558 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.73781 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 33 steps, with total reward -1.64774
Total reward = 8.43464
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 18.558 (1)
a=3 : -1e+10 (1000000)
a=4 : 8.43464 (1)
a=5 : 7.73781 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# 2$* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 18.558 (1)
a=3 : -1e+10 (1000000)
a=4 : 8.43464 (1)
a=5 : 7.73781 (1)
a=6 : 7.73781 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 13 steps, with total reward 8.2694
Total reward = 7.85593
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 18.558 (1)
a=3 : -1e+10 (1000000)
a=4 : 8.43464 (1)
a=5 : 7.73781 (1)
a=6 : 7.73781 (1)
a=7 : 7.85593 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Ending rollout after 6 steps, with total reward 16.7628
Total reward = 15.9247
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.9247 (1)
a=2 : 18.558 (1)
a=3 : -1e+10 (1000000)
a=4 : 8.43464 (1)
a=5 : 7.73781 (1)
a=6 : 7.73781 (1)
a=7 : 7.85593 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Expanding node: a=7 o=1 a=2 o=0 
Adding sample:

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 22 steps, with total reward 10.7565
Total reward = 9.70777
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.9247 (1)
a=2 : 14.1329 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 10.2187 (1)
a=3 : -1e+10 (1000000)
a=4 : 8.43464 (1)
a=5 : 7.73781 (1)
a=6 : 7.73781 (1)
a=7 : 7.85593 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Expanding node: a=7 o=1 a=1 o=0 
Adding sample:

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.7123 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 14.1329 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 10.2187 (1)
a=3 : -1e+10 (1000000)
a=4 : 8.43464 (1)
a=5 : 7.73781 (1)
a=6 : 7.73781 (1)
a=7 : 7.85593 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 14.4286 [5, 33]
Total reward: 10.6821 [7.73781, 18.558]
Policy after 8 simulations
MCTS Policy:
a=2 : 14.1329 (2)
a=2 o=0 a=7 : 10.2187 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.7123 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 14.1329 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 10.2187 (1)
a=3 : -1e+10 (1000000)
a=4 : 8.43464 (1)
a=5 : 7.73781 (1)
a=6 : 7.73781 (1)
a=7 : 7.85593 (1)
S

# # # # 
# 0$1X#
# * . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 23.2785
Total reward = 22.1146
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 22.1146 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$1X#
# 2$* #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 22.1146 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward -0.996004
Total reward = 9.0538
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 9.0538 (1)
a=5 : 22.1146 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 15.208
Total reward = 14.4476
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 9.0538 (1)
a=5 : 22.1146 (1)
a=6 : 0 (0)
a=7 : 14.4476 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 27 steps, with total reward 14.6146
Total reward = 13.8838
MCTS Values:
a=0 : 13.8838 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 9.0538 (1)
a=5 : 22.1146 (1)
a=6 : 0 (0)
a=7 : 14.4476 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 23.8043
Total reward = 22.6141
MCTS Values:
a=0 : 13.8838 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 9.0538 (1)
a=5 : 22.1146 (1)
a=6 : 22.6141 (1)
a=7 : 14.4476 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Expanding node: a=7 o=1 a=2 o=0 a=6 o=1 
Adding sample:

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 12 steps, with total reward 12.3222
Total reward = 11.1208
MCTS Values:
a=0 : 13.8838 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 9.0538 (1)
a=5 : 22.1146 (1)
a=6 : 16.8674 (2)
a=6 o=1 a=0 : 11.7061 (1)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 14.4476 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Expanding node: a=7 o=1 a=2 o=0 a=5 o=1 
Adding sample:

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# 2$* #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 6.98337
MCTS Values:
a=0 : 13.8838 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 9.0538 (1)
a=5 : 14.549 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=5 o=1 a=7 : 7.35092 (1)
a=6 : 16.8674 (2)
a=6 o=1 a=0 : 11.7061 (1)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 14.4476 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 11.125 [1, 27]
Total reward: 13.7147 [6.98337, 22.6141]
Policy after 8 simulations
MCTS Policy:
a=6 : 16.8674 (2)
a=6 o=1 a=0 : 11.7061 (1)
Values after 8 simulations
MCTS Values:
a=0 : 13.8838 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 9.0538 (1)
a=5 : 14.549 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=5 o=1 a=7 : 7.35092 (1)
a=6 : 16.8674 (2)
a=6 o=1 a=0 : 11.7061 (1)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 14.4476 (1)
Check 2

# # # # 
# 0$1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=2 o=0 a=6 o=2 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward -0.367546
Total reward = 9.65083
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 9.65083 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 23.7252
Total reward = 22.5389
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 9.65083 (1)
a=5 : 0 (0)
a=6 : 22.5389 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 5.19272
Total reward = 4.93308
MCTS Values:
a=0 : 4.93308 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 9.65083 (1)
a=5 : 0 (0)
a=6 : 22.5389 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# 2$* #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 4.93308 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 9.65083 (1)
a=5 : 8.14506 (1)
a=6 : 22.5389 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 22 steps, with total reward 16.2768
Total reward = 15.463
MCTS Values:
a=0 : 4.93308 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 9.65083 (1)
a=5 : 8.14506 (1)
a=6 : 22.5389 (1)
a=7 : 15.463 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 15.9247
Total reward = 15.1284
MCTS Values:
a=0 : 4.93308 (1)
a=1 : 15.1284 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 9.65083 (1)
a=5 : 8.14506 (1)
a=6 : 22.5389 (1)
a=7 : 15.463 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Expanding node: a=7 o=1 a=2 o=0 a=6 o=2 a=6 o=2 
Adding sample:

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 17.7378
Total reward = 16.0084
MCTS Values:
a=0 : 4.93308 (1)
a=1 : 15.1284 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 9.65083 (1)
a=5 : 8.14506 (1)
a=6 : 19.2736 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : 0 (0)
a=6 o=2 a=5 : 16.8509 (1)
a=6 o=2 a=6 : 0 (0)
a=6 o=2 a=7 : 0 (0)
a=7 : 15.463 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Expanding node: a=7 o=1 a=2 o=0 a=6 o=2 a=7 o=1 
Adding sample:

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 16.8509
MCTS Values:
a=0 : 4.93308 (1)
a=1 : 15.1284 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 9.65083 (1)
a=5 : 8.14506 (1)
a=6 : 19.2736 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : 0 (0)
a=6 o=2 a=5 : 16.8509 (1)
a=6 o=2 a=6 : 0 (0)
a=6 o=2 a=7 : 0 (0)
a=7 : 16.157 (2)
a=7 o=1 a=0 : 0 (0)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : -1e+10 (1000000)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : 17.7378 (1)
a=7 o=1 a=5 : 0 (0)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 10.125 [4, 22]
Total reward: 13.5898 [4.93308, 22.5389]
Policy after 8 simulations
MCTS Policy:
a=6 : 19.2736 (2)
a=6 o=2 a=5 : 16.8509 (1)
Values after 8 simulations
MCTS Values:
a=0 : 4.93308 (1)
a=1 : 15.1284 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 9.65083 (1)
a=5 : 8.14506 (1)
a=6 : 19.2736 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : 0 (0)
a=6 o=2 a=5 : 16.8509 (1)
a=6 o=2 a=6 : 0 (0)
a=6 o=2 a=7 : 0 (0)
a=7 : 16.157 (2)
a=7 o=1 a=0 : 0 (0)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : -1e+10 (1000000)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : 17.7378 (1)
a=7 o=1 a=5 : 0 (0)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 0 (0)
Check 2

# # # # 
# 0$1X#
# * . #
# # # # 
Observed bad
Reward 0
Matched 1 states
Created 1 local transformations out of 2 attempts
Expanding node: a=7 o=1 a=2 o=0 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$1X#
# 2$* #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 34 steps, with total reward 8.82363
Total reward = -1.61755
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1.61755 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 19 steps, with total reward 17.3984
Total reward = 16.5285
MCTS Values:
a=0 : 16.5285 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1.61755 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 24 steps, with total reward -2.90323
Total reward = -2.75807
MCTS Values:
a=0 : 16.5285 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1.61755 (1)
a=5 : -2.75807 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 8.76833
Total reward = 8.32991
MCTS Values:
a=0 : 16.5285 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1.61755 (1)
a=5 : -2.75807 (1)
a=6 : 0 (0)
a=7 : 8.32991 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# 2$* #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 16.5285 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1.61755 (1)
a=5 : -2.75807 (1)
a=6 : 7.35092 (1)
a=7 : 8.32991 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Expanding node: a=7 o=1 a=2 o=0 a=6 o=2 a=6 o=2 a=0 o=0 
Adding sample:

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# 2$* #
# # # # 
Ending rollout after 3 steps, with total reward -0.975
Total reward = -0.879937
MCTS Values:
a=0 : 7.82428 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : -0.92625 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=0 o=0 a=7 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1.61755 (1)
a=5 : -2.75807 (1)
a=6 : 7.35092 (1)
a=7 : 8.32991 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Expanding node: a=7 o=1 a=2 o=0 a=6 o=2 a=6 o=2 a=1 o=0 
Adding sample:

# # # # 
# 0$1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward -3.01663
Total reward = -2.72251
MCTS Values:
a=0 : 7.82428 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : -0.92625 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=0 o=0 a=7 : 0 (0)
a=1 : 3.38875 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : -2.8658 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1.61755 (1)
a=5 : -2.75807 (1)
a=6 : 7.35092 (1)
a=7 : 8.32991 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 13.75 [1, 34]
Total reward: 4.21641 [-2.75807, 16.5285]
Policy after 8 simulations
MCTS Policy:
a=7 : 8.32991 (1)
Values after 8 simulations
MCTS Values:
a=0 : 7.82428 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : -0.92625 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=0 o=0 a=7 : 0 (0)
a=1 : 3.38875 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : -2.8658 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1.61755 (1)
a=5 : -2.75807 (1)
a=6 : 7.35092 (1)
a=7 : 8.32991 (1)
Check 3

# # # # 
# 0$1X#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=7 o=1 a=2 o=0 a=6 o=2 a=6 o=2 a=7 o=1 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 30 steps, with total reward 0.768204
Total reward = 0.729794
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0.729794 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Ending rollout after 11 steps, with total reward -0.646835
Total reward = -0.614493
MCTS Values:
a=0 : 0 (0)
a=1 : -0.614493 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0.729794 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 25 steps, with total reward -5.22517
Total reward = -4.96391
MCTS Values:
a=0 : 0 (0)
a=1 : -0.614493 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -4.96391 (1)
a=6 : 0.729794 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# 2$* #
# # # # 
Ending rollout after 8 steps, with total reward -2.04163
Total reward = -1.93955
MCTS Values:
a=0 : 0 (0)
a=1 : -0.614493 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -4.96391 (1)
a=6 : 0.729794 (1)
a=7 : -1.93955 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 20 steps, with total reward 1.19143
Total reward = 1.13186
MCTS Values:
a=0 : 1.13186 (1)
a=1 : -0.614493 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -4.96391 (1)
a=6 : 0.729794 (1)
a=7 : -1.93955 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 15.9874
MCTS Values:
a=0 : 1.13186 (1)
a=1 : -0.614493 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 15.9874 (1)
a=5 : -4.96391 (1)
a=6 : 0.729794 (1)
a=7 : -1.93955 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1X#
# * . #
# # # # 
Expanding node: a=7 o=1 a=2 o=0 a=6 o=2 a=6 o=2 a=7 o=1 a=4 o=0 
Adding sample:

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 15.1334
MCTS Values:
a=0 : 1.13186 (1)
a=1 : -0.614493 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 15.5604 (2)
a=4 o=0 a=0 : 0 (0)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : -1e+10 (1000000)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : 0 (0)
a=4 o=0 a=6 : 5.4036 (1)
a=4 o=0 a=7 : -1e+10 (1000000)
a=5 : -4.96391 (1)
a=6 : 0.729794 (1)
a=7 : -1.93955 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1X#
# * . #
# # # # 
Adding sample:

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 16.9834
MCTS Values:
a=0 : 1.13186 (1)
a=1 : -0.614493 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 16.0347 (3)
a=4 o=0 a=0 : 0 (0)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : -1e+10 (1000000)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : 7.35092 (1)
a=4 o=0 a=6 : 5.4036 (1)
a=4 o=0 a=7 : -1e+10 (1000000)
a=5 : -4.96391 (1)
a=6 : 0.729794 (1)
a=7 : -1.93955 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 15.25 [6, 30]
Total reward: 5.30598 [-4.96391, 16.9834]
Policy after 8 simulations
MCTS Policy:
a=4 : 16.0347 (3)
a=4 o=0 a=5 : 7.35092 (1)
Values after 8 simulations
MCTS Values:
a=0 : 1.13186 (1)
a=1 : -0.614493 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 16.0347 (3)
a=4 o=0 a=0 : 0 (0)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : -1e+10 (1000000)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : 7.35092 (1)
a=4 o=0 a=6 : 5.4036 (1)
a=4 o=0 a=7 : -1e+10 (1000000)
a=5 : -4.96391 (1)
a=6 : 0.729794 (1)
a=7 : -1.93955 (1)
Sample

# # # # 
# 0$1X#
# * . #
# # # # 
Reward 10
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=2 o=0 a=6 o=2 a=6 o=2 a=7 o=1 a=4 o=0 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward -10.2567
Total reward = -9.74384
MCTS Values:
a=0 : -9.74384 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 35 steps, with total reward 1.74825
Total reward = 1.66083
MCTS Values:
a=0 : -9.74384 (1)
a=1 : 1.66083 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward -9.47264
Total reward = -8.99901
MCTS Values:
a=0 : -9.74384 (1)
a=1 : 1.66083 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -8.99901 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward -6.25755
Total reward = -5.94467
MCTS Values:
a=0 : -9.74384 (1)
a=1 : 1.66083 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -8.99901 (1)
a=6 : -5.94467 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Expanding node: a=7 o=1 a=2 o=0 a=6 o=2 a=6 o=2 a=7 o=1 a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 6.98337
MCTS Values:
a=0 : -9.74384 (1)
a=1 : 4.3221 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 7.35092 (1)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -8.99901 (1)
a=6 : -5.94467 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Adding sample:

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward -8.56948
Total reward = -7.73395
MCTS Values:
a=0 : -9.74384 (1)
a=1 : 0.303418 (3)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : -8.141 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 7.35092 (1)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -8.99901 (1)
a=6 : -5.94467 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Expanding node: a=7 o=1 a=2 o=0 a=6 o=2 a=6 o=2 a=7 o=1 a=4 o=0 a=6 o=2 
Adding sample:

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 17 steps, with total reward -2.58211
Total reward = -2.33035
MCTS Values:
a=0 : -9.74384 (1)
a=1 : 0.303418 (3)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : -8.141 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 7.35092 (1)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -8.99901 (1)
a=6 : -4.13751 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : -2.453 (1)
a=6 o=2 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Expanding node: a=7 o=1 a=2 o=0 a=6 o=2 a=6 o=2 a=7 o=1 a=4 o=0 a=5 o=2 
Adding sample:

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -9.74384 (1)
a=1 : 0.303418 (3)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : -8.141 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 7.35092 (1)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -0.426974 (2)
a=5 o=2 a=0 : 8.57375 (1)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=5 o=2 a=7 : -1e+10 (1000000)
a=6 : -4.13751 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : -2.453 (1)
a=6 o=2 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0.5 [0, 1]
Rollout depth: 15 [3, 35]
Total reward: -2.24532 [-9.74384, 8.14506]
Policy after 8 simulations
MCTS Policy:
a=1 : 0.303418 (3)
a=1 o=0 a=5 : 7.35092 (1)
Values after 8 simulations
MCTS Values:
a=0 : -9.74384 (1)
a=1 : 0.303418 (3)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : -8.141 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 7.35092 (1)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -0.426974 (2)
a=5 o=2 a=0 : 8.57375 (1)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=5 o=2 a=7 : -1e+10 (1000000)
a=6 : -4.13751 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : -2.453 (1)
a=6 o=2 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
E

# # # # 
# 0$1X#
# . * #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=7 o=1 a=2 o=0 a=6 o=2 a=6 o=2 a=7 o=1 a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -1.10361
Total reward = -1.04842
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1.04842 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 4.87675 (1)
a=4 : -1e+10 (1000000)
a=5 : -1.04842 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 9.025 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 4.87675 (1)
a=4 : -1e+10 (1000000)
a=5 : -1.04842 (1)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 9.025 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 4.87675 (1)
a=4 : -1e+10 (1000000)
a=5 : -1.04842 (1)
a=6 : 9.025 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 9.025 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 4.87675 (1)
a=4 : -1e+10 (1000000)
a=5 : -1.04842 (1)
a=6 : 9.025 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=7 o=1 a=2 o=0 a=6 o=2 a=6 o=2 a=7 o=1 a=4 o=0 a=1 o=0 a=0 o=0 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -1.85494
Total reward = -1.67408
MCTS Values:
a=0 : 3.67546 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : 0 (0)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : -1.76219 (1)
a=0 o=0 a=6 : 0 (0)
a=0 o=0 a=7 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 4.87675 (1)
a=4 : -1e+10 (1000000)
a=5 : -1.04842 (1)
a=6 : 9.025 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Expanding node: a=7 o=1 a=2 o=0 a=6 o=2 a=6 o=2 a=7 o=1 a=4 o=0 a=1 o=0 a=6 o=2 
Adding sample:

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : 3.67546 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : 0 (0)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : -1.76219 (1)
a=0 o=0 a=6 : 0 (0)
a=0 o=0 a=7 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 4.87675 (1)
a=4 : -1e+10 (1000000)
a=5 : -1.04842 (1)
a=6 : 9.2625 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 10 (1)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : 0 (0)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : 0 (0)
a=6 o=2 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0.25 [0, 1]
Rollout depth: 6.4 [2, 14]
Total reward: 6.21303 [-1.67408, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : 3.67546 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : 0 (0)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : -1.76219 (1)
a=0 o=0 a=6 : 0 (0)
a=0 o=0 a=7 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 4.87675 (1)
a=4 : -1e+10 (1000000)
a=5 : -1.04842 (1)
a=6 : 9.2625 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 10 (1)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : 0 (0)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : 0 (0)
a=6 o=2 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
E

# # # # 
# 0$1X#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 14.7212, average = 19.905
Undiscounted return = 20, average = 25
Starting run 3 with 8 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 30 steps, with total reward -11.1194
Total reward = -10.5634
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -10.5634 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 6.91789
Total reward = 6.572
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -10.5634 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.572 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 20 steps, with total reward 16.4449
Total reward = 5.62266
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -10.5634 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 5.62266 (1)
a=5 : 6.572 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# 2$* #
# # # # 
Ending rollout after 25 steps, with total reward 2.91989
Total reward = 2.7739
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -10.5634 (1)
a=2 : 2.7739 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.62266 (1)
a=5 : 6.572 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -1.43532
Total reward = -1.36355
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -10.5634 (1)
a=2 : 2.7739 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.62266 (1)
a=5 : 6.572 (1)
a=6 : 0 (0)
a=7 : -1.36355 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 5 steps, with total reward 18.1451
Total reward = 17.2378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -10.5634 (1)
a=2 : 2.7739 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.62266 (1)
a=5 : 6.572 (1)
a=6 : 17.2378 (1)
a=7 : -1.36355 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Expanding node: a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 5.79826
Total reward = 5.23293
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -10.5634 (1)
a=2 : 2.7739 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.62266 (1)
a=5 : 6.572 (1)
a=6 : 11.2354 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 5.50835 (1)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : -1.36355 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Expanding node: a=5 o=1 
Adding sample:

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# 2$* #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -10.5634 (1)
a=2 : 2.7739 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.62266 (1)
a=5 : 7.57287 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 9.025 (1)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=5 o=1 a=7 : 0 (0)
a=6 : 11.2354 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 5.50835 (1)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : -1.36355 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 14.75 [2, 30]
Total reward: 4.26076 [-10.5634, 17.2378]
Policy after 8 simulations
MCTS Policy:
a=6 : 11.2354 (2)
a=6 o=1 a=5 : 5.50835 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -10.5634 (1)
a=2 : 2.7739 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.62266 (1)
a=5 : 7.57287 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 9.025 (1)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=5 o=1 a=7 : 0 (0)
a=6 : 11.2354 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 5.50835 (1)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : -1.36355 (1)
Check 2

# # # # 
# * 1$#
# 2X. #
# # # # 
Observed good
Reward 0
Matched 1 states
Created 1 local transformations out of 4 attempts
Expanding node: a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 26 steps, with total reward -0.631721
Total reward = -0.600135
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.600135 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.600135 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 27 steps, with total reward 4.06145
Total reward = 3.85838
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.85838 (1)
a=2 : -0.600135 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 17 steps, with total reward 13.9013
Total reward = 23.2062
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.85838 (1)
a=2 : -0.600135 (1)
a=3 : -1e+10 (1000000)
a=4 : 23.2062 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 20 steps, with total reward 17.4314
Total reward = 16.5599
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.85838 (1)
a=2 : -0.600135 (1)
a=3 : -1e+10 (1000000)
a=4 : 23.2062 (1)
a=5 : 16.5599 (1)
a=6 : 0 (0)
a=7 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.85838 (1)
a=2 : -0.600135 (1)
a=3 : -1e+10 (1000000)
a=4 : 23.2062 (1)
a=5 : 16.5599 (1)
a=6 : 9.025 (1)
a=7 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 
Adding sample:

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = -1.42625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.85838 (1)
a=2 : -0.600135 (1)
a=3 : -1e+10 (1000000)
a=4 : 10.89 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.025 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 0 (0)
a=5 : 16.5599 (1)
a=6 : 9.025 (1)
a=7 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Expanding node: a=6 o=1 a=5 o=2 
Adding sample:

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 6 steps, with total reward -2.26219
Total reward = -2.04163
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.85838 (1)
a=2 : -0.600135 (1)
a=3 : -1e+10 (1000000)
a=4 : 10.89 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.025 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 0 (0)
a=5 : 7.25912 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : -2.14908 (1)
a=5 o=2 a=7 : 0 (0)
a=6 : 9.025 (1)
a=7 : 8.57375 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 12.875 [2, 27]
Total reward: 7.1444 [-2.04163, 23.2062]
Policy after 8 simulations
MCTS Policy:
a=4 : 10.89 (2)
a=4 o=0 a=1 : 9.025 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.85838 (1)
a=2 : -0.600135 (1)
a=3 : -1e+10 (1000000)
a=4 : 10.89 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.025 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 0 (0)
a=5 : 7.25912 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : -2.14908 (1)
a=5 o=2 a=7 : 0 (0)
a=6 : 9.025 (1)
a=7 : 8.57375 (1)
Sample

# # # # 
# * 1$#
# 2X. #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.4036 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.4036 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.4036 (1)
a=7 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.4036 (1)
a=7 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward -0.794144
Total reward = -0.716715
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.39164 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : -0.754436 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.4036 (1)
a=7 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=7 o=1 
Adding sample:

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.39164 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : -0.754436 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.4036 (1)
a=7 : 7.54436 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 0 (0)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.39164 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : -0.754436 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 8.3814 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.4036 (1)
a=7 : 7.54436 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 0 (0)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 14 steps, with total reward 13.7072
Total reward = 12.3707
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.39164 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : -0.754436 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 8.3814 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.88716 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 13.0218 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 7.54436 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 0 (0)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 7.73781 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 6.25 [1, 14]
Total reward: 7.30114 [-0.716715, 12.3707]
Policy after 8 simulations
MCTS Policy:
a=6 : 8.88716 (2)
a=6 o=1 a=2 : 13.0218 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.39164 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : -0.754436 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 8.3814 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.88716 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 13.0218 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 7.54436 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 0 (0)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 7.73781 (1)
Check 2

# # # # 
# * 1$#
# 2X. #
# # # # 
Observed good
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 16.8509
Total reward = 16.0084
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
a=7 : 16.0084 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 a=7 o=1 
Adding sample:

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 12 steps, with total reward 11.6754
Total reward = 10.537
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
a=7 : 13.2727 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 11.0916 (1)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 a=1 o=0 
Adding sample:

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 14.4476
Total reward = 13.0389
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.032 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 13.7252 (1)
a=1 o=0 a=7 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
a=7 : 13.2727 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 11.0916 (1)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 14.1584
Total reward = 12.778
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.032 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 13.7252 (1)
a=1 o=0 a=7 : 0 (0)
a=2 : 9.88067 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 13.4505 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
a=7 : 13.2727 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 11.0916 (1)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 15 steps, with total reward 11.8601
Total reward = 10.7038
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.032 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 13.7252 (1)
a=1 o=0 a=7 : 0 (0)
a=2 : 9.88067 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 13.4505 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.34557 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 11.2671 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 13.2727 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 11.0916 (1)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 0 (0)
Tree depth: 0.5 [0, 1]
Rollout depth: 9.625 [2, 15]
Total reward: 10.6327 [5.98737, 16.0084]
Policy after 8 simulations
MCTS Policy:
a=7 : 13.2727 (2)
a=7 o=1 a=2 : 11.0916 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.032 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 13.7252 (1)
a=1 o=0 a=7 : 0 (0)
a=2 : 9.88067 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 13.4505 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.34557 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 11.2671 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 13.2727 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 11.0916 (1)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 0 (0)
Check 3

# # # # 
# * 1$#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 a=7 o=2 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward -4.01263
Total reward = -3.812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -3.812 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -3.812 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 6.6342 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -3.812 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
a=7 : 6.6342 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : -3.812 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
a=7 : 6.6342 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 a=7 o=2 a=1 o=0 
Adding sample:

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -3.812 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
a=7 : 6.6342 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 a=7 o=2 a=7 o=2 
Adding sample:

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -3.812 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
a=7 : 7.60398 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 0 (0)
a=7 o=2 a=3 : -1e+10 (1000000)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : 9.025 (1)
a=7 o=2 a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 a=7 o=2 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 4 steps, with total reward 18.0737
Total reward = 16.3116
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -3.812 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.4729 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.1701 (1)
a=6 o=1 a=7 : 0 (0)
a=7 : 7.60398 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 0 (0)
a=7 o=2 a=3 : -1e+10 (1000000)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : 9.025 (1)
a=7 o=2 a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Adding sample:

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward -0.879938
Total reward = -0.794144
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -3.812 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.38387 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.1701 (1)
a=6 o=1 a=7 : -0.835941 (1)
a=7 : 7.60398 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 0 (0)
a=7 o=2 a=3 : -1e+10 (1000000)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : 9.025 (1)
a=7 o=2 a=7 : 0 (0)
Tree depth: 0.5 [0, 1]
Rollout depth: 5.71429 [2, 11]
Total reward: 6.50907 [-3.812, 16.3116]
Policy after 8 simulations
MCTS Policy:
a=1 : 9.2625 (2)
a=1 o=0 a=1 : 10 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -3.812 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.38387 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.1701 (1)
a=6 o=1 a=7 : -0.835941 (1)
a=7 : 7.60398 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 0 (0)
a=7 o=2 a=3 : -1e+10 (1000000)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : 9.025 (1)
a=7 o=2 a=7 : 0 (0)
E

# # # # 
# . * #
# 2X. #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 a=7 o=2 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 2 steps, with total reward 19.5
Total reward = 18.525
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 18.525 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 18.525 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.73781 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 18.525 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward -2.45706
Total reward = -2.33421
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.73781 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 18.525 (1)
a=7 : -2.33421 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 4 steps, with total reward 17.5987
Total reward = 16.7188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.73781 (1)
a=3 : 16.7188 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 18.525 (1)
a=7 : -2.33421 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -1.84257
Total reward = 8.24956
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.73781 (1)
a=3 : 16.7188 (1)
a=4 : 8.24956 (1)
a=5 : -1e+10 (1000000)
a=6 : 18.525 (1)
a=7 : -2.33421 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 a=7 o=2 a=1 o=0 a=6 o=1 
Adding sample:

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.73781 (1)
a=3 : 16.7188 (1)
a=4 : 8.24956 (1)
a=5 : -1e+10 (1000000)
a=6 : 13.775 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 9.5 (1)
a=7 : -2.33421 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 a=7 o=2 a=1 o=0 a=3 o=0 
Adding sample:

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 5.77564
Total reward = 5.21252
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.73781 (1)
a=3 : 10.9657 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : 0 (0)
a=3 o=0 a=7 : 5.48686 (1)
a=4 : 8.24956 (1)
a=5 : -1e+10 (1000000)
a=6 : 13.775 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 9.5 (1)
a=7 : -2.33421 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 7 [1, 15]
Total reward: 9.14181 [-2.33421, 18.525]
Policy after 8 simulations
MCTS Policy:
a=6 : 13.775 (2)
a=6 o=1 a=7 : 9.5 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.73781 (1)
a=3 : 10.9657 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : 0 (0)
a=3 o=0 a=7 : 5.48686 (1)
a=4 : 8.24956 (1)
a=5 : -1e+10 (1000000)
a=6 : 13.775 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 9.5 (1)
a=7 : -2.33421 (1)
Check 2

# # # # 
# . * #
# 2X. #
# # # # 
Observed good
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 a=7 o=2 a=1 o=0 a=6 o=1 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 19.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 13 steps, with total reward 14.4286
Total reward = 13.7072
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 13.7072 (1)
a=4 : 19.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward -1.35494
Total reward = -1.28719
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.28719 (1)
a=3 : 13.7072 (1)
a=4 : 19.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.28719 (1)
a=3 : 13.7072 (1)
a=4 : 19.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 2 steps, with total reward 19.5
Total reward = 18.525
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.28719 (1)
a=3 : 13.7072 (1)
a=4 : 19.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 18.525 (1)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.28719 (1)
a=3 : 13.7072 (1)
a=4 : 19.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 18.525 (1)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 a=7 o=2 a=1 o=0 a=6 o=1 a=4 o=0 
Adding sample:

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 19.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.28719 (1)
a=3 : 13.7072 (1)
a=4 : 19.5 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=4 o=0 a=7 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 18.525 (1)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 a=7 o=2 a=1 o=0 a=6 o=1 a=6 o=1 
Adding sample:

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 16.7188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.28719 (1)
a=3 : 13.7072 (1)
a=4 : 19.5 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=4 o=0 a=7 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 17.6219 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 17.5987 (1)
a=7 : 9.025 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 4.33333 [1, 13]
Total reward: 13.2111 [-1.28719, 19.5]
Policy after 8 simulations
MCTS Policy:
a=4 : 19.5 (2)
a=4 o=0 a=1 : 10 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.28719 (1)
a=3 : 13.7072 (1)
a=4 : 19.5 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=4 o=0 a=7 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 17.6219 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 17.5987 (1)
a=7 : 9.025 (1)
Sample

# # # # 
# . * #
# 2X. #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 a=7 o=2 a=1 o=0 a=6 o=1 a=4 o=0 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -1.35494
Total reward = -1.28719
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1.28719 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -1.28719 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -1.28719 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 5.13342 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward -1.66292
Total reward = -1.57977
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.57977 (1)
a=3 : -1.28719 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 5.13342 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -1.57977 (1)
a=3 : -1.28719 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 5.13342 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 a=7 o=2 a=1 o=0 a=6 o=1 a=4 o=0 a=7 o=2 
Adding sample:

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward -2.58211
Total reward = -2.33035
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -1.57977 (1)
a=3 : -1.28719 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 1.40153 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 0 (0)
a=7 o=2 a=3 : -2.453 (1)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : -1e+10 (1000000)
a=7 o=2 a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : -1.57977 (1)
a=3 : -1.28719 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 1.40153 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 0 (0)
a=7 o=2 a=3 : -2.453 (1)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : -1e+10 (1000000)
a=7 o=2 a=7 : 0 (0)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 a=7 o=2 a=1 o=0 a=6 o=1 a=4 o=0 a=3 o=0 
Adding sample:

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : -1.57977 (1)
a=3 : 3.22531 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=3 o=0 a=7 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 1.40153 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 0 (0)
a=7 o=2 a=3 : -2.453 (1)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : -1e+10 (1000000)
a=7 o=2 a=7 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 10.2 [4, 17]
Total reward: 4.70924 [-2.33035, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : -1.57977 (1)
a=3 : 3.22531 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=3 o=0 a=7 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : 1.40153 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 0 (0)
a=7 o=2 a=3 : -2.453 (1)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : -1e+10 (1000000)
a=7 o=2 a=7 : 0 (0)
E

# # # # 
# . * #
# 2X. #
# # # # 
Reward 10
Terminated
Discounted return = 23.8343, average = 21.2147
Undiscounted return = 30, average = 26.6667
Starting run 4 with 8 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2$* #
# # # # 
Ending rollout after 9 steps, with total reward 5.84006
Total reward = 5.54806
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 5.54806 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# 2$* #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 5.54806 (1)
a=7 : 5.98737 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 17 steps, with total reward 14.4013
Total reward = 13.6812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 13.6812 (1)
a=6 : 5.54806 (1)
a=7 : 5.98737 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 28 steps, with total reward 3.49909
Total reward = 3.32414
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.32414 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 13.6812 (1)
a=6 : 5.54806 (1)
a=7 : 5.98737 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 24 steps, with total reward 6.3089
Total reward = -4.00654
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.32414 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.00654 (1)
a=5 : 13.6812 (1)
a=6 : 5.54806 (1)
a=7 : 5.98737 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# 2$* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.32414 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.00654 (1)
a=5 : 13.6812 (1)
a=6 : 5.54806 (1)
a=7 : 5.98737 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Expanding node: a=5 o=1 
Adding sample:

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 5 steps, with total reward -0.428688
Total reward = 9.11311
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.32414 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.00654 (1)
a=5 : 11.3972 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 9.59275 (1)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=5 o=1 a=7 : 0 (0)
a=6 : 5.54806 (1)
a=7 : 5.98737 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.32414 (1)
a=2 : 7.94144 (2)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.00654 (1)
a=5 : 11.3972 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 9.59275 (1)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=5 o=1 a=7 : 0 (0)
a=6 : 5.54806 (1)
a=7 : 5.98737 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 12.625 [3, 28]
Total reward: 6.19128 [-4.00654, 13.6812]
Policy after 8 simulations
MCTS Policy:
a=5 : 11.3972 (2)
a=5 o=1 a=4 : 9.59275 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.32414 (1)
a=2 : 7.94144 (2)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.00654 (1)
a=5 : 11.3972 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 9.59275 (1)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=5 o=1 a=7 : 0 (0)
a=6 : 5.54806 (1)
a=7 : 5.98737 (1)
Check 1

# # # # 
# * 1$#
# 2X. #
# # # # 
Observed good
Reward 0
Matched 1 states
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 19 steps, with total reward 3.97214
Total reward = 13.7735
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 13.7735 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 16.7628
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 13.7735 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 16.7628 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 13.7735 (1)
a=5 : 7.35092 (1)
a=6 : 0 (0)
a=7 : 16.7628 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 12.438
Total reward = 11.8161
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 11.8161 (1)
a=3 : -1e+10 (1000000)
a=4 : 13.7735 (1)
a=5 : 7.35092 (1)
a=6 : 0 (0)
a=7 : 16.7628 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 16.4391
Total reward = 15.6171
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 11.8161 (1)
a=3 : -1e+10 (1000000)
a=4 : 13.7735 (1)
a=5 : 7.35092 (1)
a=6 : 15.6171 (1)
a=7 : 16.7628 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 4 steps, with total reward -0.92625
Total reward = -0.879938
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.879938 (1)
a=2 : 11.8161 (1)
a=3 : -1e+10 (1000000)
a=4 : 13.7735 (1)
a=5 : 7.35092 (1)
a=6 : 15.6171 (1)
a=7 : 16.7628 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Expanding node: a=5 o=1 a=7 o=1 
Adding sample:

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 17 steps, with total reward 9.76835
Total reward = 8.81594
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.879938 (1)
a=2 : 11.8161 (1)
a=3 : -1e+10 (1000000)
a=4 : 13.7735 (1)
a=5 : 7.35092 (1)
a=6 : 15.6171 (1)
a=7 : 12.7894 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 0 (0)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : 0 (0)
a=7 o=1 a=5 : 0 (0)
a=7 o=1 a=6 : 9.27994 (1)
a=7 o=1 a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 22 steps, with total reward 24.2287
Total reward = 21.8664
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.879938 (1)
a=2 : 11.8161 (1)
a=3 : -1e+10 (1000000)
a=4 : 13.7735 (1)
a=5 : 7.35092 (1)
a=6 : 18.7418 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 23.0172 (1)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 12.7894 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 0 (0)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : 0 (0)
a=7 o=1 a=5 : 0 (0)
a=7 o=1 a=6 : 9.27994 (1)
a=7 o=1 a=7 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 13.125 [4, 22]
Total reward: 11.8904 [-0.879938, 21.8664]
Policy after 8 simulations
MCTS Policy:
a=6 : 18.7418 (2)
a=6 o=1 a=5 : 23.0172 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.879938 (1)
a=2 : 11.8161 (1)
a=3 : -1e+10 (1000000)
a=4 : 13.7735 (1)
a=5 : 7.35092 (1)
a=6 : 18.7418 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 23.0172 (1)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 12.7894 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 0 (0)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : 0 (0)
a=7 o=1 a=5 : 0 (0)
a=7 o=1 a=6 : 9.27994 (1)
a=7 o=1 a=7 : 0 (0)
Check 2

# # # # 
# * 1$#
# 2X. #
# # # # 
Observed good
Reward 0
Matched 1 states
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 29.2659
Total reward = 27.8026
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 27.8026 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 27 steps, with total reward 3.14489
Total reward = 2.98764
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 2.98764 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 27.8026 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 6 steps, with total reward 15.8829
Total reward = 25.0887
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 2.98764 (1)
a=3 : -1e+10 (1000000)
a=4 : 25.0887 (1)
a=5 : 0 (0)
a=6 : 27.8026 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# 2$* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 2.98764 (1)
a=3 : -1e+10 (1000000)
a=4 : 25.0887 (1)
a=5 : 0 (0)
a=6 : 27.8026 (1)
a=7 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 2.98764 (1)
a=3 : -1e+10 (1000000)
a=4 : 25.0887 (1)
a=5 : 7.73781 (1)
a=6 : 27.8026 (1)
a=7 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 9.05795
Total reward = 8.1748
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 2.98764 (1)
a=3 : -1e+10 (1000000)
a=4 : 25.0887 (1)
a=5 : 7.73781 (1)
a=6 : 17.9887 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 8.60506 (1)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 
Adding sample:

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 2.98764 (1)
a=3 : -1e+10 (1000000)
a=4 : 21.8312 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.025 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 0 (0)
a=5 : 7.73781 (1)
a=6 : 17.9887 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 8.60506 (1)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 8.57375 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 9.625 [2, 27]
Total reward: 13.4955 [2.98764, 27.8026]
Policy after 8 simulations
MCTS Policy:
a=4 : 21.8312 (2)
a=4 o=0 a=1 : 9.025 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 2.98764 (1)
a=3 : -1e+10 (1000000)
a=4 : 21.8312 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.025 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 0 (0)
a=5 : 7.73781 (1)
a=6 : 17.9887 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 8.60506 (1)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 8.57375 (1)
Sample

# # # # 
# * 1$#
# 2X. #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 22 steps, with total reward -6.09438
Total reward = -5.78966
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : -5.78966 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : -5.78966 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : -5.78966 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 14.9036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 11.9643 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 15.688 (1)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : -5.78966 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 11.9643 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 15.688 (1)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.74799 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 8.57375 (1)
a=6 o=1 a=7 : 0 (0)
a=7 : -5.78966 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.594 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 11.9643 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 15.688 (1)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.74799 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 8.57375 (1)
a=6 o=1 a=7 : 0 (0)
a=7 : -5.78966 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.594 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 10.5555 (3)
a=2 o=0 a=0 : 8.14506 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 15.688 (1)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.74799 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 8.57375 (1)
a=6 o=1 a=7 : 0 (0)
a=7 : -5.78966 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 8.42857 [2, 22]
Total reward: 7.07009 [-5.78966, 14.9036]
Policy after 8 simulations
MCTS Policy:
a=2 : 10.5555 (3)
a=2 o=0 a=4 : 15.688 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.594 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 10.5555 (3)
a=2 o=0 a=0 : 8.14506 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 15.688 (1)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.74799 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 8.57375 (1)
a=6 o=1 a=7 : 0 (0)
a=7 : -5.78966 (1)
S

# # # # 
# . 1$#
# * . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 13.6176
Total reward = 2.9367
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 2.9367 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 2.9367 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 2.9367 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 2.9367 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 6.6342 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 12.387
Total reward = 11.7676
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 2.9367 (1)
a=5 : -1e+10 (1000000)
a=6 : 11.7676 (1)
a=7 : 6.6342 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=2 o=0 a=6 o=1 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 12 steps, with total reward 11.6754
Total reward = 10.537
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 2.9367 (1)
a=5 : -1e+10 (1000000)
a=6 : 11.1523 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 11.0916 (1)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 6.6342 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=2 o=0 a=1 o=0 
Adding sample:

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 19 steps, with total reward 19.1601
Total reward = 17.292
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 13.1585 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 18.2021 (1)
a=1 o=0 a=7 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 2.9367 (1)
a=5 : -1e+10 (1000000)
a=6 : 11.1523 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 11.0916 (1)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 6.6342 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=2 o=0 a=0 o=0 
Adding sample:

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 20 steps, with total reward 0.971366
Total reward = 0.876658
MCTS Values:
a=0 : 4.11379 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0.922798 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 0 (0)
a=0 o=0 a=7 : 0 (0)
a=1 : 13.1585 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 18.2021 (1)
a=1 o=0 a=7 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 2.9367 (1)
a=5 : -1e+10 (1000000)
a=6 : 11.1523 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 11.0916 (1)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 6.6342 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 11.125 [2, 20]
Total reward: 8.30252 [0.876658, 17.292]
Policy after 8 simulations
MCTS Policy:
a=1 : 13.1585 (2)
a=1 o=0 a=6 : 18.2021 (1)
Values after 8 simulations
MCTS Values:
a=0 : 4.11379 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0.922798 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 0 (0)
a=0 o=0 a=7 : 0 (0)
a=1 : 13.1585 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 18.2021 (1)
a=1 o=0 a=7 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 2.9367 (1)
a=5 : -1e+10 (1000000)
a=6 : 11.1523 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 11.0916 (1)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 6.6342 (1)
E

# # # # 
# . 1$#
# 2X* #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.5 (1)
Starting simulation

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 9.5 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.5 (1)
Starting simulation

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 22 steps, with total reward 12.9056
Total reward = 12.2603
MCTS Values:
a=0 : 9.5 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 12.2603 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.5 (1)
Starting simulation

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 9.5 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 12.2603 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
a=7 : 9.5 (1)
Starting simulation

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=2 o=0 a=1 o=0 a=3 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : 9.5 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 9.44727 (2)
a=3 o=0 a=0 : 0 (0)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : -1e+10 (1000000)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : 0 (0)
a=3 o=0 a=7 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
a=7 : 9.5 (1)
Starting simulation

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 9.5 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 9.44727 (2)
a=3 o=0 a=0 : 0 (0)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : -1e+10 (1000000)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : 0 (0)
a=3 o=0 a=7 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
a=7 : 9.5 (1)
Starting simulation

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=2 o=0 a=1 o=0 a=6 o=1 
Adding sample:

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 32 steps, with total reward 11.1
Total reward = 10.0178
MCTS Values:
a=0 : 9.5 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 9.44727 (2)
a=3 o=0 a=0 : 0 (0)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : -1e+10 (1000000)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : 0 (0)
a=3 o=0 a=7 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.75888 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : 10.545 (1)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 9.5 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 10.6667 [1, 32]
Total reward: 9.67654 [6.6342, 12.2603]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : 9.5 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 9.44727 (2)
a=3 o=0 a=0 : 0 (0)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : -1e+10 (1000000)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : 0 (0)
a=3 o=0 a=7 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.75888 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : 10.545 (1)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 9.5 (1)
E

# # # # 
# . 1$#
# 2X* #
# # # # 
Reward 10
Terminated
Discounted return = 16.7628, average = 20.1018
Undiscounted return = 20, average = 25
Starting run 5 with 8 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 17.7378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 42 steps, with total reward -10.5313
Total reward = -10.0048
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -10.0048 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -10.0048 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 0 (0)
a=6 : 4.87675 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# 2X* #
# # # # 
Ending rollout after 24 steps, with total reward -3.9098
Total reward = -3.71431
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -10.0048 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 0 (0)
a=6 : 4.87675 (1)
a=7 : -3.71431 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 7 steps, with total reward 5.92467
Total reward = 5.62844
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -10.0048 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 5.62844 (1)
a=6 : 4.87675 (1)
a=7 : -3.71431 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Ending rollout after 3 steps, with total reward -0.975
Total reward = -0.92625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : -10.0048 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 5.62844 (1)
a=6 : 4.87675 (1)
a=7 : -3.71431 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2X. #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward -4.39255
Total reward = -13.9643
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : -10.0048 (1)
a=3 : -1e+10 (1000000)
a=4 : 1.88677 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : -4.17292 (1)
a=5 : 5.62844 (1)
a=6 : 4.87675 (1)
a=7 : -3.71431 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Ending rollout after 4 steps, with total reward -0.92625
Total reward = -0.835941
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : -10.0048 (1)
a=3 : -1e+10 (1000000)
a=4 : 1.88677 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : -4.17292 (1)
a=5 : 2.39625 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : -0.879938 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=5 o=2 a=7 : 0 (0)
a=6 : 4.87675 (1)
a=7 : -3.71431 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 14.625 [3, 42]
Total reward: -0.150317 [-13.9643, 17.7378]
Policy after 8 simulations
MCTS Policy:
a=6 : 4.87675 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : -10.0048 (1)
a=3 : -1e+10 (1000000)
a=4 : 1.88677 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : -4.17292 (1)
a=5 : 2.39625 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : -0.879938 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=5 o=2 a=7 : 0 (0)
a=6 : 4.87675 (1)
a=7 : -3.71431 (1)
Check 2

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 33 steps, with total reward 2.62055
Total reward = 2.48952
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 2.48952 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 19 steps, with total reward 8.84889
Total reward = 8.40645
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 2.48952 (1)
a=6 : 0 (0)
a=7 : 8.40645 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 27 steps, with total reward 6.75078
Total reward = 6.41325
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.41325 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 2.48952 (1)
a=6 : 0 (0)
a=7 : 8.40645 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.41325 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 2.48952 (1)
a=6 : 0 (0)
a=7 : 8.40645 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 6.41325 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 2.48952 (1)
a=6 : 0 (0)
a=7 : 8.40645 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 8.66406
Total reward = 8.23086
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 6.41325 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 2.48952 (1)
a=6 : 8.23086 (1)
a=7 : 8.40645 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Expanding node: a=6 o=2 a=4 o=0 
Adding sample:

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.1451
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 6.41325 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.3594 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 8.57375 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 0 (0)
a=5 : 2.48952 (1)
a=6 : 8.23086 (1)
a=7 : 8.40645 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Adding sample:

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -1.85494
Total reward = 8.32592
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 6.41325 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.0149 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 8.57375 (1)
a=4 o=0 a=2 : -1.76219 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 0 (0)
a=5 : 2.48952 (1)
a=6 : 8.23086 (1)
a=7 : 8.40645 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 12.875 [3, 33]
Total reward: 9.69602 [2.48952, 18.5738]
Policy after 8 simulations
MCTS Policy:
a=4 : 15.0149 (3)
a=4 o=0 a=1 : 8.57375 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 6.41325 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.0149 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 8.57375 (1)
a=4 o=0 a=2 : -1.76219 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 0 (0)
a=5 : 2.48952 (1)
a=6 : 8.23086 (1)
a=7 : 8.40645 (1)
Sample

# # # # 
# * 1X#
# 2X. #
# # # # 
Reward -10
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 6.6342 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.4036 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 6.6342 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 31 steps, with total reward -0.231881
Total reward = -0.220287
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.4036 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.220287 (1)
a=7 : 6.6342 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.4036 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.220287 (1)
a=7 : 6.6342 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Expanding node: a=6 o=2 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.4036 (1)
a=2 : 7.16715 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 7.73781 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.220287 (1)
a=7 : 6.6342 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Expanding node: a=6 o=2 a=4 o=0 a=7 o=1 
Adding sample:

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.4036 (1)
a=2 : 7.16715 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 7.73781 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.220287 (1)
a=7 : 6.80879 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 0 (0)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 7.35092 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Expanding node: a=6 o=2 a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 37 steps, with total reward -0.461276
Total reward = -0.416302
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 2.49365 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -0.438212 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 7.16715 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 7.73781 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.220287 (1)
a=7 : 6.80879 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 0 (0)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 7.35092 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Expanding node: a=6 o=2 a=4 o=0 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 2.49365 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -0.438212 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 7.16715 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 7.73781 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 3.96239 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 8.57375 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
a=6 o=2 a=7 : 0 (0)
a=7 : 6.80879 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 0 (0)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 7.35092 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 13.625 [3, 37]
Total reward: 5.10799 [-0.416302, 8.14506]
Policy after 8 simulations
MCTS Policy:
a=2 : 7.16715 (2)
a=2 o=0 a=1 : 7.73781 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 2.49365 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -0.438212 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 7.16715 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 7.73781 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 3.96239 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 8.57375 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
a=6 o=2 a=7 : 0 (0)
a=7 : 6.80879 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 0 (0)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 7.35092 (1)
S

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 16.4834
Total reward = 15.6592
MCTS Values:
a=0 : 0 (0)
a=1 : 15.6592 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 16.6342
MCTS Values:
a=0 : 0 (0)
a=1 : 15.6592 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : 0 (0)
a=1 : 15.6592 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : -1e+10 (1000000)
a=6 : 4.87675 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -11.2228
Total reward = -10.6617
MCTS Values:
a=0 : 0 (0)
a=1 : 15.6592 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : -1e+10 (1000000)
a=6 : 4.87675 (1)
a=7 : -10.6617 (1)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 7 steps, with total reward -0.794144
Total reward = -0.754436
MCTS Values:
a=0 : -0.754436 (1)
a=1 : 15.6592 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : -1e+10 (1000000)
a=6 : 4.87675 (1)
a=7 : -10.6617 (1)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
Expanding node: a=6 o=2 a=4 o=0 a=2 o=0 a=4 o=0 
Adding sample:

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = -5.59873
MCTS Values:
a=0 : -0.754436 (1)
a=1 : 15.6592 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 5.51774 (2)
a=4 o=0 a=0 : 4.63291 (1)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : -1e+10 (1000000)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 4.87675 (1)
a=7 : -10.6617 (1)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Expanding node: a=6 o=2 a=4 o=0 a=2 o=0 a=1 o=0 
Adding sample:

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 9 steps, with total reward -0.716715
Total reward = -0.646835
MCTS Values:
a=0 : -0.754436 (1)
a=1 : 7.50618 (2)
a=1 o=0 a=0 : -0.680879 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 5.51774 (2)
a=4 o=0 a=0 : 4.63291 (1)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : -1e+10 (1000000)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 4.87675 (1)
a=7 : -10.6617 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=6 o=2 a=4 o=0 a=2 o=0 a=6 o=1 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : -0.754436 (1)
a=1 : 7.50618 (2)
a=1 o=0 a=0 : -0.680879 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 5.51774 (2)
a=4 o=0 a=0 : 4.63291 (1)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : -1e+10 (1000000)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.75548 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 6.98337 (1)
a=6 o=1 a=7 : 0 (0)
a=7 : -10.6617 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 9.375 [7, 15]
Total reward: 3.26783 [-10.6617, 16.6342]
Policy after 8 simulations
MCTS Policy:
a=1 : 7.50618 (2)
a=1 o=0 a=1 : 0 (0)
Values after 8 simulations
MCTS Values:
a=0 : -0.754436 (1)
a=1 : 7.50618 (2)
a=1 o=0 a=0 : -0.680879 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 5.51774 (2)
a=4 o=0 a=0 : 4.63291 (1)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : -1e+10 (1000000)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.75548 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 6.98337 (1)
a=6 o=1 a=7 : 0 (0)
a=7 : -10.6617 (1)
E

# # # # 
# . 1X#
# 2X* #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=4 o=0 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 9.025 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 9.025 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 49 steps, with total reward 4.43744
Total reward = 4.21556
MCTS Values:
a=0 : 9.025 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 4.21556 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 18 steps, with total reward 4.1812
Total reward = 3.97214
MCTS Values:
a=0 : 9.025 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 4.21556 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 3.97214 (1)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 9.025 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 4.21556 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 3.97214 (1)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Expanding node: a=6 o=2 a=4 o=0 a=2 o=0 a=1 o=0 a=0 o=0 
Adding sample:

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : 8.3814 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : 8.14506 (1)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 0 (0)
a=0 o=0 a=7 : 0 (0)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 4.21556 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 3.97214 (1)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Expanding node: a=6 o=2 a=4 o=0 a=2 o=0 a=1 o=0 a=7 o=2 
Adding sample:

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : 8.3814 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : 8.14506 (1)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 0 (0)
a=0 o=0 a=7 : 0 (0)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 4.21556 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 3.97214 (1)
a=7 : 9.2625 (2)
a=7 o=2 a=0 : 0 (0)
a=7 o=2 a=1 : 10 (1)
a=7 o=2 a=2 : -1e+10 (1000000)
a=7 o=2 a=3 : 0 (0)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : 0 (0)
a=7 o=2 a=7 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 15 [2, 49]
Total reward: 7.93444 [3.97214, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : 8.3814 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : 8.14506 (1)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 0 (0)
a=0 o=0 a=7 : 0 (0)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 4.21556 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 3.97214 (1)
a=7 : 9.2625 (2)
a=7 o=2 a=0 : 0 (0)
a=7 o=2 a=1 : 10 (1)
a=7 o=2 a=2 : -1e+10 (1000000)
a=7 o=2 a=3 : 0 (0)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : 0 (0)
a=7 o=2 a=7 : 0 (0)
E

# # # # 
# . 1X#
# 2X* #
# # # # 
Reward 10
Terminated
Discounted return = -1.35494, average = 15.8104
Undiscounted return = 0, average = 20
Starting run 6 with 8 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 12.3222
Total reward = 11.7061
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 11.7061 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 29 steps, with total reward 12.3783
Total reward = 11.7594
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 11.7594 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 11.7061 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 19 steps, with total reward 18.6851
Total reward = 17.7509
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 11.7594 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 17.7509 (1)
a=7 : 11.7061 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 3 steps, with total reward -0.475
Total reward = -0.45125
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.45125 (1)
a=2 : 11.7594 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 17.7509 (1)
a=7 : 11.7061 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 43 steps, with total reward 14.4334
Total reward = 23.7117
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.45125 (1)
a=2 : 11.7594 (1)
a=3 : -1e+10 (1000000)
a=4 : 23.7117 (1)
a=5 : 0 (0)
a=6 : 17.7509 (1)
a=7 : 11.7061 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.45125 (1)
a=2 : 11.7594 (1)
a=3 : -1e+10 (1000000)
a=4 : 23.7117 (1)
a=5 : 7.73781 (1)
a=6 : 17.7509 (1)
a=7 : 11.7061 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2X. #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = -3.69751
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.45125 (1)
a=2 : 11.7594 (1)
a=3 : -1e+10 (1000000)
a=4 : 10.0071 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 6.6342 (1)
a=5 : 7.73781 (1)
a=6 : 17.7509 (1)
a=7 : 11.7061 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Expanding node: a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 23.8343
Total reward = 21.5104
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.45125 (1)
a=2 : 11.7594 (1)
a=3 : -1e+10 (1000000)
a=4 : 10.0071 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 6.6342 (1)
a=5 : 7.73781 (1)
a=6 : 19.6307 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 22.6426 (1)
a=6 o=1 a=7 : 0 (0)
a=7 : 11.7061 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 15.875 [3, 43]
Total reward: 11.2534 [-3.69751, 23.7117]
Policy after 8 simulations
MCTS Policy:
a=6 : 19.6307 (2)
a=6 o=1 a=6 : 22.6426 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.45125 (1)
a=2 : 11.7594 (1)
a=3 : -1e+10 (1000000)
a=4 : 10.0071 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 6.6342 (1)
a=5 : 7.73781 (1)
a=6 : 19.6307 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 22.6426 (1)
a=6 o=1 a=7 : 0 (0)
a=7 : 11.7061 (1)
Check 2

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 27 steps, with total reward 2.08062
Total reward = 1.97659
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 1.97659 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -1.85494
Total reward = -1.76219
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.76219 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 1.97659 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 5 steps, with total reward 18.1451
Total reward = 17.2378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.76219 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 17.2378 (1)
a=7 : 1.97659 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 12 steps, with total reward 8.41051
Total reward = 7.98998
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.98998 (1)
a=2 : -1.76219 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 17.2378 (1)
a=7 : 1.97659 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 7 steps, with total reward 8.32592
Total reward = 7.90962
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.98998 (1)
a=2 : -1.76219 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.90962 (1)
a=6 : 17.2378 (1)
a=7 : 1.97659 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 19 steps, with total reward -4.60161
Total reward = 5.62847
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.98998 (1)
a=2 : -1.76219 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.62847 (1)
a=5 : 7.90962 (1)
a=6 : 17.2378 (1)
a=7 : 1.97659 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Expanding node: a=6 o=2 a=6 o=1 
Adding sample:

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.98998 (1)
a=2 : -1.76219 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.62847 (1)
a=5 : 7.90962 (1)
a=6 : 12.9058 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 9.025 (1)
a=7 : 1.97659 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Expanding node: a=6 o=2 a=1 o=0 
Adding sample:

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# 2X* #
# # # # 
Ending rollout after 9 steps, with total reward -2.3908
Total reward = -2.15769
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 2.91614 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : -2.27126 (1)
a=1 o=0 a=7 : 0 (0)
a=2 : -1.76219 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.62847 (1)
a=5 : 7.90962 (1)
a=6 : 12.9058 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 9.025 (1)
a=7 : 1.97659 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 10.75 [2, 27]
Total reward: 5.67454 [-2.15769, 17.2378]
Policy after 8 simulations
MCTS Policy:
a=6 : 12.9058 (2)
a=6 o=1 a=7 : 9.025 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 2.91614 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : -2.27126 (1)
a=1 o=0 a=7 : 0 (0)
a=2 : -1.76219 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.62847 (1)
a=5 : 7.90962 (1)
a=6 : 12.9058 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 9.025 (1)
a=7 : 1.97659 (1)
Check 2

# # # # 
# * 1X#
# 2X. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# 2X* #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 12 steps, with total reward -3.337
Total reward = -13.1701
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -13.1701 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 11 steps, with total reward -3.03763
Total reward = -2.88575
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -13.1701 (1)
a=5 : -2.88575 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# 2X* #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.63291
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -13.1701 (1)
a=5 : -2.88575 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 41 steps, with total reward -20.9425
Total reward = -19.8954
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -13.1701 (1)
a=5 : -2.88575 (1)
a=6 : 0 (0)
a=7 : -19.8954 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 23 steps, with total reward -10.2068
Total reward = -9.69645
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -13.1701 (1)
a=5 : -2.88575 (1)
a=6 : -9.69645 (1)
a=7 : -19.8954 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Expanding node: a=6 o=2 a=6 o=2 a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# 2X* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.58503 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 8.57375 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -13.1701 (1)
a=5 : -2.88575 (1)
a=6 : -9.69645 (1)
a=7 : -19.8954 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Expanding node: a=6 o=2 a=6 o=2 a=2 o=0 
Adding sample:

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward -4.5964
Total reward = -4.14825
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.58503 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 8.57375 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 0.242331 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -4.36658 (1)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -13.1701 (1)
a=5 : -2.88575 (1)
a=6 : -9.69645 (1)
a=7 : -19.8954 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 15 [2, 41]
Total reward: -3.49913 [-19.8954, 9.025]
Policy after 8 simulations
MCTS Policy:
a=1 : 8.58503 (2)
a=1 o=0 a=2 : 8.57375 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.58503 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 8.57375 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 0.242331 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -4.36658 (1)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -13.1701 (1)
a=5 : -2.88575 (1)
a=6 : -9.69645 (1)
a=7 : -19.8954 (1)
E

# # # # 
# 0$* #
# 2X. #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$1X#
# 2X* #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.63291
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 4.63291 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 28 steps, with total reward -7.53307
Total reward = -17.1564
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : -17.1564 (1)
a=5 : 9.025 (1)
a=6 : 4.63291 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 22 steps, with total reward 8.72441
Total reward = 8.28819
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 8.28819 (1)
a=4 : -17.1564 (1)
a=5 : 9.025 (1)
a=6 : 4.63291 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 15 steps, with total reward 11.8601
Total reward = 11.2671
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 8.28819 (1)
a=4 : -17.1564 (1)
a=5 : 9.025 (1)
a=6 : 4.63291 (1)
a=7 : 11.2671 (1)
Starting simulation

# # # # 
# 0X* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 8.28819 (1)
a=4 : -17.1564 (1)
a=5 : 9.025 (1)
a=6 : 4.63291 (1)
a=7 : 11.2671 (1)
Starting simulation

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Expanding node: a=6 o=2 a=6 o=2 a=1 o=0 a=7 o=2 
Adding sample:

# # # # 
# 0$* #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = -2.51663
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 8.28819 (1)
a=4 : -17.1564 (1)
a=5 : 9.025 (1)
a=6 : 4.63291 (1)
a=7 : 4.37524 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 0 (0)
a=7 o=2 a=3 : 0 (0)
a=7 o=2 a=4 : -2.64908 (1)
a=7 o=2 a=5 : 0 (0)
a=7 o=2 a=6 : 0 (0)
a=7 o=2 a=7 : 0 (0)
Tree depth: 0.125 [0, 1]
Rollout depth: 12.7143 [1, 28]
Total reward: 4.13002 [-17.1564, 11.2671]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 8.28819 (1)
a=4 : -17.1564 (1)
a=5 : 9.025 (1)
a=6 : 4.63291 (1)
a=7 : 4.37524 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 0 (0)
a=7 o=2 a=3 : 0 (0)
a=7 o=2 a=4 : -2.64908 (1)
a=7 o=2 a=5 : 0 (0)
a=7 o=2 a=6 : 0 (0)
a=7 o=2 a=7 : 0 (0)
E

# # # # 
# 0$* #
# 2X. #
# # # # 
Reward 10
Terminated
Discounted return = 8.57375, average = 14.6043
Undiscounted return = 10, average = 18.3333
Starting run 7 with 8 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 31 steps, with total reward 15.353
Total reward = 14.5854
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 14.5854 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# 2$* #
# # # # 
Ending rollout after 7 steps, with total reward 16.3759
Total reward = 15.5571
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 15.5571 (1)
a=6 : 14.5854 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Ending rollout after 12 steps, with total reward 15.188
Total reward = 14.4286
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 15.5571 (1)
a=6 : 14.5854 (1)
a=7 : 14.4286 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 15.5571 (1)
a=6 : 14.5854 (1)
a=7 : 14.4286 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 16.7628
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 16.7628 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 15.5571 (1)
a=6 : 14.5854 (1)
a=7 : 14.4286 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward 10.5682
Total reward = 0.0398206
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 16.7628 (1)
a=3 : -1e+10 (1000000)
a=4 : 0.0398206 (1)
a=5 : 15.5571 (1)
a=6 : 14.5854 (1)
a=7 : 14.4286 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 3 steps, with total reward -0.975
Total reward = -0.879937
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 7.94144 (2)
a=2 o=0 a=0 : -0.92625 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0.0398206 (1)
a=5 : 15.5571 (1)
a=6 : 14.5854 (1)
a=7 : 14.4286 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 7.94144 (2)
a=2 o=0 a=0 : -0.92625 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0.0398206 (1)
a=5 : 12.0654 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 9.025 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=5 o=2 a=7 : 0 (0)
a=6 : 14.5854 (1)
a=7 : 14.4286 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 10.25 [1, 31]
Total reward: 9.82095 [-0.879937, 16.7628]
Policy after 8 simulations
MCTS Policy:
a=6 : 14.5854 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 7.94144 (2)
a=2 o=0 a=0 : -0.92625 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0.0398206 (1)
a=5 : 12.0654 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 9.025 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=5 o=2 a=7 : 0 (0)
a=6 : 14.5854 (1)
a=7 : 14.4286 (1)
Check 2

# # # # 
# * 1X#
# 2$. #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward -11.6696
Total reward = -11.0861
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -11.0861 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 44 steps, with total reward -12.1188
Total reward = -11.5129
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -11.5129 (1)
a=6 : 0 (0)
a=7 : -11.0861 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 17 steps, with total reward -10.0273
Total reward = -9.52597
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -11.5129 (1)
a=6 : -9.52597 (1)
a=7 : -11.0861 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 20 steps, with total reward 3.77354
Total reward = -6.41514
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -6.41514 (1)
a=5 : -11.5129 (1)
a=6 : -9.52597 (1)
a=7 : -11.0861 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -6.41514 (1)
a=5 : -11.5129 (1)
a=6 : -9.52597 (1)
a=7 : -11.0861 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward -1.75044
Total reward = -1.66292
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : -1.66292 (1)
a=3 : -1e+10 (1000000)
a=4 : -6.41514 (1)
a=5 : -11.5129 (1)
a=6 : -9.52597 (1)
a=7 : -11.0861 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Expanding node: a=6 o=2 a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 10 steps, with total reward -8.82594
Total reward = -7.96541
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0.767294 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : -8.38464 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -1.66292 (1)
a=3 : -1e+10 (1000000)
a=4 : -6.41514 (1)
a=5 : -11.5129 (1)
a=6 : -9.52597 (1)
a=7 : -11.0861 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Expanding node: a=6 o=2 a=2 o=0 
Adding sample:

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = -0.92625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0.767294 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : -8.38464 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -1.29458 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -0.975 (1)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -6.41514 (1)
a=5 : -11.5129 (1)
a=6 : -9.52597 (1)
a=7 : -11.0861 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 15.125 [1, 44]
Total reward: -4.94933 [-11.5129, 9.5]
Policy after 8 simulations
MCTS Policy:
a=1 : 0.767294 (2)
a=1 o=0 a=1 : 0 (0)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0.767294 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : -8.38464 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -1.29458 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -0.975 (1)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -6.41514 (1)
a=5 : -11.5129 (1)
a=6 : -9.52597 (1)
a=7 : -11.0861 (1)
E

# # # # 
# 0X* #
# 2$. #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 31 steps, with total reward -5.44043
Total reward = -5.16841
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -5.16841 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 16 steps, with total reward -10.8631
Total reward = -10.3199
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -5.16841 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -10.3199 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 23 steps, with total reward 6.03751
Total reward = 5.73563
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -5.16841 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -10.3199 (1)
a=6 : 0 (0)
a=7 : 5.73563 (1)
Starting simulation

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# 2X* #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -5.16841 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -10.3199 (1)
a=6 : 5.688 (1)
a=7 : 5.73563 (1)
Starting simulation

# # # # 
# 0X* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -5.16841 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -10.3199 (1)
a=6 : 5.688 (1)
a=7 : 5.73563 (1)
Starting simulation

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 4.17714
Total reward = 3.96829
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -5.16841 (1)
a=3 : 3.96829 (1)
a=4 : 0 (0)
a=5 : -10.3199 (1)
a=6 : 5.688 (1)
a=7 : 5.73563 (1)
Starting simulation

# # # # 
# 0X* #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# 2X. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = -0.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -5.16841 (1)
a=3 : 3.96829 (1)
a=4 : -0.5 (1)
a=5 : -10.3199 (1)
a=6 : 5.688 (1)
a=7 : 5.73563 (1)
Starting simulation

# # # # 
# 0X* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -5.16841 (1)
a=3 : 3.96829 (1)
a=4 : -0.5 (1)
a=5 : -10.3199 (1)
a=6 : 5.688 (1)
a=7 : 5.73563 (1)
Tree depth: 0 [0, 0]
Rollout depth: 15.6667 [1, 31]
Total reward: 2.42545 [-10.3199, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -5.16841 (1)
a=3 : 3.96829 (1)
a=4 : -0.5 (1)
a=5 : -10.3199 (1)
a=6 : 5.688 (1)
a=7 : 5.73563 (1)
E

# # # # 
# 0X* #
# 2$. #
# # # # 
Reward 10
Terminated
Discounted return = 9.025, average = 13.8073
Undiscounted return = 10, average = 17.1429
Starting run 8 with 8 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 17.2378
Total reward = 6.37592
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.37592 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 17 steps, with total reward 10.3886
Total reward = 9.8692
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.8692 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.37592 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# 2$* #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.8692 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.37592 (1)
a=5 : 0 (0)
a=6 : 5.688 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# 2X* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.8692 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.37592 (1)
a=5 : 8.57375 (1)
a=6 : 5.688 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.8692 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.37592 (1)
a=5 : 8.57375 (1)
a=6 : 5.688 (1)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2$. #
# # # # 
Ending rollout after 11 steps, with total reward 12.2899
Total reward = 11.6754
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.8692 (1)
a=2 : 11.6754 (1)
a=3 : -1e+10 (1000000)
a=4 : 6.37592 (1)
a=5 : 8.57375 (1)
a=6 : 5.688 (1)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 7 steps, with total reward 16.3759
Total reward = 14.7793
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.8692 (1)
a=2 : 13.2273 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 15.5571 (1)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.37592 (1)
a=5 : 8.57375 (1)
a=6 : 5.688 (1)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 33 steps, with total reward 14.5517
Total reward = 13.1329
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.501 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 13.8241 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 13.2273 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 15.5571 (1)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.37592 (1)
a=5 : 8.57375 (1)
a=6 : 5.688 (1)
a=7 : 9.025 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 11.25 [2, 33]
Total reward: 9.88992 [5.688, 14.7793]
Policy after 8 simulations
MCTS Policy:
a=2 : 13.2273 (2)
a=2 o=0 a=6 : 15.5571 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.501 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 13.8241 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 13.2273 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 15.5571 (1)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.37592 (1)
a=5 : 8.57375 (1)
a=6 : 5.688 (1)
a=7 : 9.025 (1)
S

# # # # 
# 0$1$#
# * . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 17 steps, with total reward 9.80487
Total reward = 9.31462
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 9.31462 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# 2X* #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 0 (0)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 9.31462 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 4.06816
Total reward = 3.86475
MCTS Values:
a=0 : 3.86475 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 9.31462 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward -9.07426
Total reward = -8.62055
MCTS Values:
a=0 : 3.86475 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -8.62055 (1)
a=7 : 9.31462 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 5.99646
Total reward = 5.69664
MCTS Values:
a=0 : 3.86475 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.69664 (1)
a=6 : -8.62055 (1)
a=7 : 9.31462 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 15 steps, with total reward -1.11062
Total reward = -11.0551
MCTS Values:
a=0 : 3.86475 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -11.0551 (1)
a=5 : 5.69664 (1)
a=6 : -8.62055 (1)
a=7 : 9.31462 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Expanding node: a=2 o=0 a=7 o=2 
Adding sample:

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 30 steps, with total reward -0.238864
Total reward = -0.215575
MCTS Values:
a=0 : 3.86475 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -11.0551 (1)
a=5 : 5.69664 (1)
a=6 : -8.62055 (1)
a=7 : 4.54952 (2)
a=7 o=2 a=0 : 0 (0)
a=7 o=2 a=1 : -0.226921 (1)
a=7 o=2 a=2 : -1e+10 (1000000)
a=7 o=2 a=3 : -1e+10 (1000000)
a=7 o=2 a=4 : 0 (0)
a=7 o=2 a=5 : 0 (0)
a=7 o=2 a=6 : 0 (0)
a=7 o=2 a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Expanding node: a=2 o=0 a=1 o=0 
Adding sample:

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# 2X* #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.1812
MCTS Values:
a=0 : 3.86475 (1)
a=1 : 6.6031 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 4.40127 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -11.0551 (1)
a=5 : 5.69664 (1)
a=6 : -8.62055 (1)
a=7 : 4.54952 (2)
a=7 o=2 a=0 : 0 (0)
a=7 o=2 a=1 : -0.226921 (1)
a=7 o=2 a=2 : -1e+10 (1000000)
a=7 o=2 a=3 : -1e+10 (1000000)
a=7 o=2 a=4 : 0 (0)
a=7 o=2 a=5 : 0 (0)
a=7 o=2 a=6 : 0 (0)
a=7 o=2 a=7 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 16 [2, 30]
Total reward: 1.52388 [-11.0551, 9.31462]
Policy after 8 simulations
MCTS Policy:
a=1 : 6.6031 (2)
a=1 o=0 a=7 : 4.40127 (1)
Values after 8 simulations
MCTS Values:
a=0 : 3.86475 (1)
a=1 : 6.6031 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 4.40127 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -11.0551 (1)
a=5 : 5.69664 (1)
a=6 : -8.62055 (1)
a=7 : 4.54952 (2)
a=7 o=2 a=0 : 0 (0)
a=7 o=2 a=1 : -0.226921 (1)
a=7 o=2 a=2 : -1e+10 (1000000)
a=7 o=2 a=3 : -1e+10 (1000000)
a=7 o=2 a=4 : 0 (0)
a=7 o=2 a=5 : 0 (0)
a=7 o=2 a=6 : 0 (0)
a=7 o=2 a=7 : 0 (0)
E

# # # # 
# 0$1$#
# 2$* #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0$1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -2.20411
Total reward = -2.0939
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -2.0939 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -2.0939 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 6.98337 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# 2X* #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : -2.0939 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 6.98337 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 46 steps, with total reward -10.0045
Total reward = -9.50427
MCTS Values:
a=0 : -9.50427 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : -2.0939 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 6.98337 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -9.50427 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : -2.0939 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 6.98337 (1)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 17 steps, with total reward -1.00233
Total reward = -0.952217
MCTS Values:
a=0 : -9.50427 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : -2.0939 (1)
a=4 : -1e+10 (1000000)
a=5 : -0.952217 (1)
a=6 : 6.98337 (1)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# 0$1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# 2X* #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -9.50427 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : -2.0939 (1)
a=4 : -1e+10 (1000000)
a=5 : -0.952217 (1)
a=6 : 6.98337 (1)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Expanding node: a=2 o=0 a=1 o=0 a=7 o=2 
Adding sample:

# # # # 
# 0X1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# 2X* #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.40127
MCTS Values:
a=0 : -9.50427 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : -2.0939 (1)
a=4 : -1e+10 (1000000)
a=5 : -0.952217 (1)
a=6 : 6.98337 (1)
a=7 : 6.71313 (2)
a=7 o=2 a=0 : 0 (0)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : -1e+10 (1000000)
a=7 o=2 a=3 : 4.63291 (1)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : 0 (0)
a=7 o=2 a=6 : 0 (0)
a=7 o=2 a=7 : 0 (0)
Tree depth: 0.125 [0, 1]
Rollout depth: 16 [2, 46]
Total reward: 3.48241 [-9.50427, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : -9.50427 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : -2.0939 (1)
a=4 : -1e+10 (1000000)
a=5 : -0.952217 (1)
a=6 : 6.98337 (1)
a=7 : 6.71313 (2)
a=7 o=2 a=0 : 0 (0)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : -1e+10 (1000000)
a=7 o=2 a=3 : 4.63291 (1)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : 0 (0)
a=7 o=2 a=6 : 0 (0)
a=7 o=2 a=7 : 0 (0)
E

# # # # 
# 0$1$#
# 2$* #
# # # # 
Reward 10
Terminated
Discounted return = 9.025, average = 13.2095
Undiscounted return = 10, average = 16.25
Starting run 9 with 8 simulations... 
Expanding node: 

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward -3.812
Total reward = -3.6214
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -3.6214 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 16.9834
Total reward = 16.1342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 16.1342 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -3.6214 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = 4.1812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.1812 (1)
a=2 : 16.1342 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -3.6214 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 15.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.1812 (1)
a=2 : 16.1342 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -3.6214 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 11 steps, with total reward 4.31329
Total reward = 4.09762
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.1812 (1)
a=2 : 16.1342 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 4.09762 (1)
a=6 : 0 (0)
a=7 : -3.6214 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 23 steps, with total reward -3.74804
Total reward = -3.56064
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.1812 (1)
a=2 : 16.1342 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 4.09762 (1)
a=6 : -3.56064 (1)
a=7 : -3.6214 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2X* #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward -4.62325
Total reward = -4.17248
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.1812 (1)
a=2 : 5.98086 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -4.39209 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 4.09762 (1)
a=6 : -3.56064 (1)
a=7 : -3.6214 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2X. #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 9 steps, with total reward 15.208
Total reward = 3.72518
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.1812 (1)
a=2 : 5.98086 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -4.39209 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 9.56439 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 14.4476 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 0 (0)
a=5 : 4.09762 (1)
a=6 : -3.56064 (1)
a=7 : -3.6214 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 13.375 [8, 23]
Total reward: 4.02341 [-4.17248, 16.1342]
Policy after 8 simulations
MCTS Policy:
a=4 : 9.56439 (2)
a=4 o=0 a=2 : 14.4476 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.1812 (1)
a=2 : 5.98086 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -4.39209 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 9.56439 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 14.4476 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 0 (0)
a=5 : 4.09762 (1)
a=6 : -3.56064 (1)
a=7 : -3.6214 (1)
Sample

# # # # 
# * 1X#
# 2X. #
# # # # 
Reward -10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.6342 (1)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 10 steps, with total reward 12.9367
Total reward = 12.2899
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.6342 (1)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : 12.2899 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Expanding node: a=4 o=0 a=7 o=2 
Adding sample:

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward -0.526851
Total reward = -0.475483
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.6342 (1)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : 5.90719 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 0 (0)
a=7 o=2 a=3 : -1e+10 (1000000)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : 0 (0)
a=7 o=2 a=7 : -0.500509 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.6342 (1)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.3814 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 9.5 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 5.90719 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 0 (0)
a=7 o=2 a=3 : -1e+10 (1000000)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : 0 (0)
a=7 o=2 a=7 : -0.500509 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 14.6334
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.6338 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 15.4036 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.3814 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 9.5 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 5.90719 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 0 (0)
a=7 o=2 a=3 : -1e+10 (1000000)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : 0 (0)
a=7 o=2 a=7 : -0.500509 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 3.53031
Total reward = 3.1861
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.6338 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 15.4036 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 4.29485 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 3.35379 (1)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.3814 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 9.5 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 5.90719 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 0 (0)
a=7 o=2 a=3 : -1e+10 (1000000)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : 0 (0)
a=7 o=2 a=7 : -0.500509 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 9.375 [1, 15]
Total reward: 7.30431 [-0.475483, 14.6334]
Policy after 8 simulations
MCTS Policy:
a=1 : 10.6338 (2)
a=1 o=0 a=4 : 15.4036 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.6338 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 15.4036 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 4.29485 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 3.35379 (1)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.3814 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 9.5 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 0 (0)
a=7 : 5.90719 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : 0 (0)
a=7 o=2 a=3 : -1e+10 (1000000)
a=7 o=2 a=4 : -1e+10 (1000000)
a=7 o=2 a=5 : -1e+10 (1000000)
a=7 o=2 a=6 : 0 (0)
a=7 o=2 a=7 : -0.500509 (1)
E

# # # # 
# . * #
# 2X. #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward -1.11062
Total reward = -1.05509
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1.05509 (1)
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1.05509 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : -1.05509 (1)
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 3 steps, with total reward -0.475
Total reward = -0.45125
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -0.45125 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : -1.05509 (1)
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 23 steps, with total reward 3.23534
Total reward = -6.92643
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -0.45125 (1)
a=4 : -6.92643 (1)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : -1.05509 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# 2X* #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : -0.45125 (1)
a=4 : -6.92643 (1)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : -1.05509 (1)
Starting simulation

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : -0.45125 (1)
a=4 : -6.92643 (1)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : -1.05509 (1)
Starting simulation

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 0.985171
Total reward = 0.889116
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 5.19456 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0.935912 (1)
a=3 : -0.45125 (1)
a=4 : -6.92643 (1)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : -1.05509 (1)
Tree depth: 0.125 [0, 1]
Rollout depth: 10.5 [1, 23]
Total reward: 3.66341 [-6.92643, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 5.19456 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0.935912 (1)
a=3 : -0.45125 (1)
a=4 : -6.92643 (1)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
a=7 : -1.05509 (1)
E

# # # # 
# . * #
# 2X. #
# # # # 
Reward 10
Terminated
Discounted return = -0.975, average = 11.6334
Undiscounted return = 0, average = 14.4444
Starting run 10 with 8 simulations... 
Expanding node: 

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 13.3375
Total reward = 12.6707
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.6707 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2$* #
# # # # 
Ending rollout after 15 steps, with total reward -2.86106
Total reward = -2.71801
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.6707 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -2.71801 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -9.88689
Total reward = 0.607454
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.6707 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0.607454 (1)
a=5 : -2.71801 (1)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# 2$* #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.6707 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0.607454 (1)
a=5 : -2.71801 (1)
a=6 : 0 (0)
a=7 : 6.98337 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# 2$* #
# # # # 
Ending rollout after 19 steps, with total reward -0.904607
Total reward = -0.859376
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.6707 (1)
a=2 : -0.859376 (1)
a=3 : -1e+10 (1000000)
a=4 : 0.607454 (1)
a=5 : -2.71801 (1)
a=6 : 0 (0)
a=7 : 6.98337 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 23 steps, with total reward 20.9731
Total reward = 19.9245
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.6707 (1)
a=2 : -0.859376 (1)
a=3 : -1e+10 (1000000)
a=4 : 0.607454 (1)
a=5 : -2.71801 (1)
a=6 : 19.9245 (1)
a=7 : 6.98337 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Expanding node: a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 10 steps, with total reward 23.9476
Total reward = 21.6127
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.6707 (1)
a=2 : -0.859376 (1)
a=3 : -1e+10 (1000000)
a=4 : 0.607454 (1)
a=5 : -2.71801 (1)
a=6 : 20.7686 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 22.7502 (1)
a=7 : 6.98337 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0$* #
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# 0$* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# 2$* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = -1.35494
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.65786 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1.42625 (1)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -0.859376 (1)
a=3 : -1e+10 (1000000)
a=4 : 0.607454 (1)
a=5 : -2.71801 (1)
a=6 : 20.7686 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 22.7502 (1)
a=7 : 6.98337 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 12.5 [3, 23]
Total reward: 7.10829 [-2.71801, 21.6127]
Policy after 8 simulations
MCTS Policy:
a=6 : 20.7686 (2)
a=6 o=1 a=7 : 22.7502 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.65786 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1.42625 (1)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : -0.859376 (1)
a=3 : -1e+10 (1000000)
a=4 : 0.607454 (1)
a=5 : -2.71801 (1)
a=6 : 20.7686 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : 22.7502 (1)
a=7 : 6.98337 (1)
Check 2

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -0.646835
Total reward = -0.614493
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -0.614493 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 8.64662
Total reward = 8.21429
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.21429 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -0.614493 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0X* #
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# 2$* #
# # # # 
Ending rollout after 7 steps, with total reward 17.3509
Total reward = 16.4834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.4834 (1)
a=2 : 8.21429 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -0.614493 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = -1.42625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.4834 (1)
a=2 : 8.21429 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : -0.614493 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 4 steps, with total reward 27.5987
Total reward = 26.2188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.4834 (1)
a=2 : 8.21429 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 26.2188 (1)
a=6 : 0 (0)
a=7 : -0.614493 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 12 steps, with total reward -0.946203
Total reward = -0.898893
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.4834 (1)
a=2 : 8.21429 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 26.2188 (1)
a=6 : -0.898893 (1)
a=7 : -0.614493 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Expanding node: a=6 o=1 a=5 o=2 
Adding sample:

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# 2$* #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.4834 (1)
a=2 : 8.21429 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 17.3963 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 9.025 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=5 o=2 a=7 : 0 (0)
a=6 : -0.898893 (1)
a=7 : -0.614493 (1)
Starting simulation

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Expanding node: a=6 o=1 a=1 o=0 
Adding sample:

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# 2X* #
# # # # 
Ending rollout after 5 steps, with total reward 17.1701
Total reward = 15.496
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.9897 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 16.3116 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 8.21429 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 17.3963 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 9.025 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=5 o=2 a=7 : 0 (0)
a=6 : -0.898893 (1)
a=7 : -0.614493 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 7.375 [2, 15]
Total reward: 9.00582 [-1.42625, 26.2188]
Policy after 8 simulations
MCTS Policy:
a=5 : 17.3963 (2)
a=5 o=2 a=1 : 9.025 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.9897 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 16.3116 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 8.21429 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 17.3963 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 9.025 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=5 o=2 a=7 : 0 (0)
a=6 : -0.898893 (1)
a=7 : -0.614493 (1)
Check 1

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward -1.42574
Total reward = -1.35446
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.35446 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# 2X. #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.35446 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 0.0346875
Total reward = 0.0329531
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.35446 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0.0329531 (1)
a=6 : 0 (0)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 5.11798
Total reward = 4.86208
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.35446 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0.0329531 (1)
a=6 : 4.86208 (1)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.35446 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0.0329531 (1)
a=6 : 4.86208 (1)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2X. #
# # # # 
Ending rollout after 12 steps, with total reward 6.684
Total reward = 6.3498
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.35446 (1)
a=2 : 6.3498 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0.0329531 (1)
a=6 : 4.86208 (1)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# 2X. #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 
Adding sample:

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 8 steps, with total reward -0.754436
Total reward = 9.31912
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.35446 (1)
a=2 : 6.3498 (1)
a=3 : -1e+10 (1000000)
a=4 : 13.9464 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : -0.716715 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 0 (0)
a=5 : 0.0329531 (1)
a=6 : 4.86208 (1)
a=7 : 9.025 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=7 o=2 
Adding sample:

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# 2X. #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# 2X* #
# # # # 
Ending rollout after 21 steps, with total reward -1.29189
Total reward = -1.16593
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.35446 (1)
a=2 : 6.3498 (1)
a=3 : -1e+10 (1000000)
a=4 : 13.9464 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : -0.716715 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 0 (0)
a=5 : 0.0329531 (1)
a=6 : 4.86208 (1)
a=7 : 3.92953 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : -1.2273 (1)
a=7 o=2 a=3 : -1e+10 (1000000)
a=7 o=2 a=4 : 0 (0)
a=7 o=2 a=5 : 0 (0)
a=7 o=2 a=6 : 0 (0)
a=7 o=2 a=7 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 11.875 [2, 21]
Total reward: 5.70529 [-1.35446, 18.5738]
Policy after 8 simulations
MCTS Policy:
a=4 : 13.9464 (2)
a=4 o=0 a=2 : 0 (0)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.35446 (1)
a=2 : 6.3498 (1)
a=3 : -1e+10 (1000000)
a=4 : 13.9464 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : -0.716715 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=4 o=0 a=7 : 0 (0)
a=5 : 0.0329531 (1)
a=6 : 4.86208 (1)
a=7 : 3.92953 (2)
a=7 o=2 a=0 : -1e+10 (1000000)
a=7 o=2 a=1 : 0 (0)
a=7 o=2 a=2 : -1.2273 (1)
a=7 o=2 a=3 : -1e+10 (1000000)
a=7 o=2 a=4 : 0 (0)
a=7 o=2 a=5 : 0 (0)
a=7 o=2 a=6 : 0 (0)
a=7 o=2 a=7 : 0 (0)
Sample

# # # # 
# * 1$#
# 2$. #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
Check 3
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 27 steps, with total reward -5.93855
Total reward = -5.64162
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -5.64162 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 8 steps, with total reward -3.01663
Total reward = -2.8658
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.8658 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -5.64162 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# 2X* #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.8658 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
a=7 : -5.64162 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -1.85494
Total reward = -1.67408
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.8658 (1)
a=2 : 3.44983 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : -1.76219 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
a=7 : -5.64162 (1)
Starting simulation

# # # # 
# * 1X#
# 2$. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 18.0737
Total reward = 16.3116
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.8658 (1)
a=2 : 3.44983 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : -1.76219 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.1495 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 17.1701 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
a=6 o=2 a=7 : 0 (0)
a=7 : -5.64162 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Adding sample:

# # # # 
# * 1X#
# 2X. #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2X* #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Sample
Reward -10

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward -7.95787
Total reward = -7.18198
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.8658 (1)
a=2 : 3.44983 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : -1.76219 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.03898 (3)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 17.1701 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -7.55998 (1)
a=6 o=2 a=7 : 0 (0)
a=7 : -5.64162 (1)
Starting simulation

# # # # 
# * 1X#
# 2X. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2X. #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# 2X. #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# 2X. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# 2X* #
# # # # 
E
Reward 10

# # # # 
# . . #
# 2X* #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = -0.92625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.89602 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -0.975 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 3.44983 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : -1.76219 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.03898 (3)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 17.1701 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -7.55998 (1)
a=6 o=2 a=7 : 0 (0)
a=7 : -5.64162 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 9.625 [2, 27]
Total reward: 1.57287 [-7.18198, 16.3116]
Policy after 8 simulations
MCTS Policy:
a=6 : 5.03898 (3)
a=6 o=2 a=2 : 17.1701 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.89602 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -0.975 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 0 (0)
a=2 : 3.44983 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : -1.76219 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.03898 (3)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 17.1701 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -7.55998 (1)
a=6 o=2 a=7 : 0 (0)
a=7 : -5.64162 (1)
Check 2

# # # # 
# * 1$#
# 2$. #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 11 attempts
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 20.5916
Total reward = 19.562
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 19.562 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 9.5848
Total reward = 9.10556
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 19.562 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : 9.10556 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 19.562 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
a=7 : 9.10556 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 22 steps, with total reward 18.1849
Total reward = 16.4119
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 17.9869 (2)
a=2 o=0 a=0 : 17.2756 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
a=7 : 9.10556 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * . #
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 11 steps, with total reward 14.5611
Total reward = 13.1414
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 16.3718 (3)
a=2 o=0 a=0 : 17.2756 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 13.8331 (1)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
a=7 : 9.10556 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=1 o=0 
Adding sample:

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 2 steps, with total reward 19.5
Total reward = 17.5987
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.5494 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 18.525 (1)
a=2 : 16.3718 (3)
a=2 o=0 a=0 : 17.2756 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 13.8331 (1)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
a=7 : 9.10556 (1)
Starting simulation

# # # # 
# * 1$#
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=7 o=1 
Adding sample:

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . . #
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 7 steps, with total reward 15.9247
Total reward = 14.372
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.5494 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 18.525 (1)
a=2 : 16.3718 (3)
a=2 o=0 a=0 : 17.2756 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 13.8331 (1)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
a=7 : 11.7388 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 15.1284 (1)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 0 (0)
Tree depth: 0.5 [0, 1]
Rollout depth: 9.75 [1, 22]
Total reward: 13.4796 [8.14506, 19.562]
Policy after 8 simulations
MCTS Policy:
a=2 : 16.3718 (3)
a=2 o=0 a=0 : 17.2756 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.5494 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : 18.525 (1)
a=2 : 16.3718 (3)
a=2 o=0 a=0 : 17.2756 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : 0 (0)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 13.8331 (1)
a=2 o=0 a=7 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
a=7 : 11.7388 (2)
a=7 o=1 a=0 : -1e+10 (1000000)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : 15.1284 (1)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : -1e+10 (1000000)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 0 (0)
S

# # # # 
# . 1$#
# * . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
E
Reward 10

# # # # 
# . * #
# 2$. #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : 8.14506 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# 2$* #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 5.13342
MCTS Values:
a=0 : 0 (0)
a=1 : 5.13342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : 8.14506 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 15.4036
MCTS Values:
a=0 : 0 (0)
a=1 : 5.13342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : 8.14506 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 13.8331
Total reward = 13.1414
MCTS Values:
a=0 : 13.1414 (1)
a=1 : 5.13342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : 8.14506 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 15.4036
MCTS Values:
a=0 : 13.1414 (1)
a=1 : 5.13342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (2)
a=4 o=0 a=0 : 0 (0)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : -1e+10 (1000000)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 5.688 (1)
a=4 o=0 a=7 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : 8.14506 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=0 o=0 
Adding sample:

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
E
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# 2$* #
# # # # 
N
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
Check 3
Observed bad
Reward 0

# # # # 
# . * #
# 2$. #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# 2$. #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 10.5648
Total reward = 9.53469
MCTS Values:
a=0 : 11.338 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 10.0365 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 0 (0)
a=0 o=0 a=7 : 0 (0)
a=1 : 5.13342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (2)
a=4 o=0 a=0 : 0 (0)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : -1e+10 (1000000)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 5.688 (1)
a=4 o=0 a=7 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : 8.14506 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 3
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=7 o=1 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
Sample
Reward 10

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 9.50966
Total reward = 18.0825
MCTS Values:
a=0 : 11.338 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 10.0365 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 0 (0)
a=0 o=0 a=7 : 0 (0)
a=1 : 5.13342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (2)
a=4 o=0 a=0 : 0 (0)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : -1e+10 (1000000)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 5.688 (1)
a=4 o=0 a=7 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : 13.1138 (2)
a=7 o=1 a=0 : 0 (0)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : -1e+10 (1000000)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : 19.0342 (1)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 0 (0)
Tree depth: 0.375 [0, 1]
Rollout depth: 11 [4, 16]
Total reward: 11.5728 [5.13342, 18.0825]
Policy after 8 simulations
MCTS Policy:
a=4 : 15.4036 (2)
a=4 o=0 a=6 : 5.688 (1)
Values after 8 simulations
MCTS Values:
a=0 : 11.338 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 10.0365 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 0 (0)
a=0 o=0 a=7 : 0 (0)
a=1 : 5.13342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (2)
a=4 o=0 a=0 : 0 (0)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : -1e+10 (1000000)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 5.688 (1)
a=4 o=0 a=7 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
a=7 : 13.1138 (2)
a=7 o=1 a=0 : 0 (0)
a=7 o=1 a=1 : 0 (0)
a=7 o=1 a=2 : -1e+10 (1000000)
a=7 o=1 a=3 : -1e+10 (1000000)
a=7 o=1 a=4 : 19.0342 (1)
a=7 o=1 a=5 : -1e+10 (1000000)
a=7 o=1 a=6 : 0 (0)
a=7 o=1 a=7 : 0 (0)
Sample

# # # # 
# . 1$#
# * . #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.40127
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 4.40127 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 8.0671 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 6.98337 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 4.40127 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 a=0 o=0 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 14.7793
Total reward = 13.3383
MCTS Values:
a=0 : 10.538 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 14.0403 (1)
a=0 o=0 a=7 : -1e+10 (1000000)
a=1 : 8.0671 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 6.98337 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 4.40127 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 a=6 o=1 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward 9.88735
Total reward = 8.92334
MCTS Values:
a=0 : 10.538 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 14.0403 (1)
a=0 o=0 a=7 : -1e+10 (1000000)
a=1 : 8.0671 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 6.98337 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6623 (2)
a=6 o=1 a=0 : 9.39299 (1)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : 9.23677 (3)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 6.98337 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 14.0403 (1)
a=0 o=0 a=7 : -1e+10 (1000000)
a=1 : 8.0671 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 6.98337 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6623 (2)
a=6 o=1 a=0 : 9.39299 (1)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Adding sample:

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : 9.23677 (3)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 6.98337 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 14.0403 (1)
a=0 o=0 a=7 : -1e+10 (1000000)
a=1 : 8.3864 (3)
a=1 o=0 a=0 : 9.5 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 6.98337 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6623 (2)
a=6 o=1 a=0 : 9.39299 (1)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0.625 [0, 1]
Rollout depth: 8.375 [1, 21]
Total reward: 8.27426 [4.40127, 13.3383]
Policy after 8 simulations
MCTS Policy:
a=0 : 9.23677 (3)
a=0 o=0 a=6 : 14.0403 (1)
Values after 8 simulations
MCTS Values:
a=0 : 9.23677 (3)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 6.98337 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 14.0403 (1)
a=0 o=0 a=7 : -1e+10 (1000000)
a=1 : 8.3864 (3)
a=1 o=0 a=0 : 9.5 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 6.98337 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6623 (2)
a=6 o=1 a=0 : 9.39299 (1)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
N

# # # # 
# * 1$#
# . . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 a=0 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 a=0 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 a=0 o=0 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 19.5
Total reward = 17.5987
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 13.3119 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 18.525 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 a=0 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : 7.60398 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.025 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 13.3119 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 18.525 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : 7.60398 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.025 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.2024 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 18.525 (1)
a=6 o=1 a=2 : 7.35092 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.03292 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 9.025 (1)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : 7.60398 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.025 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.2024 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 18.525 (1)
a=6 o=1 a=2 : 7.35092 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0.625 [0, 1]
Rollout depth: 3.42857 [2, 8]
Total reward: 9.48923 [6.6342, 17.5987]
Policy after 8 simulations
MCTS Policy:
a=6 : 11.2024 (3)
a=6 o=1 a=1 : 18.525 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.03292 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 9.025 (1)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : 7.60398 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.025 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.2024 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 18.525 (1)
a=6 o=1 a=2 : 7.35092 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 a=0 o=0 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.4036 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 a=0 o=0 a=6 o=1 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 15.9247
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.7123 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 16.7628 (1)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.4036 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 a=0 o=0 a=6 o=1 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 14.9036
Total reward = 13.4505
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.7123 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 16.7628 (1)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : 9.71893 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 14.1584 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.4036 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 a=0 o=0 a=6 o=1 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.7123 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 16.7628 (1)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : 9.71893 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 14.1584 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98868 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 9.025 (1)
a=6 o=1 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.6416 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 16.7628 (1)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : 9.71893 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 14.1584 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=2 o=0 a=7 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98868 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 9.025 (1)
a=6 o=1 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.6416 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 16.7628 (1)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : 8.69069 (3)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 14.1584 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 6.98337 (1)
a=2 o=0 a=7 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98868 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 9.025 (1)
a=6 o=1 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0.625 [0, 1]
Rollout depth: 7.14286 [1, 13]
Total reward: 9.37176 [5.4036, 15.9247]
Policy after 8 simulations
MCTS Policy:
a=1 : 11.6416 (3)
a=1 o=0 a=6 : 16.7628 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.6416 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 16.7628 (1)
a=1 o=0 a=7 : -1e+10 (1000000)
a=2 : 8.69069 (3)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 14.1584 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 6.98337 (1)
a=2 o=0 a=7 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98868 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 9.025 (1)
a=6 o=1 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 a=0 o=0 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.57375 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.57375 (1)
a=4 : 19.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 18.5737
Total reward = 17.6451
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.57375 (1)
a=4 : 19.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 17.6451 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.73781 (1)
a=3 : 8.57375 (1)
a=4 : 19.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 17.6451 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.73781 (1)
a=3 : 8.57375 (1)
a=4 : 19.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 17.6451 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 a=0 o=0 a=6 o=1 a=1 o=0 a=4 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 14.8767
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.73781 (1)
a=3 : 8.57375 (1)
a=4 : 17.1884 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 5.13342 (1)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=4 o=0 a=7 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.6451 (1)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 a=0 o=0 a=6 o=1 a=1 o=0 a=6 o=1 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 3.40562
Total reward = 3.07357
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.73781 (1)
a=3 : 8.57375 (1)
a=4 : 17.1884 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 5.13342 (1)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=4 o=0 a=7 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.3593 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 3.23534 (1)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 7.73781 (1)
a=3 : 8.57375 (1)
a=4 : 17.1884 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 5.13342 (1)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=4 o=0 a=7 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.3593 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 3.23534 (1)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0.25 [0, 1]
Rollout depth: 8 [1, 22]
Total reward: 11.4259 [3.07357, 19.5]
Policy after 8 simulations
MCTS Policy:
a=4 : 17.1884 (2)
a=4 o=0 a=3 : 5.13342 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 7.73781 (1)
a=3 : 8.57375 (1)
a=4 : 17.1884 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 5.13342 (1)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=4 o=0 a=7 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.3593 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 3.23534 (1)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=1 a=7 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 a=0 o=0 a=6 o=1 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 6.98337 (1)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 6.98337 (1)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 a=0 o=0 a=6 o=1 a=1 o=0 a=4 o=0 a=3 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 6.98337 (1)
a=3 : 6.99256 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 6.98337 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=3 o=0 a=7 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Expanding node: a=6 o=1 a=5 o=1 a=4 o=0 a=6 o=1 a=2 o=0 a=4 o=0 a=0 o=0 a=6 o=1 a=1 o=0 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.00419 (2)
a=2 o=0 a=0 : 9.5 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=2 o=0 a=7 : -1e+10 (1000000)
a=3 : 6.99256 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 6.98337 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=3 o=0 a=7 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.00419 (2)
a=2 o=0 a=0 : 9.5 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=2 o=0 a=7 : -1e+10 (1000000)
a=3 : 6.99256 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 6.98337 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=3 o=0 a=7 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 7.54753 (3)
a=2 o=0 a=0 : 9.5 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 6.98337 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=2 o=0 a=7 : -1e+10 (1000000)
a=3 : 6.99256 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 6.98337 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=3 o=0 a=7 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
Tree depth: 0.375 [0, 1]
Rollout depth: 5.6 [1, 7]
Total reward: 8.32846 [6.6342, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 7.54753 (3)
a=2 o=0 a=0 : 9.5 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 6.98337 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=2 o=0 a=7 : -1e+10 (1000000)
a=3 : 6.99256 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 6.98337 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=3 o=0 a=7 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
a=7 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 29.0527, average = 13.3753
Undiscounted return = 40, average = 17
Simulations = 8
Runs = 10
Undiscounted return = 17 +- 4.01248
Discounted return = 13.3753 +- 3.1456
Time = 0.0112911
