Initialising fast UCB table... done
Main runs
Starting run 1 with 1 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 15.1334
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.1334 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 13 [13, 13]
Total reward: 15.1334 [15.1334, 15.1334]
Policy after 1 simulations
MCTS Policy:
a=4 : 15.1334 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.1334 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# * 1X#
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward -0.975
Total reward = -0.92625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: -0.92625 [-0.92625, -0.92625]
Policy after 1 simulations
MCTS Policy:
a=2 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
S

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.4036 (1)
Tree depth: 0 [0, 0]
Rollout depth: 12 [12, 12]
Total reward: 5.4036 [5.4036, 5.4036]
Policy after 1 simulations
MCTS Policy:
a=6 : 5.4036 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.4036 (1)
Check 2

# # # # 
# . 1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=2 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
E

# # # # 
# . 1X#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=3 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
W

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 28 steps, with total reward 2.50344
Total reward = 2.37827
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 2.37827 (1)
Tree depth: 0 [0, 0]
Rollout depth: 28 [28, 28]
Total reward: 2.37827 [2.37827, 2.37827]
Policy after 1 simulations
MCTS Policy:
a=6 : 2.37827 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 2.37827 (1)
Check 2

# # # # 
# . 1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 a=6 o=2 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 19 steps, with total reward -4.17292
Total reward = -3.96427
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -3.96427 (1)
Tree depth: 0 [0, 0]
Rollout depth: 19 [19, 19]
Total reward: -3.96427 [-3.96427, -3.96427]
Policy after 1 simulations
MCTS Policy:
a=0 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -3.96427 (1)
N

# # # # 
# * 1X#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 a=6 o=2 a=0 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 12 [12, 12]
Total reward: 5.4036 [5.4036, 5.4036]
Policy after 1 simulations
MCTS Policy:
a=2 : 5.4036 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
S

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 a=6 o=2 a=0 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 8.57375 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=1 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 8.57375 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
E

# # # # 
# . 1X#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 a=6 o=2 a=0 o=0 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 19.5
Total reward = 18.525
MCTS Values:
a=0 : 18.525 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 18.525 [18.525, 18.525]
Policy after 1 simulations
MCTS Policy:
a=0 : 18.525 (1)
Values after 1 simulations
MCTS Values:
a=0 : 18.525 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
N

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 a=6 o=2 a=0 o=0 a=2 o=0 a=1 o=0 a=0 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 17.3509
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 17.3509 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 17.3509 [17.3509, 17.3509]
Policy after 1 simulations
MCTS Policy:
a=4 : 17.3509 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 17.3509 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 a=6 o=2 a=0 o=0 a=2 o=0 a=1 o=0 a=0 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.70063, average = 9.70063
Undiscounted return = 10, average = 10
Starting run 2 with 1 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 18.5738 [18.5738, 18.5738]
Policy after 1 simulations
MCTS Policy:
a=4 : 18.5738 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# * 1$#
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=2 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
S

# # # # 
# . 1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=6 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
Check 2

# # # # 
# . 1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=2 o=0 a=6 o=1 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=6 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
Check 2

# # # # 
# . 1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=4 o=0 a=6 o=1 a=2 o=0 a=6 o=1 a=6 o=1 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
Check 2

# # # # 
# . 1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=2 o=0 a=6 o=1 a=6 o=1 a=6 o=1 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 20 steps, with total reward 7.74568
Total reward = 7.3584
MCTS Values:
a=0 : 0 (0)
a=1 : 7.3584 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 20 [20, 20]
Total reward: 7.3584 [7.3584, 7.3584]
Policy after 1 simulations
MCTS Policy:
a=1 : 7.3584 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 7.3584 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=2 o=0 a=6 o=1 a=6 o=1 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = -3.01663, average = 3.342
Undiscounted return = 0, average = 5
Starting run 3 with 1 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 30 steps, with total reward 10.8331
Total reward = 10.2915
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 10.2915 (1)
Tree depth: 0 [0, 0]
Rollout depth: 30 [30, 30]
Total reward: 10.2915 [10.2915, 10.2915]
Policy after 1 simulations
MCTS Policy:
a=6 : 10.2915 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 10.2915 (1)
Check 2

# # # # 
# 0$* #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=1 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 19.5 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 19.5 [19.5, 19.5]
Policy after 1 simulations
MCTS Policy:
a=4 : 19.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 19.5 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=1 a=4 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.35092 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=2 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.35092 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
S

# # # # 
# 0$. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=1 a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0$. #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 17.1701, average = 7.95136
Undiscounted return = 20, average = 10
Starting run 4 with 1 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward -1.84257
Total reward = -1.75044
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.75044 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 10 [10, 10]
Total reward: -1.75044 [-1.75044, -1.75044]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.75044 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -2.14908
Total reward = -2.04163
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -2.04163 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: -2.04163 [-2.04163, -2.04163]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -2.04163 (1)
a=6 : 0 (0)
Check 2

# # # # 
# 0$* #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=1 o=0 a=6 o=2 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=6 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 9.5 (1)
Check 2

# # # # 
# 0$* #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -0.33171
Total reward = -0.315125
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -0.315125 (1)
Tree depth: 0 [0, 0]
Rollout depth: 10 [10, 10]
Total reward: -0.315125 [-0.315125, -0.315125]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -0.315125 (1)
Sample

# # # # 
# 0$* #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=2 a=6 o=2 a=4 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=5 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : -1e+10 (1000000)
Check 1

# # # # 
# 0$* #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=1 o=0 a=6 o=2 a=6 o=2 a=4 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 8.80922
Total reward = 8.36876
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.36876 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 22 [22, 22]
Total reward: 8.36876 [8.36876, 8.36876]
Policy after 1 simulations
MCTS Policy:
a=2 : 8.36876 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.36876 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
S

# # # # 
# 0$. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=2 a=6 o=2 a=4 o=0 a=5 o=1 a=2 o=0 
Starting simulation

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -2.58638
Total reward = -2.45706
MCTS Values:
a=0 : -2.45706 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 11 [11, 11]
Total reward: -2.45706 [-2.45706, -2.45706]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -2.45706 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
W

# # # # 
# 0$. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=2 a=6 o=2 a=4 o=0 a=5 o=1 a=2 o=0 a=3 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=1 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0$. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=2 a=6 o=2 a=4 o=0 a=5 o=1 a=2 o=0 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0$. #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = -1.93955, average = 5.47863
Undiscounted return = 0, average = 7.5
Starting run 5 with 1 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 19 steps, with total reward 3.97214
Total reward = 3.77354
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.77354 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 19 [19, 19]
Total reward: 3.77354 [3.77354, 3.77354]
Policy after 1 simulations
MCTS Policy:
a=2 : 3.77354 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.77354 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
S

# # # # 
# 0X1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.40127
MCTS Values:
a=0 : 0 (0)
a=1 : 4.40127 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 16 [16, 16]
Total reward: 4.40127 [4.40127, 4.40127]
Policy after 1 simulations
MCTS Policy:
a=1 : 4.40127 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 4.40127 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X1X#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 9.025 (1)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.025 [9.025, 9.025]
Policy after 1 simulations
MCTS Policy:
a=6 : 9.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 9.025 (1)
Check 2

# # # # 
# 0X1X#
# . * #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 
Starting simulation

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=5 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 0 (0)
Check 1

# # # # 
# 0X1X#
# . * #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=5 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 0 (0)
Check 1

# # # # 
# 0X1X#
# . * #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 25 steps, with total reward 3.6366
Total reward = 3.45477
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 3.45477 (1)
Tree depth: 0 [0, 0]
Rollout depth: 25 [25, 25]
Total reward: 3.45477 [3.45477, 3.45477]
Policy after 1 simulations
MCTS Policy:
a=6 : 3.45477 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 3.45477 (1)
Check 2

# # # # 
# 0X1X#
# . * #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=5 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Check 1

# # # # 
# 0X1X#
# . * #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=0 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
N

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 16.3025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 16.3025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 9 [9, 9]
Total reward: 16.3025 [16.3025, 16.3025]
Policy after 1 simulations
MCTS Policy:
a=4 : 16.3025 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 16.3025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# 0X* #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=3 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
W

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 a=3 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.025 [9.025, 9.025]
Policy after 1 simulations
MCTS Policy:
a=1 : 9.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 15.8025
Total reward = 15.0124
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 15.0124 (1)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 10 [10, 10]
Total reward: 15.0124 [15.0124, 15.0124]
Policy after 1 simulations
MCTS Policy:
a=5 : 15.0124 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 15.0124 (1)
a=6 : -1e+10 (1000000)
Check 1

# # # # 
# 0X* #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 a=3 o=0 a=1 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward -1.35494
Total reward = -1.28719
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1.28719 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: -1.28719 [-1.28719, -1.28719]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1.28719 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
S

# # # # 
# 0X. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 a=3 o=0 a=1 o=0 a=5 o=2 a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 6.6342 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 8 [8, 8]
Total reward: 6.6342 [6.6342, 6.6342]
Policy after 1 simulations
MCTS Policy:
a=3 : 6.6342 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 6.6342 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
W

# # # # 
# 0X. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 a=3 o=0 a=1 o=0 a=5 o=2 a=2 o=0 a=3 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=0 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 a=3 o=0 a=1 o=0 a=5 o=2 a=2 o=0 a=3 o=0 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 16.3025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.3025 (1)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 9 [9, 9]
Total reward: 16.3025 [16.3025, 16.3025]
Policy after 1 simulations
MCTS Policy:
a=4 : 16.3025 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.3025 (1)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 a=3 o=0 a=1 o=0 a=5 o=2 a=2 o=0 a=3 o=0 a=0 o=0 a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 10 [10, 10]
Total reward: 5.98737 [5.98737, 5.98737]
Policy after 1 simulations
MCTS Policy:
a=2 : 5.98737 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
S

# # # # 
# . . #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 a=3 o=0 a=1 o=0 a=5 o=2 a=2 o=0 a=3 o=0 a=0 o=0 a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=1 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . . #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 a=3 o=0 a=1 o=0 a=5 o=2 a=2 o=0 a=3 o=0 a=0 o=0 a=4 o=0 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . . #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = -7.29497, average = 2.92391
Undiscounted return = -10, average = 4
Starting run 6 with 1 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 17.5987
Total reward = 16.7188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.7188 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 16.7188 [16.7188, 16.7188]
Policy after 1 simulations
MCTS Policy:
a=1 : 16.7188 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.7188 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 11.0355
Total reward = 10.4837
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 10.4837 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 17 [17, 17]
Total reward: 10.4837 [10.4837, 10.4837]
Policy after 1 simulations
MCTS Policy:
a=2 : 10.4837 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 10.4837 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
S

# # # # 
# 0$1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0$1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 9.025, average = 3.94076
Undiscounted return = 10, average = 5
Starting run 7 with 1 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 16.3759
Total reward = 15.5571
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 15.5571 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: 15.5571 [15.5571, 15.5571]
Policy after 1 simulations
MCTS Policy:
a=2 : 15.5571 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 15.5571 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
S

# # # # 
# 0X1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 5.688 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 11 [11, 11]
Total reward: 5.688 [5.688, 5.688]
Policy after 1 simulations
MCTS Policy:
a=5 : 5.688 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 5.688 (1)
a=6 : 0 (0)
Check 1

# # # # 
# 0X1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=2 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 8.14506 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=6 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 8.14506 (1)
Check 2

# # # # 
# 0X1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.35092 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=6 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.35092 (1)
Check 2

# # # # 
# 0X1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=2 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 7.73781 [7.73781, 7.73781]
Policy after 1 simulations
MCTS Policy:
a=1 : 7.73781 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X1X#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=2 a=6 o=2 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Check 2

# # # # 
# 0X1X#
# . * #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=2 o=0 a=5 o=2 a=6 o=2 a=6 o=2 a=1 o=0 a=6 o=2 
Starting simulation

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X1X#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 7.35092, average = 4.42792
Undiscounted return = 10, average = 5.71429
Starting run 8 with 1 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 14.9036
Total reward = 14.1584
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 14.1584 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 13 [13, 13]
Total reward: 14.1584 [14.1584, 14.1584]
Policy after 1 simulations
MCTS Policy:
a=5 : 14.1584 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 14.1584 (1)
a=6 : 0 (0)
Check 1

# # # # 
# * 1X#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 17.3509
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 17.3509 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 17.3509 [17.3509, 17.3509]
Policy after 1 simulations
MCTS Policy:
a=4 : 17.3509 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 17.3509 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# * 1X#
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 9 attempts
Expanding node: a=5 o=1 a=4 o=0 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 16 steps, with total reward -1.66958
Total reward = -1.5861
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.5861 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 16 [16, 16]
Total reward: -1.5861 [-1.5861, -1.5861]
Policy after 1 simulations
MCTS Policy:
a=2 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.5861 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=4 o=0 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 18 steps, with total reward -5.3188
Total reward = -5.05286
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -5.05286 (1)
Tree depth: 0 [0, 0]
Rollout depth: 18 [18, 18]
Total reward: -5.05286 [-5.05286, -5.05286]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -5.05286 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=4 o=0 a=6 o=2 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 14.6329
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 14.6329 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 15 [15, 15]
Total reward: 14.6329 [14.6329, 14.6329]
Policy after 1 simulations
MCTS Policy:
a=4 : 14.6329 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 14.6329 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=4 o=0 a=6 o=2 a=6 o=2 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.11311, average = 5.01357
Undiscounted return = 10, average = 6.25
Starting run 9 with 1 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=1 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.025 [9.025, 9.025]
Policy after 1 simulations
MCTS Policy:
a=5 : 9.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 0 (0)
Check 1

# # # # 
# 0X* #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=1 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 17.7378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 17.7378 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 17.7378 [17.7378, 17.7378]
Policy after 1 simulations
MCTS Policy:
a=4 : 17.7378 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 17.7378 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=2 a=4 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=5 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : -1e+10 (1000000)
Check 1

# # # # 
# 0X* #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=1 o=0 a=5 o=2 a=4 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 17.1701, average = 6.36429
Undiscounted return = 20, average = 7.77778
Starting run 10 with 1 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 19.025 [19.025, 19.025]
Policy after 1 simulations
MCTS Policy:
a=4 : 19.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# * 1$#
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 39 steps, with total reward -0.236876
Total reward = -0.225033
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.225033 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 39 [39, 39]
Total reward: -0.225033 [-0.225033, -0.225033]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.225033 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: 6.98337 [6.98337, 6.98337]
Policy after 1 simulations
MCTS Policy:
a=2 : 6.98337 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
S

# # # # 
# . 1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.025 [9.025, 9.025]
Policy after 1 simulations
MCTS Policy:
a=6 : 9.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Check 2

# # # # 
# . 1$#
# . * #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 a=2 o=0 a=6 o=1 
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 18.1451, average = 7.54237
Undiscounted return = 20, average = 9
Simulations = 1
Runs = 10
Undiscounted return = 9 +- 2.98329
Discounted return = 7.54237 +- 2.69946
Time = 0.001916
Starting run 1 with 2 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 13.2067
Total reward = 12.5463
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 12.5463 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 13.5 [11, 16]
Total reward: 9.11717 [5.688, 12.5463]
Policy after 2 simulations
MCTS Policy:
a=2 : 12.5463 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 12.5463 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
S

# # # # 
# 0X1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [1, 3]
Total reward: 9.03688 [8.57375, 9.5]
Policy after 2 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 25 steps, with total reward 2.53261
Total reward = 2.40598
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 2.40598 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 25 [25, 25]
Total reward: 6.20299 [2.40598, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 2.40598 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 9.025, average = 9.025
Undiscounted return = 10, average = 10
Starting run 2 with 2 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 14.6329
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 14.6329 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 16.6342
Total reward = 15.8025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.8025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 14.6329 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 12 [9, 15]
Total reward: 15.2177 [14.6329, 15.8025]
Policy after 2 simulations
MCTS Policy:
a=1 : 15.8025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.8025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 14.6329 (1)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 11.1208
Total reward = 10.5648
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 10.5648 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 15.9247
Total reward = 25.1284
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 25.1284 (1)
a=5 : 10.5648 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 10.5 [7, 14]
Total reward: 17.8466 [10.5648, 25.1284]
Policy after 2 simulations
MCTS Policy:
a=4 : 25.1284 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 25.1284 (1)
a=5 : 10.5648 (1)
a=6 : 0 (0)
Sample

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 9.025 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward -1.81874
Total reward = -1.7278
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.7278 (1)
a=3 : 9.025 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 11.5 [2, 21]
Total reward: 3.6486 [-1.7278, 9.025]
Policy after 2 simulations
MCTS Policy:
a=3 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.7278 (1)
a=3 : 9.025 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
W

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=4 o=0 a=3 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = -2.64908
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 5 [4, 6]
Total reward: 2.74799 [-2.64908, 8.14506]
Policy after 2 simulations
MCTS Policy:
a=2 : 8.14506 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
S

# # # # 
# 0X. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=4 o=0 a=3 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 12.3222
Total reward = 11.7061
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 11.7061 (1)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 37 steps, with total reward 4.08123
Total reward = 3.87717
MCTS Values:
a=0 : 3.87717 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 11.7061 (1)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 24.5 [12, 37]
Total reward: 7.79163 [3.87717, 11.7061]
Policy after 2 simulations
MCTS Policy:
a=5 : 11.7061 (1)
Values after 2 simulations
MCTS Values:
a=0 : 3.87717 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 11.7061 (1)
a=6 : -1e+10 (1000000)
Check 1

# # # # 
# 0X. #
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=4 o=0 a=3 o=0 a=2 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 9.025 (1)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 3 [2, 4]
Total reward: 8.58503 [8.14506, 9.025]
Policy after 2 simulations
MCTS Policy:
a=0 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : 9.025 (1)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=4 o=0 a=3 o=0 a=2 o=0 a=5 o=2 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = -1.42625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 2.5 [2, 3]
Total reward: 3.79938 [-1.42625, 9.025]
Policy after 2 simulations
MCTS Policy:
a=2 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
S

# # # # 
# 0X. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=4 o=0 a=3 o=0 a=2 o=0 a=5 o=2 a=0 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.35092 (1)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 3.5 [1, 6]
Total reward: 8.42546 [7.35092, 9.5]
Policy after 2 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.35092 (1)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0X. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=4 o=0 a=3 o=0 a=2 o=0 a=5 o=2 a=0 o=0 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 9.28688 [8.57375, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0X. #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 16.1342, average = 12.5796
Undiscounted return = 20, average = 15
Starting run 3 with 2 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = -3.01663
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -3.01663 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.01663 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: 1.98337 [-3.01663, 6.98337]
Policy after 2 simulations
MCTS Policy:
a=2 : 6.98337 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.01663 (1)
a=5 : 0 (0)
a=6 : 0 (0)
S

# # # # 
# 0X1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 16.0084
Total reward = 15.208
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 15.208 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 15.208 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6.5 [5, 8]
Total reward: 11.4729 [7.73781, 15.208]
Policy after 2 simulations
MCTS Policy:
a=6 : 15.208 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 15.208 (1)
Check 2

# # # # 
# 0X1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=2 o=0 a=6 o=2 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 20 steps, with total reward 3.77354
Total reward = 3.58486
MCTS Values:
a=0 : 0 (0)
a=1 : 3.58486 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : 4.87675 (1)
a=1 : 3.58486 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 17 [14, 20]
Total reward: 4.2308 [3.58486, 4.87675]
Policy after 2 simulations
MCTS Policy:
a=0 : 4.87675 (1)
Values after 2 simulations
MCTS Values:
a=0 : 4.87675 (1)
a=1 : 3.58486 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
N

# # # # 
# * 1X#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward -2.74146
Total reward = -2.60439
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -2.60439 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 11 [9, 13]
Total reward: 1.84905 [-2.60439, 6.30249]
Policy after 2 simulations
MCTS Policy:
a=1 : 6.30249 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -2.60439 (1)
a=6 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1.5 [1, 2]
Total reward: 9.2625 [9.025, 9.5]
Policy after 2 simulations
MCTS Policy:
a=5 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : 0 (0)
Check 1

# # # # 
# 0X* #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 a=1 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 5.98737 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6.5 [3, 10]
Total reward: 7.28056 [5.98737, 8.57375]
Policy after 2 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 5.98737 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Check 2

# # # # 
# 0X* #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 a=1 o=0 a=5 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = -0.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [1, 3]
Total reward: 4.03688 [-0.5, 8.57375]
Policy after 2 simulations
MCTS Policy:
a=2 : 8.57375 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 0 (0)
a=6 : 0 (0)
S

# # # # 
# 0X1X#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 a=1 o=0 a=5 o=2 a=6 o=2 a=2 o=0 
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3.5 [3, 4]
Total reward: 8.35941 [8.14506, 8.57375]
Policy after 2 simulations
MCTS Policy:
a=3 : 8.57375 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 0 (0)
W

# # # # 
# 0X1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 a=1 o=0 a=5 o=2 a=6 o=2 a=2 o=0 a=3 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : 6.98337 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4.5 [2, 7]
Total reward: 8.00419 [6.98337, 9.025]
Policy after 2 simulations
MCTS Policy:
a=5 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : 6.98337 (1)
Check 1

# # # # 
# 0X1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 a=1 o=0 a=5 o=2 a=6 o=2 a=2 o=0 a=3 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 24 steps, with total reward 3.07357
Total reward = 2.91989
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 2.91989 (1)
a=6 : 6.6342 (1)
Tree depth: 0 [0, 0]
Rollout depth: 16 [8, 24]
Total reward: 4.77705 [2.91989, 6.6342]
Policy after 2 simulations
MCTS Policy:
a=6 : 6.6342 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 2.91989 (1)
a=6 : 6.6342 (1)
Check 2

# # # # 
# 0X1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 a=1 o=0 a=5 o=2 a=6 o=2 a=2 o=0 a=3 o=0 a=5 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 21 steps, with total reward -6.68978
Total reward = -6.35529
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -6.35529 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 11 [1, 21]
Total reward: 1.57236 [-6.35529, 9.5]
Policy after 2 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -6.35529 (1)
a=6 : 0 (0)
E

# # # # 
# 0X1X#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 a=1 o=0 a=5 o=2 a=6 o=2 a=2 o=0 a=3 o=0 a=5 o=2 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 21 steps, with total reward 3.58486
Total reward = 3.40562
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 3.40562 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 3.40562 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 11 [1, 21]
Total reward: 6.45281 [3.40562, 9.5]
Policy after 2 simulations
MCTS Policy:
a=5 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 3.40562 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : 0 (0)
Check 1

# # # # 
# 0X1X#
# . * #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 a=1 o=0 a=5 o=2 a=6 o=2 a=2 o=0 a=3 o=0 a=5 o=2 a=6 o=2 a=1 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -10.3885
Total reward = -9.86912
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : -9.86912 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 11 [11, 11]
Total reward: 0.065439 [-9.86912, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : -9.86912 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X1X#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 5.4036, average = 10.1876
Undiscounted return = 10, average = 13.3333
Starting run 4 with 2 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 26 steps, with total reward -6.2511
Total reward = -5.93855
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -5.93855 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 39 steps, with total reward 6.83776
Total reward = 6.49587
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.49587 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -5.93855 (1)
Tree depth: 0 [0, 0]
Rollout depth: 32.5 [26, 39]
Total reward: 0.278662 [-5.93855, 6.49587]
Policy after 2 simulations
MCTS Policy:
a=2 : 6.49587 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.49587 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -5.93855 (1)
S

# # # # 
# 0X1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward -4.17248
Total reward = -3.96386
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -3.96386 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -3.96386 (1)
a=6 : 6.30249 (1)
Tree depth: 0 [0, 0]
Rollout depth: 13 [9, 17]
Total reward: 1.16932 [-3.96386, 6.30249]
Policy after 2 simulations
MCTS Policy:
a=6 : 6.30249 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -3.96386 (1)
a=6 : 6.30249 (1)
Check 2

# # # # 
# 0X1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 3.74068
Total reward = 3.55365
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 3.55365 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 35 steps, with total reward 5.21473
Total reward = 4.954
MCTS Values:
a=0 : 0 (0)
a=1 : 4.954 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 3.55365 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 24 [13, 35]
Total reward: 4.25382 [3.55365, 4.954]
Policy after 2 simulations
MCTS Policy:
a=1 : 4.954 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 4.954 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 3.55365 (1)
a=6 : 0 (0)
E

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 16 steps, with total reward 10.9354
Total reward = 10.3886
MCTS Values:
a=0 : 10.3886 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 10.8154
Total reward = 10.2746
MCTS Values:
a=0 : 10.3886 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 10.2746 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 17 [16, 18]
Total reward: 10.3316 [10.2746, 10.3886]
Policy after 2 simulations
MCTS Policy:
a=0 : 10.3886 (1)
Values after 2 simulations
MCTS Values:
a=0 : 10.3886 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 10.2746 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
N

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=1 o=0 a=0 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -0.794144
Total reward = -0.754436
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -0.754436 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7.5 [7, 8]
Total reward: 2.93988 [-0.754436, 6.6342]
Policy after 2 simulations
MCTS Policy:
a=2 : 6.6342 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -0.754436 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
S

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=1 o=0 a=0 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 9.5 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.75 [9.5, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 9.5 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 7.73781, average = 9.57515
Undiscounted return = 10, average = 12.5
Starting run 5 with 2 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 36 steps, with total reward -3.47259
Total reward = -3.29896
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -3.29896 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -3.29896 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 6.30249 (1)
Tree depth: 0 [0, 0]
Rollout depth: 22.5 [9, 36]
Total reward: 1.50177 [-3.29896, 6.30249]
Policy after 2 simulations
MCTS Policy:
a=6 : 6.30249 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -3.29896 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 6.30249 (1)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 24 steps, with total reward -2.9138
Total reward = 7.23189
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 7.23189 (1)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 13.5 [3, 24]
Total reward: 7.90282 [7.23189, 8.57375]
Policy after 2 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 7.23189 (1)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward -1.00233
Total reward = -0.952217
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.952217 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward -0.367546
Total reward = -10.3492
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.952217 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.3492 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 12.5 [8, 17]
Total reward: -5.65069 [-10.3492, -0.952217]
Policy after 2 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.952217 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.3492 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Check 1

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=6 o=2 a=6 o=2 a=5 o=2 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 31 steps, with total reward 1.63185
Total reward = 1.55025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 1.55025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 28 steps, with total reward 2.50344
Total reward = 2.37827
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 1.55025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 2.37827 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 29.5 [28, 31]
Total reward: 1.96426 [1.55025, 2.37827]
Policy after 2 simulations
MCTS Policy:
a=5 : 2.37827 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 1.55025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 2.37827 (1)
a=6 : 0 (0)
Check 1

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=6 o=2 a=6 o=2 a=5 o=2 a=5 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -2.8658
Total reward = -2.72251
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -2.72251 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -0.38689
Total reward = -0.367546
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -2.72251 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -0.367546 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8 [7, 9]
Total reward: -1.54503 [-2.72251, -0.367546]
Policy after 2 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -2.72251 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -0.367546 (1)
Sample

# # # # 
# * 1X#
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=6 o=2 a=5 o=2 a=5 o=2 a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward -1.16169
Total reward = -1.10361
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.10361 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.10361 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6.5 [5, 8]
Total reward: 3.3171 [-1.10361, 7.73781]
Policy after 2 simulations
MCTS Policy:
a=1 : 7.73781 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.10361 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=6 o=2 a=5 o=2 a=5 o=2 a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = -0.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 4.75 [-0.5, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = -0.794144, average = 7.50129
Undiscounted return = 0, average = 10
Starting run 6 with 2 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 17.3778
Total reward = 16.5089
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.5089 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 52 steps, with total reward 8.46879
Total reward = 8.04535
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.5089 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.04535 (1)
Tree depth: 0 [0, 0]
Rollout depth: 37 [22, 52]
Total reward: 12.2771 [8.04535, 16.5089]
Policy after 2 simulations
MCTS Policy:
a=1 : 16.5089 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.5089 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.04535 (1)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -0.716715
Total reward = -0.680879
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -0.680879 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : -0.680879 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [1, 9]
Total reward: 4.40956 [-0.680879, 9.5]
Policy after 2 simulations
MCTS Policy:
a=2 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : -0.680879 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
S

# # # # 
# 0X1X#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 9.07253 [8.14506, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X1X#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 9.025, average = 7.75525
Undiscounted return = 10, average = 10
Starting run 7 with 2 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 17.3509
Total reward = 16.4834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.4834 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.4834 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [1, 7]
Total reward: 12.9917 [9.5, 16.4834]
Policy after 2 simulations
MCTS Policy:
a=5 : 16.4834 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.4834 (1)
a=6 : 0 (0)
Check 1

# # # # 
# * 1X#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 13.8331
Total reward = 13.1414
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 13.1414 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11.5 [11, 12]
Total reward: 9.41471 [5.688, 13.1414]
Policy after 2 simulations
MCTS Policy:
a=6 : 13.1414 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 13.1414 (1)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward -1.75044
Total reward = -11.6629
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -11.6629 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 14 steps, with total reward -12.6044
Total reward = -11.9742
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -11.6629 (1)
a=5 : 0 (0)
a=6 : -11.9742 (1)
Tree depth: 0 [0, 0]
Rollout depth: 12.5 [11, 14]
Total reward: -11.8185 [-11.9742, -11.6629]
Policy after 2 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -11.6629 (1)
a=5 : 0 (0)
a=6 : -11.9742 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 20 steps, with total reward 9.76091
Total reward = 9.27286
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.27286 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.27286 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 20 [20, 20]
Total reward: 9.63643 [9.27286, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.27286 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 8.57375, average = 7.87217
Undiscounted return = 10, average = 10
Starting run 8 with 2 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 18 steps, with total reward -0.952217
Total reward = -10.9046
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.9046 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.9046 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 9.5 [1, 18]
Total reward: -0.702303 [-10.9046, 9.5]
Policy after 2 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.9046 (1)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 35 steps, with total reward 7.90005
Total reward = 7.50505
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 7.50505 (1)
Tree depth: 0 [0, 0]
Rollout depth: 18.5 [2, 35]
Total reward: 8.26502 [7.50505, 9.025]
Policy after 2 simulations
MCTS Policy:
a=2 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 7.50505 (1)
S

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward -4.62325
Total reward = -4.39209
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -4.39209 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 9.5 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -4.39209 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 8 [1, 15]
Total reward: 2.55396 [-4.39209, 9.5]
Policy after 2 simulations
MCTS Policy:
a=0 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : 9.5 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -4.39209 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
N

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 19.5 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 12.0378
Total reward = 11.4359
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 11.4359 (1)
a=3 : 0 (0)
a=4 : 19.5 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [1, 13]
Total reward: 15.468 [11.4359, 19.5]
Policy after 2 simulations
MCTS Policy:
a=4 : 19.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 11.4359 (1)
a=3 : 0 (0)
a=4 : 19.5 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=2 o=0 a=0 o=0 a=4 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 4.5 [2, 7]
Total reward: 8.00419 [6.98337, 9.025]
Policy after 2 simulations
MCTS Policy:
a=5 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : -1e+10 (1000000)
Check 1

# # # # 
# 0X* #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=1 o=0 a=2 o=0 a=0 o=0 a=4 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 6.6342 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 8 [8, 8]
Total reward: 8.3171 [6.6342, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 6.6342 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 16.3116, average = 8.9271
Undiscounted return = 20, average = 11.25
Starting run 9 with 2 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 16.3116
Total reward = 15.496
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 15.496 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 13.6176
Total reward = 2.9367
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 15.496 (1)
a=3 : -1e+10 (1000000)
a=4 : 2.9367 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7.5 [6, 9]
Total reward: 9.21634 [2.9367, 15.496]
Policy after 2 simulations
MCTS Policy:
a=2 : 15.496 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 15.496 (1)
a=3 : -1e+10 (1000000)
a=4 : 2.9367 (1)
a=5 : 0 (0)
a=6 : 0 (0)
S

# # # # 
# 0$1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 15.8025
Total reward = 15.0124
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 15.0124 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7 [4, 10]
Total reward: 11.5787 [8.14506, 15.0124]
Policy after 2 simulations
MCTS Policy:
a=6 : 15.0124 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 15.0124 (1)
Check 2

# # # # 
# 0$1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=2 o=0 a=6 o=1 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 16.7188
Total reward = 15.8829
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 15.8829 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward -4.0964
Total reward = -3.89158
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 15.8829 (1)
a=6 : -3.89158 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9 [5, 13]
Total reward: 5.99565 [-3.89158, 15.8829]
Policy after 2 simulations
MCTS Policy:
a=5 : 15.8829 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 15.8829 (1)
a=6 : -3.89158 (1)
Check 1

# # # # 
# 0$1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 20 steps, with total reward 17.4269
Total reward = 16.5556
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 16.5556 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 16.5556 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 12 [4, 20]
Total reward: 12.3503 [8.14506, 16.5556]
Policy after 2 simulations
MCTS Policy:
a=5 : 16.5556 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 16.5556 (1)
a=6 : 0 (0)
Check 1

# # # # 
# 0$1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 8.2062
Total reward = 7.79589
MCTS Values:
a=0 : 7.79589 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward 8.71828
Total reward = 8.28237
MCTS Values:
a=0 : 7.79589 (1)
a=1 : 8.28237 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 14.5 [8, 21]
Total reward: 8.03913 [7.79589, 8.28237]
Policy after 2 simulations
MCTS Policy:
a=1 : 8.28237 (1)
Values after 2 simulations
MCTS Values:
a=0 : 7.79589 (1)
a=1 : 8.28237 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0$1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -2.15769
Total reward = -2.04981
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -2.04981 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -2.04981 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8 [5, 11]
Total reward: 2.844 [-2.04981, 7.73781]
Policy after 2 simulations
MCTS Policy:
a=6 : 7.73781 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -2.04981 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.73781 (1)
Check 2

# # # # 
# 0$1$#
# . * #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 9.5 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -0.879938
Total reward = -0.835941
MCTS Values:
a=0 : 9.5 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -0.835941 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [1, 5]
Total reward: 4.33203 [-0.835941, 9.5]
Policy after 2 simulations
MCTS Policy:
a=0 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : 9.5 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -0.835941 (1)
a=6 : 0 (0)
N

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 a=0 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = -1.42625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : -1.42625 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4.5 [3, 6]
Total reward: 2.96233 [-1.42625, 7.35092]
Policy after 2 simulations
MCTS Policy:
a=3 : 7.35092 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : -1.42625 (1)
a=5 : 0 (0)
a=6 : 0 (0)
W

# # # # 
# * 1$#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 a=0 o=0 a=3 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -1.04842
Total reward = -0.996004
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.996004 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 9 [8, 10]
Total reward: 2.8191 [-0.996004, 6.6342]
Policy after 2 simulations
MCTS Policy:
a=2 : 6.6342 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.996004 (1)
a=6 : 0 (0)
S

# # # # 
# 0$1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 a=0 o=0 a=3 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 39 steps, with total reward 8.77488
Total reward = 8.33613
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.33613 (1)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 21 [3, 39]
Total reward: 8.45494 [8.33613, 8.57375]
Policy after 2 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.33613 (1)
a=6 : 8.57375 (1)
Check 2

# # # # 
# 0$1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 a=0 o=0 a=3 o=0 a=2 o=0 a=6 o=1 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward -1.42625
Total reward = -1.35494
MCTS Values:
a=0 : -1.35494 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4.5 [4, 5]
Total reward: 3.19144 [-1.35494, 7.73781]
Policy after 2 simulations
MCTS Policy:
a=5 : 7.73781 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1.35494 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 0 (0)
Check 1

# # # # 
# 0$1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 a=0 o=0 a=3 o=0 a=2 o=0 a=6 o=1 a=5 o=1 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 11.7061
Total reward = 11.1208
MCTS Values:
a=0 : 0 (0)
a=1 : 11.1208 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 11.1208 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 8.5 [4, 13]
Total reward: 9.63293 [8.14506, 11.1208]
Policy after 2 simulations
MCTS Policy:
a=1 : 11.1208 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 11.1208 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 0 (0)
E

# # # # 
# 0$1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 a=0 o=0 a=3 o=0 a=2 o=0 a=6 o=1 a=5 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 6.98337 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 6.98337 (1)
a=6 : 6.30249 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8 [7, 9]
Total reward: 6.64293 [6.30249, 6.98337]
Policy after 2 simulations
MCTS Policy:
a=5 : 6.98337 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 6.98337 (1)
a=6 : 6.30249 (1)
Check 1

# # # # 
# 0$1$#
# . * #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 a=0 o=0 a=3 o=0 a=2 o=0 a=6 o=1 a=5 o=1 a=1 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 38 steps, with total reward 10.0727
Total reward = 9.56902
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 9.56902 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 19.5 [1, 38]
Total reward: 9.53451 [9.5, 9.56902]
Policy after 2 simulations
MCTS Policy:
a=3 : 9.56902 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 9.56902 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 9.5 (1)
W

# # # # 
# 0$1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 a=0 o=0 a=3 o=0 a=2 o=0 a=6 o=1 a=5 o=1 a=1 o=0 a=5 o=1 a=3 o=0 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward -0.975
Total reward = -0.92625
MCTS Values:
a=0 : -0.92625 (1)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3.5 [3, 4]
Total reward: 3.60941 [-0.92625, 8.14506]
Policy after 2 simulations
MCTS Policy:
a=1 : 8.14506 (1)
Values after 2 simulations
MCTS Values:
a=0 : -0.92625 (1)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0$1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 a=0 o=0 a=3 o=0 a=2 o=0 a=6 o=1 a=5 o=1 a=1 o=0 a=5 o=1 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 4.1812
Total reward = 3.97214
MCTS Values:
a=0 : 3.97214 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 18 [18, 18]
Total reward: 6.98607 [3.97214, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 3.97214 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0$1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 4.63291, average = 8.44997
Undiscounted return = 10, average = 11.1111
Starting run 10 with 2 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.14506 (1)
a=6 : 9.025 (1)
Tree depth: 0 [0, 0]
Rollout depth: 3 [2, 4]
Total reward: 8.58503 [8.14506, 9.025]
Policy after 2 simulations
MCTS Policy:
a=6 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.14506 (1)
a=6 : 9.025 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 13.5738 [8.57375, 18.5738]
Policy after 2 simulations
MCTS Policy:
a=4 : 18.5738 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# * 1$#
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 2 simulations
MCTS Policy:
a=1 : 8.57375 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
S

# # # # 
# . 1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward -0.299368
Total reward = -0.2844
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.2844 (1)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 29 steps, with total reward -6.64673
Total reward = -6.31439
MCTS Values:
a=0 : -6.31439 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.2844 (1)
Tree depth: 0 [0, 0]
Rollout depth: 20.5 [12, 29]
Total reward: -3.2994 [-6.31439, -0.2844]
Policy after 2 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : -6.31439 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.2844 (1)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 8.67546 [7.35092, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 17.6451, average = 9.36948
Undiscounted return = 20, average = 12
Simulations = 2
Runs = 10
Undiscounted return = 12 +- 1.89737
Discounted return = 9.36948 +- 1.75338
Time = 0.0034395
Starting run 1 with 4 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = -2.64908
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 19.025
Total reward = 18.0737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
a=6 : 18.0737 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 16.4834
Total reward = 15.6592
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 15.6592 (1)
a=6 : 18.0737 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5.75 [3, 8]
Total reward: 9.6087 [-2.64908, 18.0737]
Policy after 4 simulations
MCTS Policy:
a=6 : 18.0737 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 15.6592 (1)
a=6 : 18.0737 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 12 attempts
Expanding node: a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward -0.946203
Total reward = -0.898893
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.898893 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 20 steps, with total reward 8.16562
Total reward = 7.75734
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.898893 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.75734 (1)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = -4.01263
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.898893 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : 7.75734 (1)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11.25 [3, 20]
Total reward: 2.85489 [-4.01263, 8.57375]
Policy after 4 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.898893 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : 7.75734 (1)
a=6 : 8.57375 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=6 o=1 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = 4.1812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.1812 (1)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 19 steps, with total reward 17.6533
Total reward = 16.7707
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.1812 (1)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.7707 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 13.2859
Total reward = 12.6216
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.1812 (1)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.7707 (1)
a=6 : 12.6216 (1)
Tree depth: 0 [0, 0]
Rollout depth: 13.75 [9, 19]
Total reward: 9.96899 [4.1812, 16.7707]
Policy after 4 simulations
MCTS Policy:
a=5 : 16.7707 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.1812 (1)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.7707 (1)
a=6 : 12.6216 (1)
Check 1

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=6 o=1 a=5 o=1 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 16.0084
Total reward = 15.208
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 15.208 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 15.208 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 16.9834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 0 (0)
a=6 : 15.208 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 23 steps, with total reward 2.38139
Total reward = 2.26232
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 2.26232 (1)
a=6 : 15.208 (1)
Tree depth: 0 [0, 0]
Rollout depth: 10 [2, 23]
Total reward: 10.8697 [2.26232, 16.9834]
Policy after 4 simulations
MCTS Policy:
a=4 : 16.9834 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 2.26232 (1)
a=6 : 15.208 (1)
Sample

# # # # 
# * 1$#
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=6 o=1 a=5 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 12.1391
Total reward = 11.5321
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 11.5321 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=6 o=1 a=6 o=1 a=5 o=1 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 9.83859 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 7.5 [1, 17]
Total reward: 8.86992 [6.30249, 11.5321]
Policy after 4 simulations
MCTS Policy:
a=2 : 9.83859 (2)
a=2 o=0 a=6 : 8.57375 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 9.83859 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
S

# # # # 
# . 1$#
# * . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=6 o=1 a=5 o=1 a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 4.1812
Total reward = 3.97214
MCTS Values:
a=0 : 0 (0)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=6 o=1 a=6 o=1 a=5 o=1 a=4 o=0 a=2 o=0 a=0 o=0 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 12.975
Total reward = 11.71
MCTS Values:
a=0 : 10.1419 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 12.3263 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 0 (0)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 12 [3, 18]
Total reward: 7.5608 [3.97214, 11.71]
Policy after 4 simulations
MCTS Policy:
a=0 : 10.1419 (2)
a=0 o=0 a=2 : 12.3263 (1)
Values after 4 simulations
MCTS Values:
a=0 : 10.1419 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 12.3263 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 0 (0)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
N

# # # # 
# * 1$#
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=6 o=1 a=5 o=1 a=4 o=0 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 14.1584
Total reward = 13.4505
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 13.4505 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 13.4505 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=6 o=1 a=6 o=1 a=5 o=1 a=4 o=0 a=2 o=0 a=0 o=0 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.29196 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 5.4036 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 7.75 [1, 14]
Total reward: 9.05725 [5.13342, 13.4505]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.29196 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 5.4036 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=6 o=1 a=5 o=1 a=4 o=0 a=2 o=0 a=0 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 8.14506 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 8.14506 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 8.14506 (1)
a=4 : 19.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Tree depth: 0 [0, 0]
Rollout depth: 2 [1, 4]
Total reward: 11.5425 [8.14506, 19.5]
Policy after 4 simulations
MCTS Policy:
a=4 : 19.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 8.14506 (1)
a=4 : 19.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=6 o=1 a=5 o=1 a=4 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 4.87675 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 5.98737 (1)
a=3 : 4.87675 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 5.98737 (1)
a=3 : 4.87675 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 12 [10, 14]
Total reward: 7.71603 [4.87675, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 5.98737 (1)
a=3 : 4.87675 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 22.1913, average = 22.1913
Undiscounted return = 30, average = 30
Starting run 2 with 4 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 16.0084
Total reward = 15.208
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 15.208 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 12.1391
Total reward = 11.5321
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 11.5321 (1)
a=6 : 15.208 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 4.1812
Total reward = 3.97214
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 11.5321 (1)
a=6 : 15.208 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = -3.69751
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.69751 (1)
a=5 : 11.5321 (1)
a=6 : 15.208 (1)
Tree depth: 0 [0, 0]
Rollout depth: 13 [8, 18]
Total reward: 6.75368 [-3.69751, 15.208]
Policy after 4 simulations
MCTS Policy:
a=6 : 15.208 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.69751 (1)
a=5 : 11.5321 (1)
a=6 : 15.208 (1)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward -0.835941
Total reward = 9.20586
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 9.20586 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 9.20586 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 9.20586 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 9.20586 (1)
a=5 : 6.6342 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5.5 [1, 8]
Total reward: 8.08086 [6.6342, 9.5]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 9.20586 (1)
a=5 : 6.6342 (1)
a=6 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 10.3209
Total reward = 9.80487
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 9.80487 (1)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7 [2, 16]
Total reward: 9.3509 [8.57375, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 9.80487 (1)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 8.57375 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.025, average = 15.6082
Undiscounted return = 10, average = 20
Starting run 3 with 4 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 19 steps, with total reward 3.97214
Total reward = 3.77354
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 3.77354 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 3.77354 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 23.7252
Total reward = 22.5389
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 22.5389 (1)
a=6 : 3.77354 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 13.6812
Total reward = 12.9971
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.9971 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 22.5389 (1)
a=6 : 3.77354 (1)
Tree depth: 0 [0, 0]
Rollout depth: 13 [4, 19]
Total reward: 11.8637 [3.77354, 22.5389]
Policy after 4 simulations
MCTS Policy:
a=5 : 22.5389 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.9971 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 22.5389 (1)
a=6 : 3.77354 (1)
Check 1

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 5.4036 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 5.4036 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 12.7545
Total reward = 12.1168
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 12.1168 (1)
a=6 : 5.4036 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 7.80306
Total reward = 7.41291
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 7.41291 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 12.1168 (1)
a=6 : 5.4036 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11 [3, 16]
Total reward: 8.37676 [5.4036, 12.1168]
Policy after 4 simulations
MCTS Policy:
a=5 : 12.1168 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 7.41291 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 12.1168 (1)
a=6 : 5.4036 (1)
Check 1

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=5 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 18.1451
Total reward = 17.2378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 17.2378 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 15.6592
Total reward = 14.8762
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.8762 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 17.2378 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 16.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.8762 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : 0 (0)
a=6 : 17.2378 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.8762 (1)
a=2 : 7.35092 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : 0 (0)
a=6 : 17.2378 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7 [5, 9]
Total reward: 14.0248 [7.35092, 17.2378]
Policy after 4 simulations
MCTS Policy:
a=6 : 17.2378 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.8762 (1)
a=2 : 7.35092 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : 0 (0)
a=6 : 17.2378 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 a=5 o=1 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 7.59875
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 7.59875 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 8.33093
Total reward = 7.91438
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 7.59875 (1)
a=5 : 0 (0)
a=6 : 7.91438 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 33 steps, with total reward 1.93711
Total reward = 1.84026
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 1.84026 (1)
a=3 : -1e+10 (1000000)
a=4 : 7.59875 (1)
a=5 : 0 (0)
a=6 : 7.91438 (1)
Tree depth: 0 [0, 0]
Rollout depth: 12.75 [1, 33]
Total reward: 6.71335 [1.84026, 9.5]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 1.84026 (1)
a=3 : -1e+10 (1000000)
a=4 : 7.59875 (1)
a=5 : 0 (0)
a=6 : 7.91438 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=5 o=1 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 25 steps, with total reward -5.65386
Total reward = -5.37117
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -5.37117 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = -0.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -5.37117 (1)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -5.37117 (1)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9.66667 [1, 25]
Total reward: 3.17565 [-5.37117, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -5.37117 (1)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 0 (0)
a=6 : 8.57375 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 8.14506, average = 13.1205
Undiscounted return = 10, average = 16.6667
Starting run 4 with 4 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -1.75044
Total reward = -1.66292
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.66292 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward -0.475
Total reward = -0.45125
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.66292 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.45125 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.66292 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.45125 (1)
a=6 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6.5 [3, 11]
Total reward: 3.15175 [-1.66292, 7.73781]
Policy after 4 simulations
MCTS Policy:
a=6 : 7.73781 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.66292 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.45125 (1)
a=6 : 7.73781 (1)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 4.87675 (1)
a=6 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7 [3, 14]
Total reward: 9.63481 [4.87675, 18.5738]
Policy after 4 simulations
MCTS Policy:
a=4 : 18.5738 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 4.87675 (1)
a=6 : 7.73781 (1)
Sample

# # # # 
# * 1X#
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.688 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 16.7628
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.688 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 16.7628 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 14.7212
Total reward = 13.9851
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.9851 (1)
a=2 : 5.688 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 16.7628 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 16.7628
Total reward = 15.1284
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.9851 (1)
a=2 : 5.688 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 15.9456 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 15.9247 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 7.5 [5, 11]
Total reward: 12.8911 [5.688, 16.7628]
Policy after 4 simulations
MCTS Policy:
a=6 : 15.9456 (2)
a=6 o=1 a=2 : 15.9247 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.9851 (1)
a=2 : 5.688 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 15.9456 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 15.9247 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=6 o=1 a=4 o=0 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward -0.5
Total reward = -0.475
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.475 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.475 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.475 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=2 a=2 o=0 
Adding sample:

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.475 (1)
a=2 : 8.15578 (2)
a=2 o=0 a=0 : 9.025 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 3.5 [2, 5]
Total reward: 5.89359 [-0.475, 8.57375]
Policy after 4 simulations
MCTS Policy:
a=2 : 8.15578 (2)
a=2 o=0 a=0 : 9.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.475 (1)
a=2 : 8.15578 (2)
a=2 o=0 a=0 : 9.025 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
S

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=6 o=2 a=2 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : 0 (0)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -0.349169
Total reward = -0.33171
MCTS Values:
a=0 : 0 (0)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.33171 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 9.025 (1)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.33171 (1)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=2 a=2 o=0 a=0 o=0 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : 8.79938 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 9.025 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 0 (0)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.33171 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 5 [2, 9]
Total reward: 6.0626 [-0.33171, 9.025]
Policy after 4 simulations
MCTS Policy:
a=0 : 8.79938 (2)
a=0 o=0 a=2 : 9.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : 8.79938 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 9.025 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 0 (0)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.33171 (1)
N

# # # # 
# * 1X#
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=6 o=2 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 17.5987
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 21 steps, with total reward -0.387284
Total reward = -0.36792
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : -0.36792 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.40127
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : -0.36792 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 4.63291 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 12.25 [3, 21]
Total reward: 6.90487 [-0.36792, 17.5987]
Policy after 4 simulations
MCTS Policy:
a=6 : 11 (2)
a=6 o=2 a=2 : 4.63291 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : -0.36792 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 4.63291 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
Matched 1 states
Created 1 local transformations out of 6 attempts
Expanding node: a=6 o=1 a=4 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 4.33333 [1, 8]
Total reward: 8.44482 [6.6342, 9.5]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.5 (2)
a=1 o=0 a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 7.73781 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 7.73781 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 7.73781 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4.66667 [1, 8]
Total reward: 8.468 [6.6342, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 7.73781 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 16.4834, average = 13.9612
Undiscounted return = 20, average = 17.5
Starting run 5 with 4 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward -2.04163
Total reward = -1.93955
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.93955 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -0.680879
Total reward = -0.646835
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.93955 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -0.646835 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.93955 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.14506 (1)
a=6 : -0.646835 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -0.349169
Total reward = 9.66829
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.93955 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 9.66829 (1)
a=5 : 8.14506 (1)
a=6 : -0.646835 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7.75 [4, 10]
Total reward: 3.80674 [-1.93955, 9.66829]
Policy after 4 simulations
MCTS Policy:
a=4 : 9.66829 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.93955 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 9.66829 (1)
a=5 : 8.14506 (1)
a=6 : -0.646835 (1)
Sample

# # # # 
# * 1X#
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.63291
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 14.8762
Total reward = 14.1324
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 14.1324 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 12.2899
Total reward = 11.0916
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 12.612 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 11.6754 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 10 [4, 15]
Total reward: 9.5005 [4.63291, 14.1324]
Policy after 4 simulations
MCTS Policy:
a=6 : 12.612 (2)
a=6 o=1 a=1 : 11.6754 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 12.612 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 11.6754 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 5 attempts
Expanding node: a=4 o=0 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -1.10361
Total reward = -1.04842
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.04842 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 17 steps, with total reward -2.23294
Total reward = -2.12129
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.12129 (1)
a=2 : -1.04842 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.63291
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.12129 (1)
a=2 : -1.04842 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 4.63291 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 17 steps, with total reward -1.00233
Total reward = -0.904607
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.12129 (1)
a=2 : -1.04842 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 1.86415 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -0.952217 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 14.5 [9, 17]
Total reward: 0.139648 [-2.12129, 4.63291]
Policy after 4 simulations
MCTS Policy:
a=6 : 1.86415 (2)
a=6 o=2 a=1 : 0 (0)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.12129 (1)
a=2 : -1.04842 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 1.86415 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -0.952217 (1)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
Matched 1 states
Created 1 local transformations out of 6 attempts
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 27 steps, with total reward -3.999
Total reward = -3.79905
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -3.79905 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -3.79905 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -3.79905 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = -1.35494
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.60941 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1.42625 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -3.79905 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 10 [3, 27]
Total reward: 2.60078 [-3.79905, 8.57375]
Policy after 4 simulations
MCTS Policy:
a=2 : 6.98337 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.60941 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1.42625 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -3.79905 (1)
S

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 21 steps, with total reward 12.6099
Total reward = 11.9794
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 11.9794 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 11.9794 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 a=2 o=0 a=1 o=0 
Adding sample:

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 10.2766 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 9 [2, 21]
Total reward: 8.77856 [5.98737, 11.9794]
Policy after 4 simulations
MCTS Policy:
a=1 : 10.2766 (2)
a=1 o=0 a=6 : 9.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 10.2766 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
E

# # # # 
# . 1X#
# . * #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 24 steps, with total reward 3.07357
Total reward = 2.91989
MCTS Values:
a=0 : 2.91989 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : 2.91989 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 2.91989 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 2.91989 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 10.6667 [1, 24]
Total reward: 7.35082 [2.91989, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : 2.91989 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
E

# # # # 
# . 1X#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 17.7378, average = 14.7165
Undiscounted return = 20, average = 18
Starting run 6 with 4 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward -0.92625
Total reward = -0.879938
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.879938 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 16.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : -0.879938 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 18.1451
Total reward = 17.2378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.2378 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : -0.879938 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.2378 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : -0.879938 (1)
a=6 : 5.4036 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7.25 [4, 12]
Total reward: 9.59892 [-0.879938, 17.2378]
Policy after 4 simulations
MCTS Policy:
a=1 : 17.2378 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.2378 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : -0.879938 (1)
a=6 : 5.4036 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : 7.35092 (1)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward -0.5
Total reward = -0.475
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : 7.35092 (1)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : -0.475 (1)
Tree depth: 0 [0, 0]
Rollout depth: 3 [1, 6]
Total reward: 6.23742 [-0.475, 9.5]
Policy after 4 simulations
MCTS Policy:
a=5 : 9.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : 7.35092 (1)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : -0.475 (1)
Check 1

# # # # 
# 0$* #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 5 attempts
Expanding node: a=1 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 15.5571
Total reward = 14.7793
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 14.7793 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 14.7793 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 20 steps, with total reward 3.77354
Total reward = 3.58486
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 3.58486 (1)
a=3 : 14.7793 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 3.58486 (1)
a=3 : 14.7793 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9.66667 [1, 20]
Total reward: 9.46603 [3.58486, 14.7793]
Policy after 4 simulations
MCTS Policy:
a=3 : 14.7793 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 3.58486 (1)
a=3 : 14.7793 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 9.5 (1)
W

# # # # 
# * 1X#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=3 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 6.48836
Total reward = 6.16394
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.16394 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.16394 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 9.16829
Total reward = 8.70988
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.16394 (1)
a=6 : 8.70988 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 26 steps, with total reward 6.35875
Total reward = 6.04082
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 6.04082 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.16394 (1)
a=6 : 8.70988 (1)
Tree depth: 0 [0, 0]
Rollout depth: 13 [2, 26]
Total reward: 7.48491 [6.04082, 9.025]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 6.04082 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.16394 (1)
a=6 : 8.70988 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 16.6342
Total reward = 15.8025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 15.8025 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 13.5487
Total reward = 12.8712
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 15.8025 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 12.8712 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 15.8025 (1)
a=4 : 0 (0)
a=5 : 8.57375 (1)
a=6 : 12.8712 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 15.8025 (1)
a=4 : 0 (0)
a=5 : 8.57375 (1)
a=6 : 12.8712 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8.33333 [3, 13]
Total reward: 11.8119 [8.57375, 15.8025]
Policy after 4 simulations
MCTS Policy:
a=3 : 15.8025 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 15.8025 (1)
a=4 : 0 (0)
a=5 : 8.57375 (1)
a=6 : 12.8712 (1)
W

# # # # 
# * 1X#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=3 o=0 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 15.9247
Total reward = 15.1284
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 15.1284 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 15.1284 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 15.1284 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 13.4056
Total reward = 12.7353
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 15.1284 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 12.7353 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8.75 [3, 22]
Total reward: 13.7528 [8.57375, 18.5738]
Policy after 4 simulations
MCTS Policy:
a=4 : 18.5738 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 15.1284 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 12.7353 (1)
Sample

# # # # 
# * 1X#
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=3 o=0 a=1 o=0 a=3 o=0 a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.13342 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.13342 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=1 o=0 a=5 o=1 a=3 o=0 a=1 o=0 a=3 o=0 a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 11.7522
Total reward = 10.6063
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.45442 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 11.1646 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.13342 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 13 [9, 17]
Total reward: 6.79392 [5.13342, 10.6063]
Policy after 4 simulations
MCTS Policy:
a=1 : 8.45442 (2)
a=1 o=0 a=2 : 11.1646 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.45442 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 11.1646 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.13342 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=3 o=0 a=1 o=0 a=3 o=0 a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 8.57375 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 8.57375 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 8.57375 (1)
a=4 : 19.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1.66667 [1, 3]
Total reward: 11.8934 [8.57375, 19.5]
Policy after 4 simulations
MCTS Policy:
a=4 : 19.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 8.57375 (1)
a=4 : 19.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=3 o=0 a=1 o=0 a=3 o=0 a=4 o=0 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 3.5 [1, 6]
Total reward: 9.21273 [7.35092, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 7.38864, average = 13.4952
Undiscounted return = 10, average = 16.6667
Starting run 7 with 4 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.4036 (1)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.4036 (1)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.4036 (1)
a=6 : 8.14506 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6.5 [3, 12]
Total reward: 7.27645 [5.4036, 8.57375]
Policy after 4 simulations
MCTS Policy:
a=1 : 8.57375 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.4036 (1)
a=6 : 8.14506 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward 3.58486
Total reward = 13.4056
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 13.4056 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 21 steps, with total reward 9.88735
Total reward = 9.39299
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.39299 (1)
a=3 : 0 (0)
a=4 : 13.4056 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -2.15769
Total reward = -2.04981
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.39299 (1)
a=3 : -2.04981 (1)
a=4 : 13.4056 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.39299 (1)
a=3 : -2.04981 (1)
a=4 : 13.4056 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 17.6667 [11, 21]
Total reward: 7.6872 [-2.04981, 13.4056]
Policy after 4 simulations
MCTS Policy:
a=4 : 13.4056 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.39299 (1)
a=3 : -2.04981 (1)
a=4 : 13.4056 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# 0$* #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -3.3658
Total reward = -3.19751
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : -3.19751 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 4 [1, 9]
Total reward: 6.33187 [-3.19751, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : -3.19751 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = -0.475, average = 11.4995
Undiscounted return = 0, average = 14.2857
Starting run 8 with 4 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 20 steps, with total reward 9.76091
Total reward = 9.27286
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.27286 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 9.27286 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 18.1451
Total reward = 17.2378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 9.27286 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 17.2378 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward -0.475
Total reward = -10.4512
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 9.27286 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.4512 (1)
a=5 : 0 (0)
a=6 : 17.2378 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8.75 [3, 20]
Total reward: 5.7607 [-10.4512, 17.2378]
Policy after 4 simulations
MCTS Policy:
a=6 : 17.2378 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 9.27286 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.4512 (1)
a=5 : 0 (0)
a=6 : 17.2378 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 25 steps, with total reward 7.79664
Total reward = -2.59319
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.59319 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.59319 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 23 steps, with total reward -6.76466
Total reward = -6.42643
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.59319 (1)
a=5 : 0 (0)
a=6 : -6.42643 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 3.40562
Total reward = 3.23534
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.59319 (1)
a=5 : 3.23534 (1)
a=6 : -6.42643 (1)
Tree depth: 0 [0, 0]
Rollout depth: 17.75 [1, 25]
Total reward: 0.928928 [-6.42643, 9.5]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.59319 (1)
a=5 : 3.23534 (1)
a=6 : -6.42643 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 20.1059
Total reward = 19.1006
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 19.1006 (1)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5.5 [1, 18]
Total reward: 14.2814 [9.5, 19.1006]
Policy after 4 simulations
MCTS Policy:
a=3 : 19.1006 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 19.1006 (1)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 9.5 (1)
W

# # # # 
# * 1$#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 16.8509
Total reward = 6.00837
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.00837 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.00837 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward -3.01663
Total reward = -2.8658
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.00837 (1)
a=5 : -2.8658 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward -1.94732
Total reward = -1.84995
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.00837 (1)
a=5 : -2.8658 (1)
a=6 : -1.84995 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8 [4, 13]
Total reward: 2.35942 [-2.8658, 8.14506]
Policy after 4 simulations
MCTS Policy:
a=1 : 8.14506 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.00837 (1)
a=5 : -2.8658 (1)
a=6 : -1.84995 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=1 o=0 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -2.14908
Total reward = -2.04163
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -2.04163 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = -0.975
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.975 (1)
a=5 : -2.04163 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -0.33171
Total reward = -0.315125
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.315125 (1)
a=3 : 0 (0)
a=4 : -0.975 (1)
a=5 : -2.04163 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -0.428688
Total reward = -0.407253
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.315125 (1)
a=3 : 0 (0)
a=4 : -0.975 (1)
a=5 : -2.04163 (1)
a=6 : -0.407253 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6 [2, 10]
Total reward: -0.934751 [-2.04163, -0.315125]
Policy after 4 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.315125 (1)
a=3 : 0 (0)
a=4 : -0.975 (1)
a=5 : -2.04163 (1)
a=6 : -0.407253 (1)
W

# # # # 
# * 1$#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 6.30249 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 17.5987
Total reward = 16.7188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 16.7188 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 6.30249 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 16.3759
Total reward = 5.55712
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 16.7188 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.55712 (1)
a=5 : 0 (0)
a=6 : 6.30249 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5.75 [3, 9]
Total reward: 9.28804 [5.55712, 16.7188]
Policy after 4 simulations
MCTS Policy:
a=2 : 16.7188 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 16.7188 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.55712 (1)
a=5 : 0 (0)
a=6 : 6.30249 (1)
S

# # # # 
# 0$1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 16.7628
MCTS Values:
a=0 : 16.7628 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 16.7628 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 6.6342 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4.75 [1, 8]
Total reward: 10.1587 [6.6342, 16.7628]
Policy after 4 simulations
MCTS Policy:
a=0 : 16.7628 (1)
Values after 4 simulations
MCTS Values:
a=0 : 16.7628 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 6.6342 (1)
N

# # # # 
# * 1$#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward -0.975
Total reward = -0.92625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.92625 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 24 steps, with total reward -6.42643
Total reward = -6.10511
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -6.10511 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.92625 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = -1.42625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -6.10511 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : -0.92625 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -6.10511 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : -0.92625 (1)
a=6 : 4.87675 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11 [3, 24]
Total reward: -0.895215 [-6.10511, 4.87675]
Policy after 4 simulations
MCTS Policy:
a=6 : 4.87675 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -6.10511 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : -0.92625 (1)
a=6 : 4.87675 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 28 steps, with total reward 9.85436
Total reward = 9.36164
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.36164 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 15.8829
Total reward = 15.0887
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.36164 (1)
a=2 : 15.0887 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 24.2212
Total reward = 23.0101
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.36164 (1)
a=2 : 15.0887 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 23.0101 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 16.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.36164 (1)
a=2 : 15.0887 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : 0 (0)
a=6 : 23.0101 (1)
Tree depth: 0 [0, 0]
Rollout depth: 12.5 [6, 28]
Total reward: 16.0237 [9.36164, 23.0101]
Policy after 4 simulations
MCTS Policy:
a=6 : 23.0101 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.36164 (1)
a=2 : 15.0887 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : 0 (0)
a=6 : 23.0101 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=6 o=1 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=6 o=1 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 22 steps, with total reward 3.40562
Total reward = 3.23534
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.23534 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 17.5987
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.23534 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 17.5987 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 19.5
Total reward = 18.525
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 18.525 (1)
a=2 : 3.23534 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 17.5987 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = -1.85494
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 18.525 (1)
a=2 : 3.23534 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.85494 (1)
a=5 : 17.5987 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7.75 [2, 22]
Total reward: 9.37604 [-1.85494, 18.525]
Policy after 4 simulations
MCTS Policy:
a=1 : 18.525 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 18.525 (1)
a=2 : 3.23534 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.85494 (1)
a=5 : 17.5987 (1)
a=6 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=6 o=1 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 19.025
Total reward = 18.0737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 18.0737 (1)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 24 steps, with total reward 16.3089
Total reward = 15.4935
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 15.4935 (1)
a=4 : 0 (0)
a=5 : 18.0737 (1)
a=6 : 9.025 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9.66667 [2, 24]
Total reward: 13.1481 [9.025, 18.0737]
Policy after 4 simulations
MCTS Policy:
a=5 : 18.0737 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 15.4935 (1)
a=4 : 0 (0)
a=5 : 18.0737 (1)
a=6 : 9.025 (1)
Check 1

# # # # 
# 0$* #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=6 o=1 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 8.57375 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 8.57375 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = -0.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 8.57375 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 8.57375 (1)
a=6 : 5.13342 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5.66667 [1, 13]
Total reward: 5.80179 [-0.5, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 8.57375 (1)
a=6 : 5.13342 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 5.688, average = 10.773
Undiscounted return = 10, average = 13.75
Starting run 9 with 4 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 25 steps, with total reward 9.90326
Total reward = 9.4081
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.4081 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 9.4081 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 16 steps, with total reward -2.71801
Total reward = -2.58211
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 9.4081 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -2.58211 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward -0.256671
Total reward = -0.243837
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 9.4081 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.243837 (1)
a=6 : -2.58211 (1)
Tree depth: 0 [0, 0]
Rollout depth: 16.75 [11, 25]
Total reward: 3.06754 [-2.58211, 9.4081]
Policy after 4 simulations
MCTS Policy:
a=2 : 9.4081 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 9.4081 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.243837 (1)
a=6 : -2.58211 (1)
S

# # # # 
# 0X1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.35092 (1)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : 0 (0)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.35092 (1)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 18.5737
Total reward = 17.6451
MCTS Values:
a=0 : 17.6451 (1)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.35092 (1)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5 [3, 7]
Total reward: 10.1383 [6.98337, 17.6451]
Policy after 4 simulations
MCTS Policy:
a=0 : 17.6451 (1)
Values after 4 simulations
MCTS Values:
a=0 : 17.6451 (1)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.35092 (1)
a=6 : 8.57375 (1)
N

# # # # 
# * 1$#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = -2.26219
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 27 steps, with total reward -6.47036
Total reward = -6.14684
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -6.14684 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -6.14684 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 21 steps, with total reward -2.10314
Total reward = -1.99798
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -6.14684 (1)
a=2 : -1.99798 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 14.25 [4, 27]
Total reward: -0.565489 [-6.14684, 8.14506]
Policy after 4 simulations
MCTS Policy:
a=5 : 8.14506 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -6.14684 (1)
a=2 : -1.99798 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Check 1

# # # # 
# * 1$#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=0 o=0 a=5 o=2 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 16.7628
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.7628 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.7628 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward -1.93955
Total reward = -1.84257
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : -1.84257 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.7628 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = -1.85494
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : -1.84257 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.85494 (1)
a=5 : 16.7628 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4.75 [1, 9]
Total reward: 5.64133 [-1.85494, 16.7628]
Policy after 4 simulations
MCTS Policy:
a=5 : 16.7628 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : -1.84257 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.85494 (1)
a=5 : 16.7628 (1)
a=6 : 0 (0)
Check 1

# # # # 
# * 1$#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=0 o=0 a=5 o=2 a=5 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.98337 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = -4.5964
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.5964 (1)
a=5 : 6.98337 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward -2.51663
Total reward = -2.3908
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.5964 (1)
a=5 : 6.98337 (1)
a=6 : -2.3908 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.5964 (1)
a=5 : 6.98337 (1)
a=6 : -2.3908 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8.75 [7, 12]
Total reward: 1.6576 [-4.5964, 6.98337]
Policy after 4 simulations
MCTS Policy:
a=5 : 6.98337 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.5964 (1)
a=5 : 6.98337 (1)
a=6 : -2.3908 (1)
Check 1

# # # # 
# * 1$#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=0 o=0 a=5 o=2 a=5 o=2 a=5 o=2 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 7.59875
Total reward = 7.21881
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.21881 (1)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -0.794144
Total reward = -0.754436
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.754436 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.21881 (1)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.63291
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.63291 (1)
a=2 : -0.754436 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.21881 (1)
a=6 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7.75 [4, 15]
Total reward: 4.70877 [-0.754436, 7.73781]
Policy after 4 simulations
MCTS Policy:
a=6 : 7.73781 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.63291 (1)
a=2 : -0.754436 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.21881 (1)
a=6 : 7.73781 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=2 o=0 a=0 o=0 a=5 o=2 a=5 o=2 a=5 o=2 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 5.98737 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 15.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 0 (0)
a=6 : 5.98737 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 16.9834
Total reward = 16.1342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 16.1342 (1)
a=6 : 5.98737 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 20.9607
Total reward = 19.9127
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 19.9127 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 16.1342 (1)
a=6 : 5.98737 (1)
Tree depth: 0 [0, 0]
Rollout depth: 10.75 [8, 13]
Total reward: 14.3595 [5.98737, 19.9127]
Policy after 4 simulations
MCTS Policy:
a=2 : 19.9127 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 19.9127 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 16.1342 (1)
a=6 : 5.98737 (1)
S

# # # # 
# 0X1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=0 o=0 a=5 o=2 a=5 o=2 a=5 o=2 a=6 o=1 a=2 o=0 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 4.89386
Total reward = 4.64916
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.64916 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.64916 (1)
a=6 : 7.35092 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 26 steps, with total reward 2.7739
Total reward = 2.6352
MCTS Values:
a=0 : 2.6352 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.64916 (1)
a=6 : 7.35092 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 2.6352 (1)
a=1 : 8.57375 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.64916 (1)
a=6 : 7.35092 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11.75 [3, 26]
Total reward: 5.80226 [2.6352, 8.57375]
Policy after 4 simulations
MCTS Policy:
a=1 : 8.57375 (1)
Values after 4 simulations
MCTS Values:
a=0 : 2.6352 (1)
a=1 : 8.57375 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.64916 (1)
a=6 : 7.35092 (1)
E

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=0 o=0 a=5 o=2 a=5 o=2 a=5 o=2 a=6 o=1 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -2.58638
Total reward = -2.45706
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -2.45706 (1)
a=6 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6.66667 [4, 11]
Total reward: 5.85645 [-2.45706, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -2.45706 (1)
a=6 : 7.73781 (1)
E

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 6.6342, average = 10.3132
Undiscounted return = 10, average = 13.3333
Starting run 10 with 4 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 4.1812
Total reward = 3.97214
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 26.7628
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : 26.7628 (1)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 8 [4, 18]
Total reward: 11.6545 [3.97214, 26.7628]
Policy after 4 simulations
MCTS Policy:
a=4 : 26.7628 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : 26.7628 (1)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Sample

# # # # 
# * 1X#
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -1.22283
Total reward = -1.16169
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.16169 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward -1.42625
Total reward = -1.35494
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.35494 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.16169 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -1.93955
Total reward = -1.84257
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.35494 (1)
a=2 : -1.84257 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.16169 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward -0.975
Total reward = -0.879937
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.35494 (1)
a=2 : -1.84257 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.02081 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : -0.92625 (1)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 5.75 [3, 9]
Total reward: -1.30978 [-1.84257, -0.879937]
Policy after 4 simulations
MCTS Policy:
a=6 : -1.02081 (2)
a=6 o=2 a=2 : 0 (0)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.35494 (1)
a=2 : -1.84257 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.02081 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : -0.92625 (1)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
Matched 1 states
Created 1 local transformations out of 3 attempts
Expanding node: a=4 o=0 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -2.14908
Total reward = -2.04163
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.04163 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.04163 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.18796 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 7.73781 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 4.5 [2, 7]
Total reward: 5.61984 [-2.04163, 9.025]
Policy after 4 simulations
MCTS Policy:
a=6 : 8.18796 (2)
a=6 o=2 a=2 : 7.73781 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.04163 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.18796 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 7.73781 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 30 steps, with total reward -2.87407
Total reward = -2.73036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.73036 (1)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.73036 (1)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.56422 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 7.35092 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 13.5 [4, 30]
Total reward: 4.31871 [-2.73036, 8.14506]
Policy after 4 simulations
MCTS Policy:
a=6 : 7.56422 (2)
a=6 o=2 a=2 : 7.35092 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.73036 (1)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.56422 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 7.35092 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
Matched 1 states
Created 1 local transformations out of 2 attempts
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.40127
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 a=6 o=2 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 24 steps, with total reward -0.898575
Total reward = -0.810963
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 3.26998 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -0.853646 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 14 [6, 24]
Total reward: 4.23215 [-0.810963, 7.35092]
Policy after 4 simulations
MCTS Policy:
a=1 : 5.98737 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 3.26998 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -0.853646 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 9.025 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 9.025 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = -0.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 9.025 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1.33333 [1, 2]
Total reward: 7.00625 [-0.5, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 9.025 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = -2.26219, average = 9.05562
Undiscounted return = 0, average = 12
Simulations = 4
Runs = 10
Undiscounted return = 12 +- 2.75681
Discounted return = 9.05562 +- 2.32989
Time = 0.0055706
Starting run 1 with 8 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 16.7628
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 16.7628 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward -0.475
Total reward = -0.45125
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 16.7628 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.45125 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = -4.01263
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 16.7628 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : -0.45125 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward -6.57267
Total reward = -6.24404
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 16.7628 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : -0.45125 (1)
a=6 : -6.24404 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward -0.975
Total reward = -0.92625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : 16.7628 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : -0.45125 (1)
a=6 : -6.24404 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : 12.8939 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : -0.45125 (1)
a=6 : -6.24404 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Adding sample:

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : 10.5917 (3)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 6.30249 (1)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : -0.45125 (1)
a=6 : -6.24404 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 23 steps, with total reward 3.23534
Total reward = 2.91989
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : 10.5917 (3)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 6.30249 (1)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : 1.23432 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 3.07357 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : -6.24404 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 8.5 [1, 23]
Total reward: 2.88261 [-6.24404, 16.7628]
Policy after 8 simulations
MCTS Policy:
a=2 : 10.5917 (3)
a=2 o=0 a=1 : 9.5 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : 10.5917 (3)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 6.30249 (1)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : 1.23432 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 3.07357 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : -6.24404 (1)
S

# # # # 
# 0X1X#
# * . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 0 (0)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 13.5487
Total reward = 12.8712
MCTS Values:
a=0 : 12.8712 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 12.8712 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 11.5321
Total reward = 10.9555
MCTS Values:
a=0 : 12.8712 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 10.9555 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=2 o=0 a=0 o=0 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 23 steps, with total reward 5.51772
Total reward = 4.97974
MCTS Values:
a=0 : 8.92549 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 5.24183 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 10.9555 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=1 
Adding sample:

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 22.4794
Total reward = 20.2876
MCTS Values:
a=0 : 8.92549 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 5.24183 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 15.6216 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 21.3554 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Expanding node: a=2 o=0 a=1 o=0 
Adding sample:

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -0.33171
Total reward = -0.299368
MCTS Values:
a=0 : 8.92549 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 5.24183 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 4.36282 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : -0.315125 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 15.6216 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 21.3554 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 
Adding sample:

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : 8.92549 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 5.24183 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 4.36282 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : -0.315125 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.35941 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 9.025 (1)
a=6 : 15.6216 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 21.3554 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 10 [2, 23]
Total reward: 9.31732 [-0.299368, 20.2876]
Policy after 8 simulations
MCTS Policy:
a=6 : 15.6216 (2)
a=6 o=1 a=6 : 21.3554 (1)
Values after 8 simulations
MCTS Values:
a=0 : 8.92549 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 5.24183 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 4.36282 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : -0.315125 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.35941 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 9.025 (1)
a=6 : 15.6216 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 21.3554 (1)
Check 2

# # # # 
# 0X1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 7.09664
Total reward = 6.74181
MCTS Values:
a=0 : 6.74181 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 6.74181 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 6.74181 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 6.74181 (1)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=5 o=1 
Adding sample:

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 9.09275
Total reward = 8.2062
MCTS Values:
a=0 : 6.74181 (1)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.17563 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 8.63811 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=1 o=0 
Adding sample:

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward -0.475
Total reward = -0.428687
MCTS Values:
a=0 : 6.74181 (1)
a=1 : 3.85819 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : -0.45125 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.17563 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 8.63811 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 1.03547
Total reward = 0.934513
MCTS Values:
a=0 : 3.83816 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0.983697 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 3.85819 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : -0.45125 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.17563 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 8.63811 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 
Adding sample:

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 16.8509
Total reward = 15.208
MCTS Values:
a=0 : 3.83816 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0.983697 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 3.85819 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : -0.45125 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.17563 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 8.63811 (1)
a=6 : 10.9211 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : 16.0084 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 7.375 [3, 17]
Total reward: 6.69826 [-0.428687, 15.208]
Policy after 8 simulations
MCTS Policy:
a=6 : 10.9211 (2)
a=6 o=2 a=6 : 16.0084 (1)
Values after 8 simulations
MCTS Values:
a=0 : 3.83816 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0.983697 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 3.85819 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : -0.45125 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.17563 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 8.63811 (1)
a=6 : 10.9211 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : 16.0084 (1)
Check 2

# # # # 
# 0X1X#
# * . #
# # # # 
Observed bad
Reward 0
Matched 1 states
Created 1 local transformations out of 2 attempts
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 22 steps, with total reward -10.2478
Total reward = -9.73541
MCTS Values:
a=0 : 0 (0)
a=1 : -9.73541 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward -4.39209
Total reward = -4.17248
MCTS Values:
a=0 : 0 (0)
a=1 : -9.73541 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -4.17248 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -2.64908
Total reward = -2.51663
MCTS Values:
a=0 : -2.51663 (1)
a=1 : -9.73541 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -4.17248 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -2.51663 (1)
a=1 : -9.73541 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 6.30249 (1)
a=6 : -4.17248 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=1 
Adding sample:

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 28 steps, with total reward 2.50344
Total reward = 2.25936
MCTS Values:
a=0 : -2.51663 (1)
a=1 : -9.73541 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.28092 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 2.37827 (1)
a=6 : -4.17248 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=0 o=0 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : 2.41715 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 7.73781 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : -9.73541 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.28092 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 2.37827 (1)
a=6 : -4.17248 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 
Adding sample:

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : 2.41715 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 7.73781 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : -9.73541 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 5.30426 (3)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 2.37827 (1)
a=5 o=2 a=0 : 0 (0)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : 7.73781 (1)
a=5 o=2 a=6 : 0 (0)
a=6 : -4.17248 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=6 o=2 
Adding sample:

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.4036
MCTS Values:
a=0 : 2.41715 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 7.73781 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : -9.73541 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 5.30426 (3)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 2.37827 (1)
a=5 o=2 a=0 : 0 (0)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : 7.73781 (1)
a=5 o=2 a=6 : 0 (0)
a=6 : 0.615559 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : 5.688 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 12.875 [5, 28]
Total reward: 1.53035 [-9.73541, 7.35092]
Policy after 8 simulations
MCTS Policy:
a=5 : 5.30426 (3)
a=5 o=1 a=6 : 2.37827 (1)
a=5 o=2 a=5 : 7.73781 (1)
Values after 8 simulations
MCTS Values:
a=0 : 2.41715 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 7.73781 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : -9.73541 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 5.30426 (3)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 2.37827 (1)
a=5 o=2 a=0 : 0 (0)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : 7.73781 (1)
a=5 o=2 a=6 : 0 (0)
a=6 : 0.615559 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : 5.688 (1)
Check 1

# # # # 
# 0X1X#
# * . #
# # # # 
Observed bad
Reward 0
Matched 1 states
Created 1 local transformations out of 2 attempts
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 4.1812
Total reward = 3.97214
MCTS Values:
a=0 : 0 (0)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward -0.583769
Total reward = -0.55458
MCTS Values:
a=0 : 0 (0)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -0.55458 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : 0 (0)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 6.30249 (1)
a=6 : -0.55458 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 6.30249 (1)
a=6 : -0.55458 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=0 o=0 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : 7.54436 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 8.14506 (1)
a=0 o=0 a=6 : 0 (0)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 6.30249 (1)
a=6 : -0.55458 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=5 o=2 
Adding sample:

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 28 steps, with total reward -2.90016
Total reward = -2.61739
MCTS Values:
a=0 : 7.54436 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 8.14506 (1)
a=0 o=0 a=6 : 0 (0)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 1.84255 (2)
a=5 o=2 a=0 : 0 (0)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : -2.75515 (1)
a=5 o=2 a=6 : 0 (0)
a=6 : -0.55458 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=1 o=0 
Adding sample:

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -1.51086
Total reward = -1.36355
MCTS Values:
a=0 : 7.54436 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 8.14506 (1)
a=0 o=0 a=6 : 0 (0)
a=1 : 1.3043 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : -1.43532 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 1.84255 (2)
a=5 o=2 a=0 : 0 (0)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : -2.75515 (1)
a=5 o=2 a=6 : 0 (0)
a=6 : -0.55458 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=6 o=2 
Adding sample:

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : 7.54436 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 8.14506 (1)
a=0 o=0 a=6 : 0 (0)
a=1 : 1.3043 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : -1.43532 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 1.84255 (2)
a=5 o=2 a=0 : 0 (0)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : -2.75515 (1)
a=5 o=2 a=6 : 0 (0)
a=6 : 3.79524 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 8.57375 (1)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.5 [0, 1]
Rollout depth: 11.25 [3, 28]
Total reward: 3.62161 [-2.61739, 8.14506]
Policy after 8 simulations
MCTS Policy:
a=0 : 7.54436 (2)
a=0 o=0 a=5 : 8.14506 (1)
Values after 8 simulations
MCTS Values:
a=0 : 7.54436 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 8.14506 (1)
a=0 o=0 a=6 : 0 (0)
a=1 : 1.3043 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : -1.43532 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 1.84255 (2)
a=5 o=2 a=0 : 0 (0)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : -2.75515 (1)
a=5 o=2 a=6 : 0 (0)
a=6 : 3.79524 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 8.57375 (1)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : 0 (0)
N

# # # # 
# * 1X#
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=0 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward -1.43532
Total reward = -1.36355
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.36355 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward -0.45125
Total reward = -10.4287
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.36355 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward -3.69751
Total reward = -3.51263
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.36355 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : -3.51263 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward -0.732154
Total reward = -0.695546
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.36355 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : -3.51263 (1)
a=6 : -0.695546 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : -1.36355 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : -3.51263 (1)
a=6 : -0.695546 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.66914 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 6.30249 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1.36355 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : -3.51263 (1)
a=6 : -0.695546 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward -1.42625
Total reward = -1.28719
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.66914 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 6.30249 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1.36355 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : -3.51263 (1)
a=6 : -0.991369 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : 0 (0)
a=6 o=2 a=5 : -1.35494 (1)
a=6 o=2 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=2 o=0 
Adding sample:

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.1812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.66914 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 6.30249 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 1.40883 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 4.40127 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : -3.51263 (1)
a=6 : -0.991369 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : 0 (0)
a=6 o=2 a=5 : -1.35494 (1)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.375 [0, 1]
Rollout depth: 9.5 [4, 17]
Total reward: 0.0289859 [-10.4287, 7.35092]
Policy after 8 simulations
MCTS Policy:
a=1 : 6.66914 (2)
a=1 o=0 a=3 : 6.30249 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.66914 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 6.30249 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 1.40883 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 4.40127 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : -3.51263 (1)
a=6 : -0.991369 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : 0 (0)
a=6 o=2 a=5 : -1.35494 (1)
a=6 o=2 a=6 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward -1.2306
Total reward = -1.16907
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.16907 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -1.10361
Total reward = -1.04842
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.16907 (1)
a=3 : -1.04842 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.16907 (1)
a=3 : -1.04842 (1)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = -1.42625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.16907 (1)
a=3 : -1.04842 (1)
a=4 : -1.42625 (1)
a=5 : 9.5 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward -9.79937
Total reward = -9.3094
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.16907 (1)
a=3 : -1.04842 (1)
a=4 : -1.42625 (1)
a=5 : 9.5 (1)
a=6 : -9.3094 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -1.16907 (1)
a=3 : -1.04842 (1)
a=4 : -1.42625 (1)
a=5 : 9.5 (1)
a=6 : -9.3094 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=1 o=0 a=5 o=2 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 18.525
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -1.16907 (1)
a=3 : -1.04842 (1)
a=4 : -1.42625 (1)
a=5 : 14.0125 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : 0 (0)
a=5 o=2 a=4 : 19.5 (1)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : -9.3094 (1)
Tree depth: 0.125 [0, 1]
Rollout depth: 6.5 [1, 13]
Total reward: 4.38398 [-9.3094, 18.525]
Policy after 8 simulations
MCTS Policy:
a=5 : 14.0125 (2)
a=5 o=2 a=4 : 19.5 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -1.16907 (1)
a=3 : -1.04842 (1)
a=4 : -1.42625 (1)
a=5 : 14.0125 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : 0 (0)
a=5 o=2 a=4 : 19.5 (1)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : -9.3094 (1)
Check 1

# # # # 
# 0X* #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=1 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = -0.975
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.975 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 12.755
Total reward = 12.1172
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 12.1172 (1)
a=4 : -0.975 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 12.1172 (1)
a=4 : -0.975 (1)
a=5 : 0 (0)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 3.77896
Total reward = 3.59002
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.59002 (1)
a=3 : 12.1172 (1)
a=4 : -0.975 (1)
a=5 : 0 (0)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 28 steps, with total reward 6.47558
Total reward = 6.1518
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.59002 (1)
a=3 : 12.1172 (1)
a=4 : -0.975 (1)
a=5 : 6.1518 (1)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 3.59002 (1)
a=3 : 12.1172 (1)
a=4 : -0.975 (1)
a=5 : 6.1518 (1)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=1 o=0 a=5 o=1 a=3 o=0 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 3.59002 (1)
a=3 : 9.73406 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 7.73781 (1)
a=3 o=0 a=6 : 0 (0)
a=4 : -0.975 (1)
a=5 : 6.1518 (1)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 3.59002 (1)
a=3 : 9.73406 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 7.73781 (1)
a=3 o=0 a=6 : 0 (0)
a=4 : -0.975 (1)
a=5 : 6.1518 (1)
a=6 : 9.5 (1)
Tree depth: 0.125 [0, 1]
Rollout depth: 11.6667 [1, 28]
Total reward: 7.21687 [-0.975, 12.1172]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 3.59002 (1)
a=3 : 9.73406 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 7.73781 (1)
a=3 o=0 a=6 : 0 (0)
a=4 : -0.975 (1)
a=5 : 6.1518 (1)
a=6 : 9.5 (1)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 6.98337, average = 6.98337
Undiscounted return = 10, average = 10
Starting run 2 with 8 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 16.9834
Total reward = 16.1342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.1342 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.1342 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 16.1342 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 16.1342 (1)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 16.1342 (1)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 17.3509
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.9623 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 7.73781 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 16.1342 (1)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=5 o=1 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.9623 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 7.73781 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 11.2183 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 6.6342 (1)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 17.3509
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.7585 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 7.73781 (1)
a=4 o=0 a=2 : 7.73781 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 11.2183 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 6.6342 (1)
a=6 : 7.73781 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 4.75 [2, 8]
Total reward: 12.6875 [6.30249, 18.5738]
Policy after 8 simulations
MCTS Policy:
a=4 : 17.7585 (3)
a=4 o=0 a=1 : 7.73781 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.7585 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 7.73781 (1)
a=4 o=0 a=2 : 7.73781 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 11.2183 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 6.6342 (1)
a=6 : 7.73781 (1)
Sample

# # # # 
# * 1$#
# . . #
# # # # 
Reward -10
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 21 steps, with total reward 3.58486
Total reward = 3.40562
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.40562 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.40562 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.40562 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.35941 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 9.025 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.40562 (1)
a=2 : 7.54436 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.35941 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 9.025 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.14118 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 5.13342 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 7.54436 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.35941 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 9.025 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.14118 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 5.13342 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 7.54436 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.15221 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 8.14506 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 9.025 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.14118 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 5.13342 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.03791 (3)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.15221 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 8.14506 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 9.025 (1)
Tree depth: 0.625 [0, 1]
Rollout depth: 6.875 [1, 21]
Total reward: 7.10659 [3.40562, 9.025]
Policy after 8 simulations
MCTS Policy:
a=6 : 8.15221 (3)
a=6 o=1 a=6 : 9.025 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.14118 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 5.13342 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.03791 (3)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.15221 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 8.14506 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 9.025 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.688 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.688 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.688 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.03688 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.688 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 11.7061
Total reward = 10.5648
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.03688 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.27606 (2)
a=2 o=0 a=0 : 11.1208 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.688 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 16.7188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.03688 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.27606 (2)
a=2 o=0 a=0 : 11.1208 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.2034 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.03688 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.27606 (2)
a=2 o=0 a=0 : 11.1208 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.0482 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 8.14506 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.19125 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.27606 (2)
a=2 o=0 a=0 : 11.1208 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.0482 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 8.14506 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.5987 (1)
Tree depth: 0.625 [0, 1]
Rollout depth: 6.28571 [1, 13]
Total reward: 9.28381 [5.688, 16.7188]
Policy after 8 simulations
MCTS Policy:
a=6 : 10.0482 (3)
a=6 o=1 a=6 : 17.5987 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.19125 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.27606 (2)
a=2 o=0 a=0 : 11.1208 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.0482 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 8.14506 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.5987 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
Matched 2 states
Created 1 local transformations out of 2 attempts
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 17.5987
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 18.5737
Total reward = 17.6451
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.6451 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.6451 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 16.4834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.0642 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 17.3509 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.0642 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 17.3509 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.9506 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 6.6342 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.2341 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 17.3509 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 9.025 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.9506 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 6.6342 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.2341 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 17.3509 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 9.025 (1)
a=2 : 8.58503 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.9506 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 6.6342 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 12.3222
Total reward = 11.1208
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.2341 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 17.3509 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 9.025 (1)
a=2 : 8.58503 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.674 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 6.6342 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 11.7061 (1)
Tree depth: 0.625 [0, 1]
Rollout depth: 5 [1, 12]
Total reward: 11.8618 [6.30249, 17.6451]
Policy after 8 simulations
MCTS Policy:
a=1 : 14.2341 (3)
a=1 o=0 a=4 : 17.3509 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.2341 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 17.3509 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 9.025 (1)
a=2 : 8.58503 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.674 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 6.6342 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 11.7061 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 5.98737 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 18.0737
Total reward = 17.1701
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 17.1701 (1)
a=3 : 5.98737 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 17.3509
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 17.1701 (1)
a=3 : 5.98737 (1)
a=4 : 17.3509 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 17.1701 (1)
a=3 : 5.98737 (1)
a=4 : 17.3509 (1)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 17.1701 (1)
a=3 : 5.98737 (1)
a=4 : 17.3509 (1)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=4 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 17.7378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 17.1701 (1)
a=3 : 5.98737 (1)
a=4 : 17.5444 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 8.14506 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 13.335 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : 5.98737 (1)
a=4 : 17.5444 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 8.14506 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 13.335 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : 5.98737 (1)
a=4 : 17.5444 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 8.14506 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 5 [1, 10]
Total reward: 12.1558 [5.98737, 17.7378]
Policy after 8 simulations
MCTS Policy:
a=4 : 17.5444 (2)
a=4 o=0 a=2 : 8.14506 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 13.335 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : 5.98737 (1)
a=4 : 17.5444 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 8.14506 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward 3.58486
Total reward = 3.23534
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 6.36767 (2)
a=2 o=0 a=0 : 3.40562 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=4 o=0 a=3 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 6.36767 (2)
a=2 o=0 a=0 : 3.40562 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 8.14506 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 8.57375 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 6.36767 (2)
a=2 o=0 a=0 : 3.40562 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 8.14506 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 8.57375 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 6.36767 (2)
a=2 o=0 a=0 : 3.40562 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 8.00931 (3)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 8.14506 (1)
a=3 o=0 a=2 : 8.57375 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0.375 [0, 1]
Rollout depth: 6.6 [1, 21]
Total reward: 8.34541 [3.23534, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 6.36767 (2)
a=2 o=0 a=0 : 3.40562 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 8.00931 (3)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 8.14506 (1)
a=3 o=0 a=2 : 8.57375 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 5.88287, average = 6.43312
Undiscounted return = 10, average = 10
Starting run 3 with 8 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 24.5887
Total reward = 23.3593
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 23.3593 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward -10.2844
Total reward = -9.77018
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -9.77018 (1)
a=6 : 23.3593 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 15.9874
Total reward = 15.188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.188 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -9.77018 (1)
a=6 : 23.3593 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 9.09275
Total reward = 8.63811
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.188 (1)
a=2 : 8.63811 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -9.77018 (1)
a=6 : 23.3593 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 16.3025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.188 (1)
a=2 : 8.63811 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.3025 (1)
a=5 : -9.77018 (1)
a=6 : 23.3593 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.188 (1)
a=2 : 8.63811 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.3025 (1)
a=5 : -9.77018 (1)
a=6 : 14.5236 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 5.98737 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 16.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.188 (1)
a=2 : 8.63811 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.4683 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 6.98337 (1)
a=5 : -9.77018 (1)
a=6 : 14.5236 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 5.98737 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.344 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.63811 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.4683 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 6.98337 (1)
a=5 : -9.77018 (1)
a=6 : 14.5236 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 5.98737 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.375 [0, 1]
Rollout depth: 9 [6, 13]
Total reward: 10.6925 [-9.77018, 23.3593]
Policy after 8 simulations
MCTS Policy:
a=4 : 16.4683 (2)
a=4 o=0 a=6 : 6.98337 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.344 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.63811 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.4683 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 6.98337 (1)
a=5 : -9.77018 (1)
a=6 : 14.5236 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 5.98737 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Sample

# # # # 
# * 1X#
# . . #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 17.1701
Total reward = 16.3116
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 16.3116 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 16.3116 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 14.0403
Total reward = 13.3383
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.3383 (1)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 16.3116 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 31 steps, with total reward -3.54161
Total reward = -3.19631
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.3383 (1)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.55763 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -3.36453 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.7417 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 8.57375 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.55763 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -3.36453 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward -1.94732
Total reward = -1.75745
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.7417 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 8.57375 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 1.55965 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -1.84995 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.55763 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -3.36453 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.48891 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 8.57375 (1)
a=1 o=0 a=3 : 7.35092 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 1.55965 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -1.84995 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.55763 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -3.36453 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.48891 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 8.57375 (1)
a=1 o=0 a=3 : 7.35092 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 1.55965 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -1.84995 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.58315 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 6.98337 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -3.36453 (1)
Tree depth: 0.625 [0, 1]
Rollout depth: 11.125 [3, 31]
Total reward: 6.41693 [-3.19631, 16.3116]
Policy after 8 simulations
MCTS Policy:
a=1 : 9.48891 (3)
a=1 o=0 a=2 : 8.57375 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.48891 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 8.57375 (1)
a=1 o=0 a=3 : 7.35092 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 1.55965 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -1.84995 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.58315 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 6.98337 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -3.36453 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward -1.35446
Total reward = -1.28673
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : -1.28673 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = -0.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : -1.28673 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : -1.28673 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : -1.28673 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 4.87675 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : -1.28673 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 4.87675 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1.28673 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 4.87675 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=6 o=2 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1.28673 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 6.72525 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : 0 (0)
a=6 o=2 a=4 : 0 (0)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 9.025 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 6.8 [1, 16]
Total reward: 6.33297 [-1.28673, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1.28673 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 6.72525 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : 0 (0)
a=6 o=2 a=4 : 0 (0)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 9.025 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 19.025, average = 10.6304
Undiscounted return = 20, average = 13.3333
Starting run 4 with 8 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = -5.8188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -1.85494
Total reward = -1.76219
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 0 (0)
a=6 : -1.76219 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 4.1812
Total reward = 3.97214
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 0 (0)
a=6 : -1.76219 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 0 (0)
a=6 : -1.76219 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 8.33831
Total reward = 7.92139
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 7.92139 (1)
a=6 : -1.76219 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -1.43532
Total reward = 8.20463
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.61481 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 8.63645 (1)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 7.92139 (1)
a=6 : -1.76219 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.61481 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 8.63645 (1)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 7.45238 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 7.35092 (1)
a=6 : -1.76219 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 16.7628
Total reward = 15.1284
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.61481 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 8.63645 (1)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 9.55029 (2)
a=2 o=0 a=0 : 15.9247 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 7.45238 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 7.35092 (1)
a=6 : -1.76219 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 9 [2, 18]
Total reward: 5.45675 [-5.8188, 15.1284]
Policy after 8 simulations
MCTS Policy:
a=2 : 9.55029 (2)
a=2 o=0 a=0 : 15.9247 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.61481 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 8.63645 (1)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 9.55029 (2)
a=2 o=0 a=0 : 15.9247 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 7.45238 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 7.35092 (1)
a=6 : -1.76219 (1)
S

# # # # 
# 0X1$#
# * . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.35092 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : 5.688 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.35092 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward -2.47417
Total reward = -2.35046
MCTS Values:
a=0 : 5.688 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -2.35046 (1)
a=6 : 7.35092 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 32 steps, with total reward -0.880822
Total reward = -0.836781
MCTS Values:
a=0 : 5.688 (1)
a=1 : -0.836781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -2.35046 (1)
a=6 : 7.35092 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=1 
Adding sample:

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : 5.688 (1)
a=1 : -0.836781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -2.35046 (1)
a=6 : 7.74799 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 8.57375 (1)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=2 o=0 a=0 o=0 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward -2.51663
Total reward = -11.7713
MCTS Values:
a=0 : -3.04163 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -12.3908 (1)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : -0.836781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -2.35046 (1)
a=6 : 7.74799 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 8.57375 (1)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Adding sample:

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 27 steps, with total reward -3.35217
Total reward = -3.02533
MCTS Values:
a=0 : -3.04163 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -12.3908 (1)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : -0.836781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -2.35046 (1)
a=6 : 4.15688 (3)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 8.57375 (1)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -3.18456 (1)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Expanding node: a=2 o=0 a=1 o=0 
Adding sample:

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -3.04163 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -12.3908 (1)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 3.25707 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 7.73781 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -2.35046 (1)
a=6 : 4.15688 (3)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 8.57375 (1)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -3.18456 (1)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.5 [0, 1]
Rollout depth: 13.375 [3, 32]
Total reward: 1.31888 [-11.7713, 8.14506]
Policy after 8 simulations
MCTS Policy:
a=6 : 4.15688 (3)
a=6 o=1 a=1 : 8.57375 (1)
Values after 8 simulations
MCTS Values:
a=0 : -3.04163 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -12.3908 (1)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 3.25707 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 7.73781 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -2.35046 (1)
a=6 : 4.15688 (3)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 8.57375 (1)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -3.18456 (1)
a=6 o=1 a=6 : 0 (0)
Check 2

# # # # 
# 0X1$#
# * . #
# # # # 
Observed good
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 25 steps, with total reward 7.79664
Total reward = 7.40681
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.40681 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.40681 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.40681 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 7.40681 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Expanding node: a=2 o=0 a=6 o=1 a=1 o=0 
Adding sample:

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 17.1701
Total reward = 15.496
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 12.498 (2)
a=1 o=0 a=0 : 16.3116 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 7.40681 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 
Adding sample:

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 12.498 (2)
a=1 o=0 a=0 : 16.3116 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.35941 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 8.57375 (1)
a=5 o=1 a=6 : 0 (0)
a=6 : 7.40681 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=2 o=0 a=6 o=1 a=0 o=0 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 16.7188
MCTS Values:
a=0 : 12.2283 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 17.5987 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 12.498 (2)
a=1 o=0 a=0 : 16.3116 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.35941 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 8.57375 (1)
a=5 o=1 a=6 : 0 (0)
a=6 : 7.40681 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=1 a=6 o=1 
Adding sample:

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : 12.2283 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 17.5987 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 12.498 (2)
a=1 o=0 a=0 : 16.3116 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.35941 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 8.57375 (1)
a=5 o=1 a=6 : 0 (0)
a=6 : 7.99028 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 9.025 (1)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.5 [0, 1]
Rollout depth: 5.875 [1, 25]
Total reward: 10.269 [7.40681, 16.7188]
Policy after 8 simulations
MCTS Policy:
a=1 : 12.498 (2)
a=1 o=0 a=0 : 16.3116 (1)
Values after 8 simulations
MCTS Values:
a=0 : 12.2283 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 17.5987 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 12.498 (2)
a=1 o=0 a=0 : 16.3116 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.35941 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 8.57375 (1)
a=5 o=1 a=6 : 0 (0)
a=6 : 7.99028 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 9.025 (1)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
E

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 9.025 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 9.025 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 9.025 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=2 o=0 a=6 o=1 a=1 o=0 a=0 o=0 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : 8.79938 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : 0 (0)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 9.025 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Expanding node: a=2 o=0 a=6 o=1 a=1 o=0 a=5 o=1 
Adding sample:

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : 8.79938 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : 0 (0)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 9.025 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.82253 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 10 (1)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : 0 (0)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=6 : 6.6342 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 4.2 [2, 8]
Total reward: 8.70198 [6.6342, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : 8.79938 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : 0 (0)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 9.025 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.82253 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 10 (1)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : 0 (0)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=6 : 6.6342 (1)
E

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 8.57375, average = 10.1162
Undiscounted return = 10, average = 12.5
Starting run 5 with 8 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 17.5987
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 5.4036 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 5.4036 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.6342 (1)
a=6 : 5.4036 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = -0.975
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 6.6342 (1)
a=6 : 5.4036 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.5494 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 6.6342 (1)
a=6 : 5.4036 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.1812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.5494 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.6031 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 6.6342 (1)
a=6 : 5.4036 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward -3.16972
Total reward = -2.86067
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.5494 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.6031 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 1.88677 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : -3.01123 (1)
a=6 : 5.4036 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 8.71429 [2, 18]
Total reward: 6.06339 [-2.86067, 17.5987]
Policy after 8 simulations
MCTS Policy:
a=1 : 13.5494 (2)
a=1 o=0 a=1 : 10 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.5494 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.6031 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 1.88677 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : -3.01123 (1)
a=6 : 5.4036 (1)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = -3.69751
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -3.69751 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -3.69751 (1)
a=5 : 0 (0)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 30 steps, with total reward -0.51454
Total reward = -0.488813
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.488813 (1)
a=3 : 0 (0)
a=4 : -3.69751 (1)
a=5 : 0 (0)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.488813 (1)
a=3 : 0 (0)
a=4 : -3.69751 (1)
a=5 : 9.5 (1)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -0.488813 (1)
a=3 : 0 (0)
a=4 : -3.69751 (1)
a=5 : 9.5 (1)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward -2.64908
Total reward = -2.51663
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -0.488813 (1)
a=3 : -2.51663 (1)
a=4 : -3.69751 (1)
a=5 : 9.5 (1)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -0.488813 (1)
a=3 : -2.51663 (1)
a=4 : -3.69751 (1)
a=5 : 9.5 (1)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=1 o=0 a=5 o=2 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -0.488813 (1)
a=3 : -2.51663 (1)
a=4 : -3.69751 (1)
a=5 : 9.5 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 10 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : 0 (0)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : 8.14506 (1)
Tree depth: 0.125 [0, 1]
Rollout depth: 10.2 [1, 30]
Total reward: 5.05526 [-3.69751, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -0.488813 (1)
a=3 : -2.51663 (1)
a=4 : -3.69751 (1)
a=5 : 9.5 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 10 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : 0 (0)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : 8.14506 (1)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.5, average = 9.993
Undiscounted return = 10, average = 12
Starting run 6 with 8 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 15.9874
Total reward = 15.188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 15.188 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 13.7072
Total reward = 13.0218
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 13.0218 (1)
a=6 : 15.188 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward -0.45125
Total reward = -0.428688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.428688 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 13.0218 (1)
a=6 : 15.188 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -2.14908
Total reward = -12.0416
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.428688 (1)
a=3 : -1e+10 (1000000)
a=4 : -12.0416 (1)
a=5 : 13.0218 (1)
a=6 : 15.188 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 19.5
Total reward = 18.525
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 18.525 (1)
a=2 : -0.428688 (1)
a=3 : -1e+10 (1000000)
a=4 : -12.0416 (1)
a=5 : 13.0218 (1)
a=6 : 15.188 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.0125 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -0.428688 (1)
a=3 : -1e+10 (1000000)
a=4 : -12.0416 (1)
a=5 : 13.0218 (1)
a=6 : 15.188 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward -2.04981
Total reward = -1.84995
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.0125 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -0.428688 (1)
a=3 : -1e+10 (1000000)
a=4 : -12.0416 (1)
a=5 : 13.0218 (1)
a=6 : 6.66902 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1.94732 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=5 o=1 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.0125 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -0.428688 (1)
a=3 : -1e+10 (1000000)
a=4 : -12.0416 (1)
a=5 : 10.7978 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 9.025 (1)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=6 : 6.66902 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1.94732 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.375 [0, 1]
Rollout depth: 7.42857 [2, 14]
Total reward: 6.31104 [-12.0416, 18.525]
Policy after 8 simulations
MCTS Policy:
a=1 : 14.0125 (2)
a=1 o=0 a=1 : 10 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.0125 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -0.428688 (1)
a=3 : -1e+10 (1000000)
a=4 : -12.0416 (1)
a=5 : 10.7978 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 9.025 (1)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=6 : 6.66902 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1.94732 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = -0.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 9.025 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward -1.66958
Total reward = -1.5861
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 9.025 (1)
a=6 : -1.5861 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 9.025 (1)
a=6 : -1.5861 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 27 steps, with total reward -13.0528
Total reward = -12.4002
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -12.4002 (1)
a=4 : -0.5 (1)
a=5 : 9.025 (1)
a=6 : -1.5861 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 23 steps, with total reward -5.78966
Total reward = -5.50018
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -5.50018 (1)
a=3 : -12.4002 (1)
a=4 : -0.5 (1)
a=5 : 9.025 (1)
a=6 : -1.5861 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -5.50018 (1)
a=3 : -12.4002 (1)
a=4 : -0.5 (1)
a=5 : 9.025 (1)
a=6 : -1.5861 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=1 o=0 a=5 o=2 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -5.50018 (1)
a=3 : -12.4002 (1)
a=4 : -0.5 (1)
a=5 : 8.58503 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 8.57375 (1)
a=5 o=2 a=3 : 0 (0)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : -1.5861 (1)
Tree depth: 0.125 [0, 1]
Rollout depth: 12 [1, 27]
Total reward: 2.14795 [-12.4002, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -5.50018 (1)
a=3 : -12.4002 (1)
a=4 : -0.5 (1)
a=5 : 8.58503 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 8.57375 (1)
a=5 o=2 a=3 : 0 (0)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : -1.5861 (1)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.5, average = 9.91083
Undiscounted return = 10, average = 11.6667
Starting run 7 with 8 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 20 steps, with total reward 3.77354
Total reward = 3.58486
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 3.58486 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = 14.1812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 14.1812 (1)
a=5 : 0 (0)
a=6 : 3.58486 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 19.025
Total reward = 18.0737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 14.1812 (1)
a=5 : 18.0737 (1)
a=6 : 3.58486 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -3.19751
Total reward = -3.03763
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -3.03763 (1)
a=3 : -1e+10 (1000000)
a=4 : 14.1812 (1)
a=5 : 18.0737 (1)
a=6 : 3.58486 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 14 steps, with total reward -3.44033
Total reward = -3.26831
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -3.26831 (1)
a=2 : -3.03763 (1)
a=3 : -1e+10 (1000000)
a=4 : 14.1812 (1)
a=5 : 18.0737 (1)
a=6 : 3.58486 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 25.4247
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -3.26831 (1)
a=2 : -3.03763 (1)
a=3 : -1e+10 (1000000)
a=4 : 14.1812 (1)
a=5 : 21.7492 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 26.7628 (1)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=6 : 3.58486 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = -5.36709
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -3.26831 (1)
a=2 : -3.03763 (1)
a=3 : -1e+10 (1000000)
a=4 : 4.40706 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 4.87675 (1)
a=5 : 21.7492 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 26.7628 (1)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=6 : 3.58486 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -3.26831 (1)
a=2 : -3.03763 (1)
a=3 : -1e+10 (1000000)
a=4 : 4.40706 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 4.87675 (1)
a=5 : 17.0787 (3)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 26.7628 (1)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 8.14506 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : 3.58486 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 10.875 [3, 20]
Total reward: 7.16616 [-5.36709, 25.4247]
Policy after 8 simulations
MCTS Policy:
a=5 : 17.0787 (3)
a=5 o=1 a=4 : 26.7628 (1)
a=5 o=2 a=1 : 8.14506 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -3.26831 (1)
a=2 : -3.03763 (1)
a=3 : -1e+10 (1000000)
a=4 : 4.40706 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 4.87675 (1)
a=5 : 17.0787 (3)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 26.7628 (1)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 8.14506 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : 3.58486 (1)
Check 1

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 48 steps, with total reward 5.53036
Total reward = 15.2538
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.2538 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 24.0403
Total reward = 22.8383
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.2538 (1)
a=5 : 0 (0)
a=6 : 22.8383 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.2538 (1)
a=5 : 8.14506 (1)
a=6 : 22.8383 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 25 steps, with total reward 16.5047
Total reward = 15.6795
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.6795 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.2538 (1)
a=5 : 8.14506 (1)
a=6 : 22.8383 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.6795 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.2538 (1)
a=5 : 8.14506 (1)
a=6 : 22.8383 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 14.6334
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.6795 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.2538 (1)
a=5 : 8.14506 (1)
a=6 : 18.7359 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 15.4036 (1)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=5 o=1 a=1 o=0 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 14.3343
Total reward = 12.9367
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.3081 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 13.6176 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.2538 (1)
a=5 : 8.14506 (1)
a=6 : 18.7359 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 15.4036 (1)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=1 a=4 o=0 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 16.9834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.3081 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 13.6176 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.1186 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 7.35092 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 8.14506 (1)
a=6 : 18.7359 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 15.4036 (1)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.375 [0, 1]
Rollout depth: 15 [4, 48]
Total reward: 14.1817 [6.98337, 22.8383]
Policy after 8 simulations
MCTS Policy:
a=6 : 18.7359 (2)
a=6 o=1 a=4 : 15.4036 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.3081 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 13.6176 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.1186 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 7.35092 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 8.14506 (1)
a=6 : 18.7359 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 15.4036 (1)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
Matched 1 states
Created 1 local transformations out of 3 attempts
Expanding node: a=5 o=1 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 11.1646
Total reward = 10.6063
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 10.6063 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 5.76482
Total reward = 5.47658
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.47658 (1)
a=6 : 10.6063 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 17.5987
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.47658 (1)
a=6 : 10.6063 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 16.3759
Total reward = 15.5571
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 15.5571 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.47658 (1)
a=6 : 10.6063 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 13.5487
Total reward = 22.8712
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 15.5571 (1)
a=3 : -1e+10 (1000000)
a=4 : 22.8712 (1)
a=5 : 5.47658 (1)
a=6 : 10.6063 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 17.7378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 15.5571 (1)
a=3 : -1e+10 (1000000)
a=4 : 20.3045 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 8.14506 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 5.47658 (1)
a=6 : 10.6063 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=1 o=0 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.5494 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 15.5571 (1)
a=3 : -1e+10 (1000000)
a=4 : 20.3045 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 8.14506 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 5.47658 (1)
a=6 : 10.6063 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 
Adding sample:

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward -2.58638
Total reward = -2.33421
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.5494 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.61146 (2)
a=2 o=0 a=0 : -2.45706 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 20.3045 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 8.14506 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 5.47658 (1)
a=6 : 10.6063 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 10.4286 [3, 18]
Total reward: 12.1267 [-2.33421, 22.8712]
Policy after 8 simulations
MCTS Policy:
a=4 : 20.3045 (2)
a=4 o=0 a=1 : 8.14506 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.5494 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.61146 (2)
a=2 o=0 a=0 : -2.45706 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 20.3045 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 8.14506 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 5.47658 (1)
a=6 : 10.6063 (1)
Sample

# # # # 
# * 1$#
# . . #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward -1.85494
Total reward = -1.76219
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.76219 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 10.5648
Total reward = 10.0365
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.76219 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.0365 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 3.40562
Total reward = 3.23534
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.76219 (1)
a=2 : 3.23534 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.0365 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.76219 (1)
a=2 : 3.23534 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.33536 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 6.98337 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.76219 (1)
a=2 : 6.13017 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.33536 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 6.98337 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 19.5
Total reward = 17.5987
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.76219 (1)
a=2 : 6.13017 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.4232 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 18.525 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 6.98337 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward -2.26219
Total reward = -2.04163
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.76219 (1)
a=2 : 6.13017 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.05696 (4)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 18.525 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : -2.14908 (1)
a=6 o=2 a=2 : 6.98337 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward -0.367546
Total reward = -0.33171
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.04695 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -0.349169 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.13017 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.05696 (4)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 18.525 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : -2.14908 (1)
a=6 o=2 a=2 : 6.98337 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.625 [0, 1]
Rollout depth: 8.25 [1, 22]
Total reward: 5.29928 [-2.04163, 17.5987]
Policy after 8 simulations
MCTS Policy:
a=6 : 8.05696 (4)
a=6 o=1 a=1 : 18.525 (1)
a=6 o=2 a=2 : 6.98337 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.04695 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -0.349169 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.13017 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.05696 (4)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 18.525 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : -2.14908 (1)
a=6 o=2 a=2 : 6.98337 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
Matched 1 states
Created 1 local transformations out of 4 attempts
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.24169 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.35092 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 16.7188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.24169 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.35092 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 12.6463 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.24169 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.35092 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 7.50618 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 12.6463 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 16.7628
Total reward = 15.1284
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.24169 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.35092 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 7.50618 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 13.4737 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 15.9247 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.24169 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.35092 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 7.50618 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 12.3615 (4)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 9.5 (1)
a=6 o=1 a=2 : 15.9247 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.5987 (1)
Tree depth: 0.625 [0, 1]
Rollout depth: 3.875 [1, 10]
Total reward: 10.1177 [5.98737, 16.7188]
Policy after 8 simulations
MCTS Policy:
a=6 : 12.3615 (4)
a=6 o=1 a=6 : 17.5987 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.24169 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.35092 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 7.50618 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 12.3615 (4)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 9.5 (1)
a=6 o=1 a=2 : 15.9247 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.5987 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
Matched 3 states
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.40127
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 6.48751 (2)
a=2 o=0 a=0 : 4.63291 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 6.48751 (2)
a=2 o=0 a=0 : 4.63291 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.36059 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 8.14506 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.22378 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.48751 (2)
a=2 o=0 a=0 : 4.63291 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.36059 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 8.14506 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.1812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.22378 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.48751 (2)
a=2 o=0 a=0 : 4.63291 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.3008 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 8.14506 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 4.40127 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 16.8509
Total reward = 15.208
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.88517 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 16.0084 (1)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.48751 (2)
a=2 o=0 a=0 : 4.63291 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.3008 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 8.14506 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 4.40127 (1)
Tree depth: 0.625 [0, 1]
Rollout depth: 8 [3, 16]
Total reward: 7.69161 [4.1812, 15.208]
Policy after 8 simulations
MCTS Policy:
a=1 : 9.88517 (3)
a=1 o=0 a=2 : 16.0084 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.88517 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 16.0084 (1)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.48751 (2)
a=2 o=0 a=0 : 4.63291 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.3008 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 8.14506 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 4.40127 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 18.5738 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.73781 (1)
a=4 : 18.5738 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 17.7378
Total reward = 16.8509
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.73781 (1)
a=4 : 18.5738 (1)
a=5 : -1e+10 (1000000)
a=6 : 16.8509 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 13.3383
Total reward = 12.6714
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 12.6714 (1)
a=3 : 7.73781 (1)
a=4 : 18.5738 (1)
a=5 : -1e+10 (1000000)
a=6 : 16.8509 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 12.6714 (1)
a=3 : 7.73781 (1)
a=4 : 18.5738 (1)
a=5 : -1e+10 (1000000)
a=6 : 16.8509 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=4 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = -3.3658
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 12.6714 (1)
a=3 : 7.73781 (1)
a=4 : 7.60398 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 6.98337 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 16.8509 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=6 o=1 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 18.525
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 12.6714 (1)
a=3 : 7.73781 (1)
a=4 : 7.60398 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 6.98337 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.688 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 19.5 (1)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.65279 (2)
a=2 o=0 a=0 : 6.98337 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : 7.73781 (1)
a=4 : 7.60398 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 6.98337 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.688 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 19.5 (1)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.375 [0, 1]
Rollout depth: 5.71429 [1, 11]
Total reward: 10.9534 [-3.3658, 18.5738]
Policy after 8 simulations
MCTS Policy:
a=6 : 17.688 (2)
a=6 o=1 a=4 : 19.5 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.65279 (2)
a=2 o=0 a=0 : 6.98337 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : 7.73781 (1)
a=4 : 7.60398 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 6.98337 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.688 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 19.5 (1)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Check 2

# # # # 
# . * #
# . . #
# # # # 
Observed good
Reward 0
Matched 1 states
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=6 o=1 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 12.2899
Total reward = 11.6754
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 11.6754 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 15.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 11.6754 (1)
a=3 : 0 (0)
a=4 : 15.688 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 16.6342
Total reward = 15.8025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 11.6754 (1)
a=3 : 0 (0)
a=4 : 15.688 (1)
a=5 : -1e+10 (1000000)
a=6 : 15.8025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 11.6754 (1)
a=3 : 8.14506 (1)
a=4 : 15.688 (1)
a=5 : -1e+10 (1000000)
a=6 : 15.8025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 11.6754 (1)
a=3 : 8.14506 (1)
a=4 : 15.688 (1)
a=5 : -1e+10 (1000000)
a=6 : 15.8025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=6 o=1 a=6 o=1 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.40127
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 11.6754 (1)
a=3 : 8.14506 (1)
a=4 : 15.688 (1)
a=5 : -1e+10 (1000000)
a=6 : 10.1019 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 4.63291 (1)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=6 o=1 a=4 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 11.6754 (1)
a=3 : 8.14506 (1)
a=4 : 17.3565 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 9.5 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.1019 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 4.63291 (1)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=6 o=1 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 12.975
Total reward = 11.71
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 11.6927 (2)
a=2 o=0 a=0 : 12.3263 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : 8.14506 (1)
a=4 : 17.3565 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 9.5 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.1019 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 4.63291 (1)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.375 [0, 1]
Rollout depth: 9.71429 [1, 17]
Total reward: 12.0559 [4.40127, 19.025]
Policy after 8 simulations
MCTS Policy:
a=4 : 17.3565 (2)
a=4 o=0 a=2 : 9.5 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 11.6927 (2)
a=2 o=0 a=0 : 12.3263 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : 8.14506 (1)
a=4 : 17.3565 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 9.5 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.1019 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 4.63291 (1)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=6 o=1 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 5.98737 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.025 (1)
a=3 : 5.98737 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.025 (1)
a=3 : 5.98737 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=6 o=1 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.3814 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 8.14506 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 5.98737 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=6 o=1 a=4 o=0 a=3 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.3814 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 8.14506 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 7.06622 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 8.57375 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.3814 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 8.14506 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 7.06622 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 8.57375 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.75427 (3)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 8.14506 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 7.06622 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 8.57375 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0.375 [0, 1]
Rollout depth: 4.75 [2, 10]
Total reward: 8.79941 [5.98737, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.75427 (3)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 8.14506 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 7.06622 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 8.57375 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 22.6426, average = 11.7297
Undiscounted return = 30, average = 14.2857
Starting run 8 with 8 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -1.51086
Total reward = -1.43532
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1.43532 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1.43532 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 16.7628
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.7628 (1)
a=6 : -1.43532 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 17.7378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 16.7628 (1)
a=6 : -1.43532 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 16.7628 (1)
a=6 : -1.43532 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 16.7628
Total reward = 5.12844
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : 11.4331 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 15.9247 (1)
a=5 : 16.7628 (1)
a=6 : -1.43532 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = -3.19751
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : 11.4331 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 15.9247 (1)
a=5 : 6.78265 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -3.3658 (1)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : -1.43532 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : 11.4331 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 15.9247 (1)
a=5 : 6.78265 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -3.3658 (1)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : -1.43532 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 5.625 [1, 10]
Total reward: 7.43858 [-3.19751, 17.7378]
Policy after 8 simulations
MCTS Policy:
a=4 : 11.4331 (2)
a=4 o=0 a=6 : 15.9247 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : 11.4331 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 15.9247 (1)
a=5 : 6.78265 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -3.3658 (1)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : -1.43532 (1)
Sample

# # # # 
# * 1$#
# . . #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward -2.10662
Total reward = -2.00129
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -2.00129 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -2.00129 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 34 steps, with total reward 4.47546
Total reward = 4.25169
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.25169 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -2.00129 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.25169 (1)
a=2 : 7.94144 (2)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -2.00129 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.11953 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.30249 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 7.94144 (2)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -2.00129 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 27 steps, with total reward 2.6352
Total reward = 2.37827
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.11953 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.30249 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.08705 (3)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -2.00129 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.11953 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.30249 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.08705 (3)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 2.86826 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 8.14506 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 25 steps, with total reward 11.065
Total reward = 9.98612
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.74173 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.30249 (1)
a=1 o=0 a=3 : 10.5117 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.08705 (3)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 2.86826 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 8.14506 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.625 [0, 1]
Rollout depth: 15.25 [3, 34]
Total reward: 5.52785 [-2.00129, 9.98612]
Policy after 8 simulations
MCTS Policy:
a=1 : 6.74173 (3)
a=1 o=0 a=3 : 10.5117 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.74173 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.30249 (1)
a=1 o=0 a=3 : 10.5117 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.08705 (3)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 2.86826 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 8.14506 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 6.98337 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 6.98337 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 6.98337 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 6.98337 (1)
a=4 : 19.025 (1)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.57375 (1)
a=3 : 6.98337 (1)
a=4 : 19.025 (1)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=4 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 19.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.57375 (1)
a=3 : 6.98337 (1)
a=4 : 19.2625 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 16.9834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.57375 (1)
a=3 : 6.98337 (1)
a=4 : 18.5028 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 7.35092 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.57375 (1)
a=3 : 6.98337 (1)
a=4 : 18.5028 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 7.35092 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 4 [2, 7]
Total reward: 12.5113 [6.98337, 19.5]
Policy after 8 simulations
MCTS Policy:
a=4 : 18.5028 (3)
a=4 o=0 a=1 : 10 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.57375 (1)
a=3 : 6.98337 (1)
a=4 : 18.5028 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 7.35092 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.14506 (1)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.14506 (1)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=4 o=0 a=3 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = 3.97214
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.14506 (1)
a=3 : 6.27295 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 4.1812 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.82253 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 6.27295 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 4.1812 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.82253 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 6.27295 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 4.1812 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.09309 (3)
a=2 o=0 a=0 : 6.98337 (1)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 6.27295 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 4.1812 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0.375 [0, 1]
Rollout depth: 7.75 [3, 17]
Total reward: 8.35314 [3.97214, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.09309 (3)
a=2 o=0 a=0 : 6.98337 (1)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 6.27295 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 4.1812 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 27.5987, average = 13.7133
Undiscounted return = 30, average = 16.25
Starting run 9 with 8 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 14.6334
Total reward = 13.9017
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 13.9017 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward -1.67408
Total reward = -1.59038
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 13.9017 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 20 steps, with total reward -4.37153
Total reward = -4.15295
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -4.15295 (1)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 13.9017 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -4.15295 (1)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 13.9017 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -4.15295 (1)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 13.9017 (1)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = -4.86658
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -4.15295 (1)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : 7.07921 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 5.4036 (1)
a=5 : 13.9017 (1)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 12 steps, with total reward -11.5498
Total reward = -10.4237
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -4.15295 (1)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : 7.07921 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 5.4036 (1)
a=5 : 1.73902 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -10.9723 (1)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -4.15295 (1)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : 7.07921 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 5.4036 (1)
a=5 : 1.73902 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -10.9723 (1)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : 6.43562 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 5.4036 (1)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : 0 (0)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.375 [0, 1]
Rollout depth: 10.5 [2, 20]
Total reward: 3.09555 [-10.4237, 19.025]
Policy after 8 simulations
MCTS Policy:
a=4 : 7.07921 (2)
a=4 o=0 a=6 : 5.4036 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -4.15295 (1)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : 7.07921 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 5.4036 (1)
a=5 : 1.73902 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -10.9723 (1)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : 6.43562 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 5.4036 (1)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : 0 (0)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : 0 (0)
Sample

# # # # 
# * 1$#
# . . #
# # # # 
Reward -10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 11.391
Total reward = 10.8214
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.8214 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 27 steps, with total reward 2.6352
Total reward = 2.50344
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.8214 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 11.7061
Total reward = 10.5648
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.6931 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 11.1208 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.6931 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 11.1208 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 3.40562
Total reward = 3.07357
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.15325 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 3.23534 (1)
a=6 o=1 a=2 : 11.1208 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.32425 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.15325 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 3.23534 (1)
a=6 o=1 a=2 : 11.1208 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.75427 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 8.14506 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.32425 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.15325 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 3.23534 (1)
a=6 o=1 a=2 : 11.1208 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.625 [0, 1]
Rollout depth: 12 [2, 27]
Total reward: 7.67138 [2.50344, 10.8214]
Policy after 8 simulations
MCTS Policy:
a=1 : 8.75427 (3)
a=1 o=0 a=1 : 10 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.75427 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 8.14506 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.32425 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.15325 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 3.23534 (1)
a=6 o=1 a=2 : 11.1208 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 16.3025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 16.3025 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : 16.3025 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : 16.3025 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : 16.3025 (1)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : 16.3025 (1)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=4 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 19.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : 17.9012 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : 17.9012 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=6 o=1 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 15.0887
Total reward = 13.6176
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : 17.9012 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.5588 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 14.3343 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 4.8 [1, 9]
Total reward: 11.9714 [7.35092, 19.5]
Policy after 8 simulations
MCTS Policy:
a=4 : 17.9012 (2)
a=4 o=0 a=1 : 10 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : 17.9012 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.5588 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 14.3343 (1)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.63291
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 4.63291 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 4.63291 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 4.63291 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 4.63291 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.42546 (2)
a=2 o=0 a=0 : 7.73781 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 4.63291 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=4 o=0 a=3 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 20 steps, with total reward 3.77354
Total reward = 3.40562
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.42546 (2)
a=2 o=0 a=0 : 7.73781 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 4.01926 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 3.58486 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.42546 (2)
a=2 o=0 a=0 : 7.73781 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 4.01926 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 3.58486 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.47489 (3)
a=2 o=0 a=0 : 7.73781 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 9.025 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 4.01926 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 3.58486 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0.375 [0, 1]
Rollout depth: 8.6 [1, 20]
Total reward: 7.9329 [3.40562, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.47489 (3)
a=2 o=0 a=0 : 7.73781 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 9.025 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 4.01926 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 3.58486 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 7.59875, average = 13.0339
Undiscounted return = 10, average = 15.5556
Starting run 10 with 8 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 18.0737
Total reward = 17.1701
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.1701 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.1701 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.73781 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward -0.367546
Total reward = -10.3492
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.1701 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.3492 (1)
a=5 : 7.73781 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 2.74589
Total reward = 2.60859
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.1701 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.3492 (1)
a=5 : 7.73781 (1)
a=6 : 2.60859 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.1517 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 5.4036 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.3492 (1)
a=5 : 7.73781 (1)
a=6 : 2.60859 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -0.428688
Total reward = -0.38689
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.1517 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 5.4036 (1)
a=2 : 4.31905 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -0.407253 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.3492 (1)
a=5 : 7.73781 (1)
a=6 : 2.60859 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = -3.19751
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.1517 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 5.4036 (1)
a=2 : 4.31905 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -0.407253 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.3492 (1)
a=5 : 2.27015 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -3.3658 (1)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : 2.60859 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 7.75 [2, 18]
Total reward: 3.46767 [-10.3492, 17.1701]
Policy after 8 simulations
MCTS Policy:
a=1 : 11.1517 (2)
a=1 o=0 a=6 : 5.4036 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.1517 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 5.4036 (1)
a=2 : 4.31905 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -0.407253 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.3492 (1)
a=5 : 2.27015 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -3.3658 (1)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : 2.60859 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 16.8509
Total reward = 16.0084
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 16.0084 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward -1.2306
Total reward = -1.16907
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1.16907 (1)
a=6 : 16.0084 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward -2.51663
Total reward = -2.3908
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -2.3908 (1)
a=4 : 0 (0)
a=5 : -1.16907 (1)
a=6 : 16.0084 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 17.2378
Total reward = 26.3759
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -2.3908 (1)
a=4 : 26.3759 (1)
a=5 : -1.16907 (1)
a=6 : 16.0084 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -2.3908 (1)
a=4 : 26.3759 (1)
a=5 : -1.16907 (1)
a=6 : 16.0084 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward -1.36355
Total reward = -1.29537
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.29537 (1)
a=3 : -2.3908 (1)
a=4 : 26.3759 (1)
a=5 : -1.16907 (1)
a=6 : 16.0084 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=1 o=0 a=4 o=0 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.29537 (1)
a=3 : -2.3908 (1)
a=4 : 22.7005 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 9.5 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : 0 (0)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1.16907 (1)
a=6 : 16.0084 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=1 o=0 a=6 o=1 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 18.525
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.29537 (1)
a=3 : -2.3908 (1)
a=4 : 22.7005 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 9.5 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : 0 (0)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1.16907 (1)
a=6 : 17.2667 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 19.5 (1)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 6.71429 [1, 13]
Total reward: 10.6349 [-2.3908, 26.3759]
Policy after 8 simulations
MCTS Policy:
a=4 : 22.7005 (2)
a=4 o=0 a=2 : 9.5 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.29537 (1)
a=3 : -2.3908 (1)
a=4 : 22.7005 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 9.5 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : 0 (0)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1.16907 (1)
a=6 : 17.2667 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 19.5 (1)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Sample

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.57375 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 20 steps, with total reward -4.37153
Total reward = -4.15295
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.57375 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -4.15295 (1)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.57375 (1)
a=3 : 9.025 (1)
a=4 : -1e+10 (1000000)
a=5 : -4.15295 (1)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.57375 (1)
a=3 : 9.025 (1)
a=4 : -1e+10 (1000000)
a=5 : -4.15295 (1)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=1 o=0 a=4 o=0 a=3 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.57375 (1)
a=3 : 8.18796 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 7.73781 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 0 (0)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -4.15295 (1)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Expanding node: a=1 o=0 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.79938 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 9.5 (1)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 8.18796 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 7.73781 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 0 (0)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -4.15295 (1)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.79938 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 9.5 (1)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 8.18796 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 7.73781 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 0 (0)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -4.15295 (1)
a=6 : -1e+10 (1000000)
Tree depth: 0.25 [0, 1]
Rollout depth: 6.2 [1, 20]
Total reward: 7.47771 [-4.15295, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.79938 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 9.5 (1)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 8.18796 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 7.73781 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 0 (0)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -4.15295 (1)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 18.525, average = 13.583
Undiscounted return = 20, average = 16
Simulations = 8
Runs = 10
Undiscounted return = 16 +- 2.52982
Discounted return = 13.583 +- 2.30141
Time = 0.0071135
Initialising fast UCB table... done
Main runs
Starting run 1 with 1 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 15.1334
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.1334 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 13 [13, 13]
Total reward: 15.1334 [15.1334, 15.1334]
Policy after 1 simulations
MCTS Policy:
a=4 : 15.1334 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.1334 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# * 1X#
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward -0.975
Total reward = -0.92625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: -0.92625 [-0.92625, -0.92625]
Policy after 1 simulations
MCTS Policy:
a=2 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
S

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.4036 (1)
Tree depth: 0 [0, 0]
Rollout depth: 12 [12, 12]
Total reward: 5.4036 [5.4036, 5.4036]
Policy after 1 simulations
MCTS Policy:
a=6 : 5.4036 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.4036 (1)
Check 2

# # # # 
# . 1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=2 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
E

# # # # 
# . 1X#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=3 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
W

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 28 steps, with total reward 2.50344
Total reward = 2.37827
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 2.37827 (1)
Tree depth: 0 [0, 0]
Rollout depth: 28 [28, 28]
Total reward: 2.37827 [2.37827, 2.37827]
Policy after 1 simulations
MCTS Policy:
a=6 : 2.37827 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 2.37827 (1)
Check 2

# # # # 
# . 1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 a=6 o=2 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 19 steps, with total reward -4.17292
Total reward = -3.96427
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -3.96427 (1)
Tree depth: 0 [0, 0]
Rollout depth: 19 [19, 19]
Total reward: -3.96427 [-3.96427, -3.96427]
Policy after 1 simulations
MCTS Policy:
a=0 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -3.96427 (1)
N

# # # # 
# * 1X#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 a=6 o=2 a=0 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 12 [12, 12]
Total reward: 5.4036 [5.4036, 5.4036]
Policy after 1 simulations
MCTS Policy:
a=2 : 5.4036 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
S

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 a=6 o=2 a=0 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 8.57375 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=1 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 8.57375 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
E

# # # # 
# . 1X#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 a=6 o=2 a=0 o=0 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 19.5
Total reward = 18.525
MCTS Values:
a=0 : 18.525 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 18.525 [18.525, 18.525]
Policy after 1 simulations
MCTS Policy:
a=0 : 18.525 (1)
Values after 1 simulations
MCTS Values:
a=0 : 18.525 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
N

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 a=6 o=2 a=0 o=0 a=2 o=0 a=1 o=0 a=0 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 17.3509
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 17.3509 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 17.3509 [17.3509, 17.3509]
Policy after 1 simulations
MCTS Policy:
a=4 : 17.3509 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 17.3509 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=6 o=2 a=1 o=0 a=3 o=0 a=6 o=2 a=0 o=0 a=2 o=0 a=1 o=0 a=0 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.70063, average = 9.70063
Undiscounted return = 10, average = 10
Starting run 2 with 1 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 18.5738 [18.5738, 18.5738]
Policy after 1 simulations
MCTS Policy:
a=4 : 18.5738 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# * 1$#
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=2 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
S

# # # # 
# . 1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=6 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
Check 2

# # # # 
# . 1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=2 o=0 a=6 o=1 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=6 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
Check 2

# # # # 
# . 1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=4 o=0 a=6 o=1 a=2 o=0 a=6 o=1 a=6 o=1 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
Check 2

# # # # 
# . 1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=2 o=0 a=6 o=1 a=6 o=1 a=6 o=1 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 20 steps, with total reward 7.74568
Total reward = 7.3584
MCTS Values:
a=0 : 0 (0)
a=1 : 7.3584 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 20 [20, 20]
Total reward: 7.3584 [7.3584, 7.3584]
Policy after 1 simulations
MCTS Policy:
a=1 : 7.3584 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 7.3584 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=2 o=0 a=6 o=1 a=6 o=1 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = -3.01663, average = 3.342
Undiscounted return = 0, average = 5
Starting run 3 with 1 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 30 steps, with total reward 10.8331
Total reward = 10.2915
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 10.2915 (1)
Tree depth: 0 [0, 0]
Rollout depth: 30 [30, 30]
Total reward: 10.2915 [10.2915, 10.2915]
Policy after 1 simulations
MCTS Policy:
a=6 : 10.2915 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 10.2915 (1)
Check 2

# # # # 
# 0$* #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=1 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 19.5 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 19.5 [19.5, 19.5]
Policy after 1 simulations
MCTS Policy:
a=4 : 19.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 19.5 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=1 a=4 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.35092 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=2 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.35092 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
S

# # # # 
# 0$. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=1 a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0$. #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 17.1701, average = 7.95136
Undiscounted return = 20, average = 10
Starting run 4 with 1 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward -1.84257
Total reward = -1.75044
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.75044 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 10 [10, 10]
Total reward: -1.75044 [-1.75044, -1.75044]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.75044 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -2.14908
Total reward = -2.04163
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -2.04163 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: -2.04163 [-2.04163, -2.04163]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -2.04163 (1)
a=6 : 0 (0)
Check 2

# # # # 
# 0$* #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=1 o=0 a=6 o=2 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=6 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 9.5 (1)
Check 2

# # # # 
# 0$* #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -0.33171
Total reward = -0.315125
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -0.315125 (1)
Tree depth: 0 [0, 0]
Rollout depth: 10 [10, 10]
Total reward: -0.315125 [-0.315125, -0.315125]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -0.315125 (1)
Sample

# # # # 
# 0$* #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=2 a=6 o=2 a=4 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=5 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : -1e+10 (1000000)
Check 1

# # # # 
# 0$* #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=1 o=0 a=6 o=2 a=6 o=2 a=4 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 8.80922
Total reward = 8.36876
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.36876 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 22 [22, 22]
Total reward: 8.36876 [8.36876, 8.36876]
Policy after 1 simulations
MCTS Policy:
a=2 : 8.36876 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.36876 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
S

# # # # 
# 0$. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=2 a=6 o=2 a=4 o=0 a=5 o=1 a=2 o=0 
Starting simulation

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -2.58638
Total reward = -2.45706
MCTS Values:
a=0 : -2.45706 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 11 [11, 11]
Total reward: -2.45706 [-2.45706, -2.45706]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -2.45706 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
W

# # # # 
# 0$. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=2 a=6 o=2 a=4 o=0 a=5 o=1 a=2 o=0 a=3 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=1 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0$. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=6 o=2 a=6 o=2 a=4 o=0 a=5 o=1 a=2 o=0 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0$. #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = -1.93955, average = 5.47863
Undiscounted return = 0, average = 7.5
Starting run 5 with 1 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 19 steps, with total reward 3.97214
Total reward = 3.77354
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.77354 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 19 [19, 19]
Total reward: 3.77354 [3.77354, 3.77354]
Policy after 1 simulations
MCTS Policy:
a=2 : 3.77354 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.77354 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
S

# # # # 
# 0X1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.40127
MCTS Values:
a=0 : 0 (0)
a=1 : 4.40127 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 16 [16, 16]
Total reward: 4.40127 [4.40127, 4.40127]
Policy after 1 simulations
MCTS Policy:
a=1 : 4.40127 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 4.40127 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X1X#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 9.025 (1)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.025 [9.025, 9.025]
Policy after 1 simulations
MCTS Policy:
a=6 : 9.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 9.025 (1)
Check 2

# # # # 
# 0X1X#
# . * #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 
Starting simulation

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=5 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 0 (0)
Check 1

# # # # 
# 0X1X#
# . * #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=5 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 0 (0)
Check 1

# # # # 
# 0X1X#
# . * #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 25 steps, with total reward 3.6366
Total reward = 3.45477
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 3.45477 (1)
Tree depth: 0 [0, 0]
Rollout depth: 25 [25, 25]
Total reward: 3.45477 [3.45477, 3.45477]
Policy after 1 simulations
MCTS Policy:
a=6 : 3.45477 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 3.45477 (1)
Check 2

# # # # 
# 0X1X#
# . * #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=5 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Check 1

# # # # 
# 0X1X#
# . * #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=0 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
N

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 16.3025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 16.3025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 9 [9, 9]
Total reward: 16.3025 [16.3025, 16.3025]
Policy after 1 simulations
MCTS Policy:
a=4 : 16.3025 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 16.3025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# 0X* #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=3 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
W

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 a=3 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.025 [9.025, 9.025]
Policy after 1 simulations
MCTS Policy:
a=1 : 9.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 15.8025
Total reward = 15.0124
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 15.0124 (1)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 10 [10, 10]
Total reward: 15.0124 [15.0124, 15.0124]
Policy after 1 simulations
MCTS Policy:
a=5 : 15.0124 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 15.0124 (1)
a=6 : -1e+10 (1000000)
Check 1

# # # # 
# 0X* #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 a=3 o=0 a=1 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward -1.35494
Total reward = -1.28719
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1.28719 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: -1.28719 [-1.28719, -1.28719]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1.28719 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
S

# # # # 
# 0X. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 a=3 o=0 a=1 o=0 a=5 o=2 a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 6.6342 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 8 [8, 8]
Total reward: 6.6342 [6.6342, 6.6342]
Policy after 1 simulations
MCTS Policy:
a=3 : 6.6342 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 6.6342 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
W

# # # # 
# 0X. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 a=3 o=0 a=1 o=0 a=5 o=2 a=2 o=0 a=3 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=0 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 a=3 o=0 a=1 o=0 a=5 o=2 a=2 o=0 a=3 o=0 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 16.3025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.3025 (1)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 9 [9, 9]
Total reward: 16.3025 [16.3025, 16.3025]
Policy after 1 simulations
MCTS Policy:
a=4 : 16.3025 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.3025 (1)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 a=3 o=0 a=1 o=0 a=5 o=2 a=2 o=0 a=3 o=0 a=0 o=0 a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 10 [10, 10]
Total reward: 5.98737 [5.98737, 5.98737]
Policy after 1 simulations
MCTS Policy:
a=2 : 5.98737 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
S

# # # # 
# . . #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 a=3 o=0 a=1 o=0 a=5 o=2 a=2 o=0 a=3 o=0 a=0 o=0 a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=1 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 7.35092 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . . #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=6 o=2 a=5 o=2 a=5 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=4 o=0 a=3 o=0 a=1 o=0 a=5 o=2 a=2 o=0 a=3 o=0 a=0 o=0 a=4 o=0 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . . #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = -7.29497, average = 2.92391
Undiscounted return = -10, average = 4
Starting run 6 with 1 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 17.5987
Total reward = 16.7188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.7188 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 16.7188 [16.7188, 16.7188]
Policy after 1 simulations
MCTS Policy:
a=1 : 16.7188 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.7188 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 11.0355
Total reward = 10.4837
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 10.4837 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 17 [17, 17]
Total reward: 10.4837 [10.4837, 10.4837]
Policy after 1 simulations
MCTS Policy:
a=2 : 10.4837 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 10.4837 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
S

# # # # 
# 0$1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0$1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 9.025, average = 3.94076
Undiscounted return = 10, average = 5
Starting run 7 with 1 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 16.3759
Total reward = 15.5571
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 15.5571 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: 15.5571 [15.5571, 15.5571]
Policy after 1 simulations
MCTS Policy:
a=2 : 15.5571 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 15.5571 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
S

# # # # 
# 0X1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 5.688 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 11 [11, 11]
Total reward: 5.688 [5.688, 5.688]
Policy after 1 simulations
MCTS Policy:
a=5 : 5.688 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 5.688 (1)
a=6 : 0 (0)
Check 1

# # # # 
# 0X1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=2 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 8.14506 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=6 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 8.14506 (1)
Check 2

# # # # 
# 0X1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.35092 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=6 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.35092 (1)
Check 2

# # # # 
# 0X1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=2 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 7.73781 [7.73781, 7.73781]
Policy after 1 simulations
MCTS Policy:
a=1 : 7.73781 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X1X#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=2 a=6 o=2 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Check 2

# # # # 
# 0X1X#
# . * #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=2 o=0 a=5 o=2 a=6 o=2 a=6 o=2 a=1 o=0 a=6 o=2 
Starting simulation

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X1X#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 7.35092, average = 4.42792
Undiscounted return = 10, average = 5.71429
Starting run 8 with 1 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 14.9036
Total reward = 14.1584
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 14.1584 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 13 [13, 13]
Total reward: 14.1584 [14.1584, 14.1584]
Policy after 1 simulations
MCTS Policy:
a=5 : 14.1584 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 14.1584 (1)
a=6 : 0 (0)
Check 1

# # # # 
# * 1X#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 17.3509
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 17.3509 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 17.3509 [17.3509, 17.3509]
Policy after 1 simulations
MCTS Policy:
a=4 : 17.3509 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 17.3509 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# * 1X#
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 9 attempts
Expanding node: a=5 o=1 a=4 o=0 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 16 steps, with total reward -1.66958
Total reward = -1.5861
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.5861 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 16 [16, 16]
Total reward: -1.5861 [-1.5861, -1.5861]
Policy after 1 simulations
MCTS Policy:
a=2 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.5861 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=4 o=0 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 18 steps, with total reward -5.3188
Total reward = -5.05286
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -5.05286 (1)
Tree depth: 0 [0, 0]
Rollout depth: 18 [18, 18]
Total reward: -5.05286 [-5.05286, -5.05286]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -5.05286 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=4 o=0 a=6 o=2 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 14.6329
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 14.6329 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 15 [15, 15]
Total reward: 14.6329 [14.6329, 14.6329]
Policy after 1 simulations
MCTS Policy:
a=4 : 14.6329 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 14.6329 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=4 o=0 a=6 o=2 a=6 o=2 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.11311, average = 5.01357
Undiscounted return = 10, average = 6.25
Starting run 9 with 1 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=1 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.025 [9.025, 9.025]
Policy after 1 simulations
MCTS Policy:
a=5 : 9.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 0 (0)
Check 1

# # # # 
# 0X* #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=1 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 17.7378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 17.7378 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 17.7378 [17.7378, 17.7378]
Policy after 1 simulations
MCTS Policy:
a=4 : 17.7378 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 17.7378 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=2 a=4 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=5 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : -1e+10 (1000000)
Check 1

# # # # 
# 0X* #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=1 o=0 a=5 o=2 a=4 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 17.1701, average = 6.36429
Undiscounted return = 20, average = 7.77778
Starting run 10 with 1 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 19.025 [19.025, 19.025]
Policy after 1 simulations
MCTS Policy:
a=4 : 19.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# * 1$#
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 39 steps, with total reward -0.236876
Total reward = -0.225033
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.225033 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 39 [39, 39]
Total reward: -0.225033 [-0.225033, -0.225033]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.225033 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: 6.98337 [6.98337, 6.98337]
Policy after 1 simulations
MCTS Policy:
a=2 : 6.98337 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
S

# # # # 
# . 1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.025 [9.025, 9.025]
Policy after 1 simulations
MCTS Policy:
a=6 : 9.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Check 2

# # # # 
# . 1$#
# . * #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 a=2 o=0 a=6 o=1 
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 18.1451, average = 7.54237
Undiscounted return = 20, average = 9
Simulations = 1
Runs = 10
Undiscounted return = 9 +- 2.98329
Discounted return = 7.54237 +- 2.69946
Time = 0.0029612
Starting run 1 with 2 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 13.2067
Total reward = 12.5463
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 12.5463 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 13.5 [11, 16]
Total reward: 9.11717 [5.688, 12.5463]
Policy after 2 simulations
MCTS Policy:
a=2 : 12.5463 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 12.5463 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
S

# # # # 
# 0X1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [1, 3]
Total reward: 9.03688 [8.57375, 9.5]
Policy after 2 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 25 steps, with total reward 2.53261
Total reward = 2.40598
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 2.40598 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 25 [25, 25]
Total reward: 6.20299 [2.40598, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 2.40598 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 9.025, average = 9.025
Undiscounted return = 10, average = 10
Starting run 2 with 2 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 14.6329
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 14.6329 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 16.6342
Total reward = 15.8025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.8025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 14.6329 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 12 [9, 15]
Total reward: 15.2177 [14.6329, 15.8025]
Policy after 2 simulations
MCTS Policy:
a=1 : 15.8025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.8025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 14.6329 (1)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 11.1208
Total reward = 10.5648
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 10.5648 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 15.9247
Total reward = 25.1284
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 25.1284 (1)
a=5 : 10.5648 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 10.5 [7, 14]
Total reward: 17.8466 [10.5648, 25.1284]
Policy after 2 simulations
MCTS Policy:
a=4 : 25.1284 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 25.1284 (1)
a=5 : 10.5648 (1)
a=6 : 0 (0)
Sample

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 9.025 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward -1.81874
Total reward = -1.7278
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.7278 (1)
a=3 : 9.025 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 11.5 [2, 21]
Total reward: 3.6486 [-1.7278, 9.025]
Policy after 2 simulations
MCTS Policy:
a=3 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.7278 (1)
a=3 : 9.025 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
W

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=4 o=0 a=3 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = -2.64908
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 5 [4, 6]
Total reward: 2.74799 [-2.64908, 8.14506]
Policy after 2 simulations
MCTS Policy:
a=2 : 8.14506 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
S

# # # # 
# 0X. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=4 o=0 a=3 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 12.3222
Total reward = 11.7061
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 11.7061 (1)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 37 steps, with total reward 4.08123
Total reward = 3.87717
MCTS Values:
a=0 : 3.87717 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 11.7061 (1)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 24.5 [12, 37]
Total reward: 7.79163 [3.87717, 11.7061]
Policy after 2 simulations
MCTS Policy:
a=5 : 11.7061 (1)
Values after 2 simulations
MCTS Values:
a=0 : 3.87717 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 11.7061 (1)
a=6 : -1e+10 (1000000)
Check 1

# # # # 
# 0X. #
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=4 o=0 a=3 o=0 a=2 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 9.025 (1)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 3 [2, 4]
Total reward: 8.58503 [8.14506, 9.025]
Policy after 2 simulations
MCTS Policy:
a=0 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : 9.025 (1)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=4 o=0 a=3 o=0 a=2 o=0 a=5 o=2 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = -1.42625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 2.5 [2, 3]
Total reward: 3.79938 [-1.42625, 9.025]
Policy after 2 simulations
MCTS Policy:
a=2 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
S

# # # # 
# 0X. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=4 o=0 a=3 o=0 a=2 o=0 a=5 o=2 a=0 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.35092 (1)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 3.5 [1, 6]
Total reward: 8.42546 [7.35092, 9.5]
Policy after 2 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.35092 (1)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0X. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=4 o=0 a=3 o=0 a=2 o=0 a=5 o=2 a=0 o=0 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 9.28688 [8.57375, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0X. #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 16.1342, average = 12.5796
Undiscounted return = 20, average = 15
Starting run 3 with 2 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = -3.01663
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -3.01663 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.01663 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: 1.98337 [-3.01663, 6.98337]
Policy after 2 simulations
MCTS Policy:
a=2 : 6.98337 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.01663 (1)
a=5 : 0 (0)
a=6 : 0 (0)
S

# # # # 
# 0X1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 16.0084
Total reward = 15.208
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 15.208 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 15.208 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6.5 [5, 8]
Total reward: 11.4729 [7.73781, 15.208]
Policy after 2 simulations
MCTS Policy:
a=6 : 15.208 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 15.208 (1)
Check 2

# # # # 
# 0X1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=2 o=0 a=6 o=2 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 20 steps, with total reward 3.77354
Total reward = 3.58486
MCTS Values:
a=0 : 0 (0)
a=1 : 3.58486 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : 4.87675 (1)
a=1 : 3.58486 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 17 [14, 20]
Total reward: 4.2308 [3.58486, 4.87675]
Policy after 2 simulations
MCTS Policy:
a=0 : 4.87675 (1)
Values after 2 simulations
MCTS Values:
a=0 : 4.87675 (1)
a=1 : 3.58486 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
N

# # # # 
# * 1X#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward -2.74146
Total reward = -2.60439
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -2.60439 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 11 [9, 13]
Total reward: 1.84905 [-2.60439, 6.30249]
Policy after 2 simulations
MCTS Policy:
a=1 : 6.30249 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -2.60439 (1)
a=6 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1.5 [1, 2]
Total reward: 9.2625 [9.025, 9.5]
Policy after 2 simulations
MCTS Policy:
a=5 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : 0 (0)
Check 1

# # # # 
# 0X* #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 a=1 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 5.98737 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6.5 [3, 10]
Total reward: 7.28056 [5.98737, 8.57375]
Policy after 2 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 5.98737 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Check 2

# # # # 
# 0X* #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 a=1 o=0 a=5 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = -0.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [1, 3]
Total reward: 4.03688 [-0.5, 8.57375]
Policy after 2 simulations
MCTS Policy:
a=2 : 8.57375 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 0 (0)
a=6 : 0 (0)
S

# # # # 
# 0X1X#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 a=1 o=0 a=5 o=2 a=6 o=2 a=2 o=0 
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3.5 [3, 4]
Total reward: 8.35941 [8.14506, 8.57375]
Policy after 2 simulations
MCTS Policy:
a=3 : 8.57375 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 0 (0)
W

# # # # 
# 0X1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 a=1 o=0 a=5 o=2 a=6 o=2 a=2 o=0 a=3 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : 6.98337 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4.5 [2, 7]
Total reward: 8.00419 [6.98337, 9.025]
Policy after 2 simulations
MCTS Policy:
a=5 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : 6.98337 (1)
Check 1

# # # # 
# 0X1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 a=1 o=0 a=5 o=2 a=6 o=2 a=2 o=0 a=3 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 24 steps, with total reward 3.07357
Total reward = 2.91989
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 2.91989 (1)
a=6 : 6.6342 (1)
Tree depth: 0 [0, 0]
Rollout depth: 16 [8, 24]
Total reward: 4.77705 [2.91989, 6.6342]
Policy after 2 simulations
MCTS Policy:
a=6 : 6.6342 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 2.91989 (1)
a=6 : 6.6342 (1)
Check 2

# # # # 
# 0X1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 a=1 o=0 a=5 o=2 a=6 o=2 a=2 o=0 a=3 o=0 a=5 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 21 steps, with total reward -6.68978
Total reward = -6.35529
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -6.35529 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 11 [1, 21]
Total reward: 1.57236 [-6.35529, 9.5]
Policy after 2 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -6.35529 (1)
a=6 : 0 (0)
E

# # # # 
# 0X1X#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 a=1 o=0 a=5 o=2 a=6 o=2 a=2 o=0 a=3 o=0 a=5 o=2 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 21 steps, with total reward 3.58486
Total reward = 3.40562
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 3.40562 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 3.40562 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 11 [1, 21]
Total reward: 6.45281 [3.40562, 9.5]
Policy after 2 simulations
MCTS Policy:
a=5 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 3.40562 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
a=6 : 0 (0)
Check 1

# # # # 
# 0X1X#
# . * #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 a=1 o=0 a=5 o=2 a=6 o=2 a=2 o=0 a=3 o=0 a=5 o=2 a=6 o=2 a=1 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -10.3885
Total reward = -9.86912
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : -9.86912 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 11 [11, 11]
Total reward: 0.065439 [-9.86912, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : -9.86912 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X1X#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 5.4036, average = 10.1876
Undiscounted return = 10, average = 13.3333
Starting run 4 with 2 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 26 steps, with total reward -6.2511
Total reward = -5.93855
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -5.93855 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 39 steps, with total reward 6.83776
Total reward = 6.49587
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.49587 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -5.93855 (1)
Tree depth: 0 [0, 0]
Rollout depth: 32.5 [26, 39]
Total reward: 0.278662 [-5.93855, 6.49587]
Policy after 2 simulations
MCTS Policy:
a=2 : 6.49587 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.49587 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -5.93855 (1)
S

# # # # 
# 0X1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward -4.17248
Total reward = -3.96386
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -3.96386 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -3.96386 (1)
a=6 : 6.30249 (1)
Tree depth: 0 [0, 0]
Rollout depth: 13 [9, 17]
Total reward: 1.16932 [-3.96386, 6.30249]
Policy after 2 simulations
MCTS Policy:
a=6 : 6.30249 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -3.96386 (1)
a=6 : 6.30249 (1)
Check 2

# # # # 
# 0X1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 3.74068
Total reward = 3.55365
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 3.55365 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 35 steps, with total reward 5.21473
Total reward = 4.954
MCTS Values:
a=0 : 0 (0)
a=1 : 4.954 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 3.55365 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 24 [13, 35]
Total reward: 4.25382 [3.55365, 4.954]
Policy after 2 simulations
MCTS Policy:
a=1 : 4.954 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 4.954 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 3.55365 (1)
a=6 : 0 (0)
E

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 16 steps, with total reward 10.9354
Total reward = 10.3886
MCTS Values:
a=0 : 10.3886 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 10.8154
Total reward = 10.2746
MCTS Values:
a=0 : 10.3886 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 10.2746 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 17 [16, 18]
Total reward: 10.3316 [10.2746, 10.3886]
Policy after 2 simulations
MCTS Policy:
a=0 : 10.3886 (1)
Values after 2 simulations
MCTS Values:
a=0 : 10.3886 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 10.2746 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
N

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=1 o=0 a=0 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -0.794144
Total reward = -0.754436
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -0.754436 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7.5 [7, 8]
Total reward: 2.93988 [-0.754436, 6.6342]
Policy after 2 simulations
MCTS Policy:
a=2 : 6.6342 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -0.754436 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
S

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=1 o=0 a=0 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 9.5 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.75 [9.5, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 9.5 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 7.73781, average = 9.57515
Undiscounted return = 10, average = 12.5
Starting run 5 with 2 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 36 steps, with total reward -3.47259
Total reward = -3.29896
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -3.29896 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -3.29896 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 6.30249 (1)
Tree depth: 0 [0, 0]
Rollout depth: 22.5 [9, 36]
Total reward: 1.50177 [-3.29896, 6.30249]
Policy after 2 simulations
MCTS Policy:
a=6 : 6.30249 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -3.29896 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 6.30249 (1)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 24 steps, with total reward -2.9138
Total reward = 7.23189
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 7.23189 (1)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 13.5 [3, 24]
Total reward: 7.90282 [7.23189, 8.57375]
Policy after 2 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 7.23189 (1)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward -1.00233
Total reward = -0.952217
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.952217 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward -0.367546
Total reward = -10.3492
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.952217 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.3492 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 12.5 [8, 17]
Total reward: -5.65069 [-10.3492, -0.952217]
Policy after 2 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.952217 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.3492 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Check 1

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=6 o=2 a=6 o=2 a=5 o=2 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 31 steps, with total reward 1.63185
Total reward = 1.55025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 1.55025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 28 steps, with total reward 2.50344
Total reward = 2.37827
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 1.55025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 2.37827 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 29.5 [28, 31]
Total reward: 1.96426 [1.55025, 2.37827]
Policy after 2 simulations
MCTS Policy:
a=5 : 2.37827 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 1.55025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 2.37827 (1)
a=6 : 0 (0)
Check 1

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=6 o=2 a=6 o=2 a=5 o=2 a=5 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -2.8658
Total reward = -2.72251
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -2.72251 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -0.38689
Total reward = -0.367546
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -2.72251 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -0.367546 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8 [7, 9]
Total reward: -1.54503 [-2.72251, -0.367546]
Policy after 2 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -2.72251 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -0.367546 (1)
Sample

# # # # 
# * 1X#
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=6 o=2 a=5 o=2 a=5 o=2 a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward -1.16169
Total reward = -1.10361
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.10361 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.10361 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6.5 [5, 8]
Total reward: 3.3171 [-1.10361, 7.73781]
Policy after 2 simulations
MCTS Policy:
a=1 : 7.73781 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.10361 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=6 o=2 a=5 o=2 a=5 o=2 a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = -0.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 4.75 [-0.5, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = -0.794144, average = 7.50129
Undiscounted return = 0, average = 10
Starting run 6 with 2 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 17.3778
Total reward = 16.5089
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.5089 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 52 steps, with total reward 8.46879
Total reward = 8.04535
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.5089 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.04535 (1)
Tree depth: 0 [0, 0]
Rollout depth: 37 [22, 52]
Total reward: 12.2771 [8.04535, 16.5089]
Policy after 2 simulations
MCTS Policy:
a=1 : 16.5089 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.5089 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.04535 (1)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -0.716715
Total reward = -0.680879
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -0.680879 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : -0.680879 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [1, 9]
Total reward: 4.40956 [-0.680879, 9.5]
Policy after 2 simulations
MCTS Policy:
a=2 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : -0.680879 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
S

# # # # 
# 0X1X#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 9.07253 [8.14506, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X1X#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 9.025, average = 7.75525
Undiscounted return = 10, average = 10
Starting run 7 with 2 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 17.3509
Total reward = 16.4834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.4834 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.4834 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [1, 7]
Total reward: 12.9917 [9.5, 16.4834]
Policy after 2 simulations
MCTS Policy:
a=5 : 16.4834 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.4834 (1)
a=6 : 0 (0)
Check 1

# # # # 
# * 1X#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 13.8331
Total reward = 13.1414
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 13.1414 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11.5 [11, 12]
Total reward: 9.41471 [5.688, 13.1414]
Policy after 2 simulations
MCTS Policy:
a=6 : 13.1414 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 13.1414 (1)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward -1.75044
Total reward = -11.6629
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -11.6629 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 14 steps, with total reward -12.6044
Total reward = -11.9742
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -11.6629 (1)
a=5 : 0 (0)
a=6 : -11.9742 (1)
Tree depth: 0 [0, 0]
Rollout depth: 12.5 [11, 14]
Total reward: -11.8185 [-11.9742, -11.6629]
Policy after 2 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -11.6629 (1)
a=5 : 0 (0)
a=6 : -11.9742 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 20 steps, with total reward 9.76091
Total reward = 9.27286
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.27286 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.27286 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 20 [20, 20]
Total reward: 9.63643 [9.27286, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.27286 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 8.57375, average = 7.87217
Undiscounted return = 10, average = 10
Starting run 8 with 2 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 18 steps, with total reward -0.952217
Total reward = -10.9046
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.9046 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.9046 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 9.5 [1, 18]
Total reward: -0.702303 [-10.9046, 9.5]
Policy after 2 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.9046 (1)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 35 steps, with total reward 7.90005
Total reward = 7.50505
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 7.50505 (1)
Tree depth: 0 [0, 0]
Rollout depth: 18.5 [2, 35]
Total reward: 8.26502 [7.50505, 9.025]
Policy after 2 simulations
MCTS Policy:
a=2 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 7.50505 (1)
S

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward -4.62325
Total reward = -4.39209
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -4.39209 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 9.5 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -4.39209 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 8 [1, 15]
Total reward: 2.55396 [-4.39209, 9.5]
Policy after 2 simulations
MCTS Policy:
a=0 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : 9.5 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -4.39209 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
N

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 19.5 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 12.0378
Total reward = 11.4359
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 11.4359 (1)
a=3 : 0 (0)
a=4 : 19.5 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [1, 13]
Total reward: 15.468 [11.4359, 19.5]
Policy after 2 simulations
MCTS Policy:
a=4 : 19.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 11.4359 (1)
a=3 : 0 (0)
a=4 : 19.5 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=2 o=0 a=0 o=0 a=4 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 4.5 [2, 7]
Total reward: 8.00419 [6.98337, 9.025]
Policy after 2 simulations
MCTS Policy:
a=5 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : -1e+10 (1000000)
Check 1

# # # # 
# 0X* #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=1 o=0 a=2 o=0 a=0 o=0 a=4 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 6.6342 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 8 [8, 8]
Total reward: 8.3171 [6.6342, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 6.6342 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 16.3116, average = 8.9271
Undiscounted return = 20, average = 11.25
Starting run 9 with 2 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 16.3116
Total reward = 15.496
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 15.496 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 13.6176
Total reward = 2.9367
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 15.496 (1)
a=3 : -1e+10 (1000000)
a=4 : 2.9367 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7.5 [6, 9]
Total reward: 9.21634 [2.9367, 15.496]
Policy after 2 simulations
MCTS Policy:
a=2 : 15.496 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 15.496 (1)
a=3 : -1e+10 (1000000)
a=4 : 2.9367 (1)
a=5 : 0 (0)
a=6 : 0 (0)
S

# # # # 
# 0$1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 15.8025
Total reward = 15.0124
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 15.0124 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7 [4, 10]
Total reward: 11.5787 [8.14506, 15.0124]
Policy after 2 simulations
MCTS Policy:
a=6 : 15.0124 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 15.0124 (1)
Check 2

# # # # 
# 0$1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=2 o=0 a=6 o=1 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 16.7188
Total reward = 15.8829
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 15.8829 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward -4.0964
Total reward = -3.89158
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 15.8829 (1)
a=6 : -3.89158 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9 [5, 13]
Total reward: 5.99565 [-3.89158, 15.8829]
Policy after 2 simulations
MCTS Policy:
a=5 : 15.8829 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 15.8829 (1)
a=6 : -3.89158 (1)
Check 1

# # # # 
# 0$1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 20 steps, with total reward 17.4269
Total reward = 16.5556
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 16.5556 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 16.5556 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 12 [4, 20]
Total reward: 12.3503 [8.14506, 16.5556]
Policy after 2 simulations
MCTS Policy:
a=5 : 16.5556 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 16.5556 (1)
a=6 : 0 (0)
Check 1

# # # # 
# 0$1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 8.2062
Total reward = 7.79589
MCTS Values:
a=0 : 7.79589 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward 8.71828
Total reward = 8.28237
MCTS Values:
a=0 : 7.79589 (1)
a=1 : 8.28237 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 14.5 [8, 21]
Total reward: 8.03913 [7.79589, 8.28237]
Policy after 2 simulations
MCTS Policy:
a=1 : 8.28237 (1)
Values after 2 simulations
MCTS Values:
a=0 : 7.79589 (1)
a=1 : 8.28237 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0$1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -2.15769
Total reward = -2.04981
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -2.04981 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -2.04981 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8 [5, 11]
Total reward: 2.844 [-2.04981, 7.73781]
Policy after 2 simulations
MCTS Policy:
a=6 : 7.73781 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -2.04981 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.73781 (1)
Check 2

# # # # 
# 0$1$#
# . * #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 9.5 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -0.879938
Total reward = -0.835941
MCTS Values:
a=0 : 9.5 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -0.835941 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [1, 5]
Total reward: 4.33203 [-0.835941, 9.5]
Policy after 2 simulations
MCTS Policy:
a=0 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : 9.5 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -0.835941 (1)
a=6 : 0 (0)
N

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 a=0 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = -1.42625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : -1.42625 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4.5 [3, 6]
Total reward: 2.96233 [-1.42625, 7.35092]
Policy after 2 simulations
MCTS Policy:
a=3 : 7.35092 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : -1.42625 (1)
a=5 : 0 (0)
a=6 : 0 (0)
W

# # # # 
# * 1$#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 a=0 o=0 a=3 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -1.04842
Total reward = -0.996004
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.996004 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 9 [8, 10]
Total reward: 2.8191 [-0.996004, 6.6342]
Policy after 2 simulations
MCTS Policy:
a=2 : 6.6342 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.996004 (1)
a=6 : 0 (0)
S

# # # # 
# 0$1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 a=0 o=0 a=3 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 39 steps, with total reward 8.77488
Total reward = 8.33613
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.33613 (1)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 21 [3, 39]
Total reward: 8.45494 [8.33613, 8.57375]
Policy after 2 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.33613 (1)
a=6 : 8.57375 (1)
Check 2

# # # # 
# 0$1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 a=0 o=0 a=3 o=0 a=2 o=0 a=6 o=1 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward -1.42625
Total reward = -1.35494
MCTS Values:
a=0 : -1.35494 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4.5 [4, 5]
Total reward: 3.19144 [-1.35494, 7.73781]
Policy after 2 simulations
MCTS Policy:
a=5 : 7.73781 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1.35494 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 0 (0)
Check 1

# # # # 
# 0$1$#
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 a=0 o=0 a=3 o=0 a=2 o=0 a=6 o=1 a=5 o=1 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 11.7061
Total reward = 11.1208
MCTS Values:
a=0 : 0 (0)
a=1 : 11.1208 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 11.1208 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 8.5 [4, 13]
Total reward: 9.63293 [8.14506, 11.1208]
Policy after 2 simulations
MCTS Policy:
a=1 : 11.1208 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 11.1208 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 0 (0)
E

# # # # 
# 0$1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 a=0 o=0 a=3 o=0 a=2 o=0 a=6 o=1 a=5 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 6.98337 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 6.98337 (1)
a=6 : 6.30249 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8 [7, 9]
Total reward: 6.64293 [6.30249, 6.98337]
Policy after 2 simulations
MCTS Policy:
a=5 : 6.98337 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 6.98337 (1)
a=6 : 6.30249 (1)
Check 1

# # # # 
# 0$1$#
# . * #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 a=0 o=0 a=3 o=0 a=2 o=0 a=6 o=1 a=5 o=1 a=1 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 38 steps, with total reward 10.0727
Total reward = 9.56902
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 9.56902 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 19.5 [1, 38]
Total reward: 9.53451 [9.5, 9.56902]
Policy after 2 simulations
MCTS Policy:
a=3 : 9.56902 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 9.56902 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 9.5 (1)
W

# # # # 
# 0$1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 a=0 o=0 a=3 o=0 a=2 o=0 a=6 o=1 a=5 o=1 a=1 o=0 a=5 o=1 a=3 o=0 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward -0.975
Total reward = -0.92625
MCTS Values:
a=0 : -0.92625 (1)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3.5 [3, 4]
Total reward: 3.60941 [-0.92625, 8.14506]
Policy after 2 simulations
MCTS Policy:
a=1 : 8.14506 (1)
Values after 2 simulations
MCTS Values:
a=0 : -0.92625 (1)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0$1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 a=5 o=1 a=1 o=0 a=6 o=1 a=0 o=0 a=3 o=0 a=2 o=0 a=6 o=1 a=5 o=1 a=1 o=0 a=5 o=1 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 4.1812
Total reward = 3.97214
MCTS Values:
a=0 : 3.97214 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 18 [18, 18]
Total reward: 6.98607 [3.97214, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 3.97214 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
E

# # # # 
# 0$1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 4.63291, average = 8.44997
Undiscounted return = 10, average = 11.1111
Starting run 10 with 2 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.14506 (1)
a=6 : 9.025 (1)
Tree depth: 0 [0, 0]
Rollout depth: 3 [2, 4]
Total reward: 8.58503 [8.14506, 9.025]
Policy after 2 simulations
MCTS Policy:
a=6 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.14506 (1)
a=6 : 9.025 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 13.5738 [8.57375, 18.5738]
Policy after 2 simulations
MCTS Policy:
a=4 : 18.5738 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# * 1$#
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 2 simulations
MCTS Policy:
a=1 : 8.57375 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
S

# # # # 
# . 1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward -0.299368
Total reward = -0.2844
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.2844 (1)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 29 steps, with total reward -6.64673
Total reward = -6.31439
MCTS Values:
a=0 : -6.31439 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.2844 (1)
Tree depth: 0 [0, 0]
Rollout depth: 20.5 [12, 29]
Total reward: -3.2994 [-6.31439, -0.2844]
Policy after 2 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : -6.31439 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.2844 (1)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 8.67546 [7.35092, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
E

# # # # 
# . 1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 17.6451, average = 9.36948
Undiscounted return = 20, average = 12
Simulations = 2
Runs = 10
Undiscounted return = 12 +- 1.89737
Discounted return = 9.36948 +- 1.75338
Time = 0.0038508
Starting run 1 with 4 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = -2.64908
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 19.025
Total reward = 18.0737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
a=6 : 18.0737 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 16.4834
Total reward = 15.6592
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 15.6592 (1)
a=6 : 18.0737 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5.75 [3, 8]
Total reward: 9.6087 [-2.64908, 18.0737]
Policy after 4 simulations
MCTS Policy:
a=6 : 18.0737 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 15.6592 (1)
a=6 : 18.0737 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 12 attempts
Expanding node: a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward -0.946203
Total reward = -0.898893
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.898893 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 20 steps, with total reward 8.16562
Total reward = 7.75734
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.898893 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.75734 (1)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = -4.01263
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.898893 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : 7.75734 (1)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11.25 [3, 20]
Total reward: 2.85489 [-4.01263, 8.57375]
Policy after 4 simulations
MCTS Policy:
a=6 : 8.57375 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.898893 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : 7.75734 (1)
a=6 : 8.57375 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=6 o=1 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = 4.1812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.1812 (1)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 19 steps, with total reward 17.6533
Total reward = 16.7707
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.1812 (1)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.7707 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 13.2859
Total reward = 12.6216
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.1812 (1)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.7707 (1)
a=6 : 12.6216 (1)
Tree depth: 0 [0, 0]
Rollout depth: 13.75 [9, 19]
Total reward: 9.96899 [4.1812, 16.7707]
Policy after 4 simulations
MCTS Policy:
a=5 : 16.7707 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.1812 (1)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.7707 (1)
a=6 : 12.6216 (1)
Check 1

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=6 o=1 a=5 o=1 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 16.0084
Total reward = 15.208
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 15.208 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 15.208 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 16.9834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 0 (0)
a=6 : 15.208 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 23 steps, with total reward 2.38139
Total reward = 2.26232
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 2.26232 (1)
a=6 : 15.208 (1)
Tree depth: 0 [0, 0]
Rollout depth: 10 [2, 23]
Total reward: 10.8697 [2.26232, 16.9834]
Policy after 4 simulations
MCTS Policy:
a=4 : 16.9834 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 2.26232 (1)
a=6 : 15.208 (1)
Sample

# # # # 
# * 1$#
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=6 o=1 a=5 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 12.1391
Total reward = 11.5321
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 11.5321 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=6 o=1 a=6 o=1 a=5 o=1 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 9.83859 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 7.5 [1, 17]
Total reward: 8.86992 [6.30249, 11.5321]
Policy after 4 simulations
MCTS Policy:
a=2 : 9.83859 (2)
a=2 o=0 a=6 : 8.57375 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 9.83859 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.30249 (1)
S

# # # # 
# . 1$#
# * . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=6 o=1 a=5 o=1 a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 4.1812
Total reward = 3.97214
MCTS Values:
a=0 : 0 (0)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=6 o=1 a=6 o=1 a=5 o=1 a=4 o=0 a=2 o=0 a=0 o=0 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 12.975
Total reward = 11.71
MCTS Values:
a=0 : 10.1419 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 12.3263 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 0 (0)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 12 [3, 18]
Total reward: 7.5608 [3.97214, 11.71]
Policy after 4 simulations
MCTS Policy:
a=0 : 10.1419 (2)
a=0 o=0 a=2 : 12.3263 (1)
Values after 4 simulations
MCTS Values:
a=0 : 10.1419 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 12.3263 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 0 (0)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
N

# # # # 
# * 1$#
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=6 o=1 a=5 o=1 a=4 o=0 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 14.1584
Total reward = 13.4505
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 13.4505 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 13.4505 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=6 o=1 a=6 o=1 a=5 o=1 a=4 o=0 a=2 o=0 a=0 o=0 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.29196 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 5.4036 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 7.75 [1, 14]
Total reward: 9.05725 [5.13342, 13.4505]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.29196 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 5.4036 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=6 o=1 a=5 o=1 a=4 o=0 a=2 o=0 a=0 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 8.14506 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 8.14506 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 8.14506 (1)
a=4 : 19.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Tree depth: 0 [0, 0]
Rollout depth: 2 [1, 4]
Total reward: 11.5425 [8.14506, 19.5]
Policy after 4 simulations
MCTS Policy:
a=4 : 19.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 8.14506 (1)
a=4 : 19.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=6 o=1 a=5 o=1 a=4 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 4.87675 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 5.98737 (1)
a=3 : 4.87675 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 5.98737 (1)
a=3 : 4.87675 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 12 [10, 14]
Total reward: 7.71603 [4.87675, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 5.98737 (1)
a=3 : 4.87675 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 22.1913, average = 22.1913
Undiscounted return = 30, average = 30
Starting run 2 with 4 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 16.0084
Total reward = 15.208
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 15.208 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 12.1391
Total reward = 11.5321
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 11.5321 (1)
a=6 : 15.208 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 4.1812
Total reward = 3.97214
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 11.5321 (1)
a=6 : 15.208 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = -3.69751
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.69751 (1)
a=5 : 11.5321 (1)
a=6 : 15.208 (1)
Tree depth: 0 [0, 0]
Rollout depth: 13 [8, 18]
Total reward: 6.75368 [-3.69751, 15.208]
Policy after 4 simulations
MCTS Policy:
a=6 : 15.208 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.69751 (1)
a=5 : 11.5321 (1)
a=6 : 15.208 (1)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward -0.835941
Total reward = 9.20586
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 9.20586 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 9.20586 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 9.20586 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 9.20586 (1)
a=5 : 6.6342 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5.5 [1, 8]
Total reward: 8.08086 [6.6342, 9.5]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 9.20586 (1)
a=5 : 6.6342 (1)
a=6 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 10.3209
Total reward = 9.80487
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 9.80487 (1)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7 [2, 16]
Total reward: 9.3509 [8.57375, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 9.80487 (1)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 8.57375 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.025, average = 15.6082
Undiscounted return = 10, average = 20
Starting run 3 with 4 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 19 steps, with total reward 3.97214
Total reward = 3.77354
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 3.77354 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 3.77354 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 23.7252
Total reward = 22.5389
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 22.5389 (1)
a=6 : 3.77354 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 13.6812
Total reward = 12.9971
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.9971 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 22.5389 (1)
a=6 : 3.77354 (1)
Tree depth: 0 [0, 0]
Rollout depth: 13 [4, 19]
Total reward: 11.8637 [3.77354, 22.5389]
Policy after 4 simulations
MCTS Policy:
a=5 : 22.5389 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.9971 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 22.5389 (1)
a=6 : 3.77354 (1)
Check 1

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 5.4036 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 5.4036 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 12.7545
Total reward = 12.1168
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 12.1168 (1)
a=6 : 5.4036 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 7.80306
Total reward = 7.41291
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 7.41291 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 12.1168 (1)
a=6 : 5.4036 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11 [3, 16]
Total reward: 8.37676 [5.4036, 12.1168]
Policy after 4 simulations
MCTS Policy:
a=5 : 12.1168 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 7.41291 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 12.1168 (1)
a=6 : 5.4036 (1)
Check 1

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=5 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 18.1451
Total reward = 17.2378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 17.2378 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 15.6592
Total reward = 14.8762
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.8762 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 17.2378 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 16.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.8762 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : 0 (0)
a=6 : 17.2378 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.8762 (1)
a=2 : 7.35092 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : 0 (0)
a=6 : 17.2378 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7 [5, 9]
Total reward: 14.0248 [7.35092, 17.2378]
Policy after 4 simulations
MCTS Policy:
a=6 : 17.2378 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.8762 (1)
a=2 : 7.35092 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : 0 (0)
a=6 : 17.2378 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 a=5 o=1 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 7.59875
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 7.59875 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 8.33093
Total reward = 7.91438
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 7.59875 (1)
a=5 : 0 (0)
a=6 : 7.91438 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 33 steps, with total reward 1.93711
Total reward = 1.84026
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 1.84026 (1)
a=3 : -1e+10 (1000000)
a=4 : 7.59875 (1)
a=5 : 0 (0)
a=6 : 7.91438 (1)
Tree depth: 0 [0, 0]
Rollout depth: 12.75 [1, 33]
Total reward: 6.71335 [1.84026, 9.5]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 1.84026 (1)
a=3 : -1e+10 (1000000)
a=4 : 7.59875 (1)
a=5 : 0 (0)
a=6 : 7.91438 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=5 o=1 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 25 steps, with total reward -5.65386
Total reward = -5.37117
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -5.37117 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = -0.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -5.37117 (1)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -5.37117 (1)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9.66667 [1, 25]
Total reward: 3.17565 [-5.37117, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -5.37117 (1)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 0 (0)
a=6 : 8.57375 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 8.14506, average = 13.1205
Undiscounted return = 10, average = 16.6667
Starting run 4 with 4 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -1.75044
Total reward = -1.66292
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.66292 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward -0.475
Total reward = -0.45125
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.66292 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.45125 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.66292 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.45125 (1)
a=6 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6.5 [3, 11]
Total reward: 3.15175 [-1.66292, 7.73781]
Policy after 4 simulations
MCTS Policy:
a=6 : 7.73781 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.66292 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.45125 (1)
a=6 : 7.73781 (1)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 4.87675 (1)
a=6 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7 [3, 14]
Total reward: 9.63481 [4.87675, 18.5738]
Policy after 4 simulations
MCTS Policy:
a=4 : 18.5738 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 4.87675 (1)
a=6 : 7.73781 (1)
Sample

# # # # 
# * 1X#
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.688 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 16.7628
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.688 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 16.7628 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 14.7212
Total reward = 13.9851
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.9851 (1)
a=2 : 5.688 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 16.7628 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 16.7628
Total reward = 15.1284
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.9851 (1)
a=2 : 5.688 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 15.9456 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 15.9247 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 7.5 [5, 11]
Total reward: 12.8911 [5.688, 16.7628]
Policy after 4 simulations
MCTS Policy:
a=6 : 15.9456 (2)
a=6 o=1 a=2 : 15.9247 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.9851 (1)
a=2 : 5.688 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 15.9456 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 15.9247 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=6 o=1 a=4 o=0 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward -0.5
Total reward = -0.475
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.475 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.475 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.475 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=2 a=2 o=0 
Adding sample:

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.475 (1)
a=2 : 8.15578 (2)
a=2 o=0 a=0 : 9.025 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 3.5 [2, 5]
Total reward: 5.89359 [-0.475, 8.57375]
Policy after 4 simulations
MCTS Policy:
a=2 : 8.15578 (2)
a=2 o=0 a=0 : 9.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.475 (1)
a=2 : 8.15578 (2)
a=2 o=0 a=0 : 9.025 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.73781 (1)
S

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=6 o=2 a=2 o=0 
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : 0 (0)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -0.349169
Total reward = -0.33171
MCTS Values:
a=0 : 0 (0)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.33171 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 9.025 (1)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.33171 (1)
Starting simulation

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=2 a=2 o=0 a=0 o=0 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : 8.79938 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 9.025 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 0 (0)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.33171 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 5 [2, 9]
Total reward: 6.0626 [-0.33171, 9.025]
Policy after 4 simulations
MCTS Policy:
a=0 : 8.79938 (2)
a=0 o=0 a=2 : 9.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : 8.79938 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 9.025 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=6 : 0 (0)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -0.33171 (1)
N

# # # # 
# * 1X#
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=6 o=2 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 17.5987
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 21 steps, with total reward -0.387284
Total reward = -0.36792
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : -0.36792 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.40127
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : -0.36792 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 4.63291 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 12.25 [3, 21]
Total reward: 6.90487 [-0.36792, 17.5987]
Policy after 4 simulations
MCTS Policy:
a=6 : 11 (2)
a=6 o=2 a=2 : 4.63291 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : -0.36792 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 4.63291 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
Matched 1 states
Created 1 local transformations out of 6 attempts
Expanding node: a=6 o=1 a=4 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=6 o=1 a=4 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 4.33333 [1, 8]
Total reward: 8.44482 [6.6342, 9.5]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.5 (2)
a=1 o=0 a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=4 o=0 a=6 o=2 a=2 o=0 a=0 o=0 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 7.73781 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 7.73781 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 7.73781 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4.66667 [1, 8]
Total reward: 8.468 [6.6342, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 7.73781 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 6.6342 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 16.4834, average = 13.9612
Undiscounted return = 20, average = 17.5
Starting run 5 with 4 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward -2.04163
Total reward = -1.93955
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.93955 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -0.680879
Total reward = -0.646835
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.93955 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -0.646835 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.93955 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.14506 (1)
a=6 : -0.646835 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -0.349169
Total reward = 9.66829
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.93955 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 9.66829 (1)
a=5 : 8.14506 (1)
a=6 : -0.646835 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7.75 [4, 10]
Total reward: 3.80674 [-1.93955, 9.66829]
Policy after 4 simulations
MCTS Policy:
a=4 : 9.66829 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.93955 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 9.66829 (1)
a=5 : 8.14506 (1)
a=6 : -0.646835 (1)
Sample

# # # # 
# * 1X#
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.63291
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 14.8762
Total reward = 14.1324
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 14.1324 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 12.2899
Total reward = 11.0916
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 12.612 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 11.6754 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 10 [4, 15]
Total reward: 9.5005 [4.63291, 14.1324]
Policy after 4 simulations
MCTS Policy:
a=6 : 12.612 (2)
a=6 o=1 a=1 : 11.6754 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 12.612 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 11.6754 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 5 attempts
Expanding node: a=4 o=0 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -1.10361
Total reward = -1.04842
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.04842 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 17 steps, with total reward -2.23294
Total reward = -2.12129
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.12129 (1)
a=2 : -1.04842 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.63291
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.12129 (1)
a=2 : -1.04842 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 4.63291 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 17 steps, with total reward -1.00233
Total reward = -0.904607
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.12129 (1)
a=2 : -1.04842 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 1.86415 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -0.952217 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 14.5 [9, 17]
Total reward: 0.139648 [-2.12129, 4.63291]
Policy after 4 simulations
MCTS Policy:
a=6 : 1.86415 (2)
a=6 o=2 a=1 : 0 (0)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.12129 (1)
a=2 : -1.04842 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 1.86415 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -0.952217 (1)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
Matched 1 states
Created 1 local transformations out of 6 attempts
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 27 steps, with total reward -3.999
Total reward = -3.79905
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -3.79905 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -3.79905 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -3.79905 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = -1.35494
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.60941 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1.42625 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -3.79905 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 10 [3, 27]
Total reward: 2.60078 [-3.79905, 8.57375]
Policy after 4 simulations
MCTS Policy:
a=2 : 6.98337 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.60941 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1.42625 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -3.79905 (1)
S

# # # # 
# . 1X#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 a=2 o=0 
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 21 steps, with total reward 12.6099
Total reward = 11.9794
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 11.9794 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 11.9794 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
Starting simulation

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 a=2 o=0 a=1 o=0 
Adding sample:

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 10.2766 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 9 [2, 21]
Total reward: 8.77856 [5.98737, 11.9794]
Policy after 4 simulations
MCTS Policy:
a=1 : 10.2766 (2)
a=1 o=0 a=6 : 9.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 10.2766 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.98737 (1)
E

# # # # 
# . 1X#
# . * #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 24 steps, with total reward 3.07357
Total reward = 2.91989
MCTS Values:
a=0 : 2.91989 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : 2.91989 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 2.91989 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 2.91989 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 10.6667 [1, 24]
Total reward: 7.35082 [2.91989, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : 2.91989 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
E

# # # # 
# . 1X#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 17.7378, average = 14.7165
Undiscounted return = 20, average = 18
Starting run 6 with 4 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward -0.92625
Total reward = -0.879938
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.879938 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 16.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : -0.879938 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 18.1451
Total reward = 17.2378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.2378 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : -0.879938 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.2378 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : -0.879938 (1)
a=6 : 5.4036 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7.25 [4, 12]
Total reward: 9.59892 [-0.879938, 17.2378]
Policy after 4 simulations
MCTS Policy:
a=1 : 17.2378 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.2378 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : -0.879938 (1)
a=6 : 5.4036 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : 7.35092 (1)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward -0.5
Total reward = -0.475
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : 7.35092 (1)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : -0.475 (1)
Tree depth: 0 [0, 0]
Rollout depth: 3 [1, 6]
Total reward: 6.23742 [-0.475, 9.5]
Policy after 4 simulations
MCTS Policy:
a=5 : 9.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : 7.35092 (1)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : -0.475 (1)
Check 1

# # # # 
# 0$* #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 5 attempts
Expanding node: a=1 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 15.5571
Total reward = 14.7793
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 14.7793 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 14.7793 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 20 steps, with total reward 3.77354
Total reward = 3.58486
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 3.58486 (1)
a=3 : 14.7793 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 3.58486 (1)
a=3 : 14.7793 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9.66667 [1, 20]
Total reward: 9.46603 [3.58486, 14.7793]
Policy after 4 simulations
MCTS Policy:
a=3 : 14.7793 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 3.58486 (1)
a=3 : 14.7793 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 9.5 (1)
W

# # # # 
# * 1X#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=3 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 6.48836
Total reward = 6.16394
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.16394 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.16394 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 9.16829
Total reward = 8.70988
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.16394 (1)
a=6 : 8.70988 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 26 steps, with total reward 6.35875
Total reward = 6.04082
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 6.04082 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.16394 (1)
a=6 : 8.70988 (1)
Tree depth: 0 [0, 0]
Rollout depth: 13 [2, 26]
Total reward: 7.48491 [6.04082, 9.025]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 6.04082 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.16394 (1)
a=6 : 8.70988 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 16.6342
Total reward = 15.8025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 15.8025 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 13.5487
Total reward = 12.8712
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 15.8025 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 12.8712 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 15.8025 (1)
a=4 : 0 (0)
a=5 : 8.57375 (1)
a=6 : 12.8712 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 15.8025 (1)
a=4 : 0 (0)
a=5 : 8.57375 (1)
a=6 : 12.8712 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8.33333 [3, 13]
Total reward: 11.8119 [8.57375, 15.8025]
Policy after 4 simulations
MCTS Policy:
a=3 : 15.8025 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 15.8025 (1)
a=4 : 0 (0)
a=5 : 8.57375 (1)
a=6 : 12.8712 (1)
W

# # # # 
# * 1X#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=3 o=0 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 15.9247
Total reward = 15.1284
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 15.1284 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 15.1284 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 15.1284 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 13.4056
Total reward = 12.7353
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 15.1284 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 12.7353 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8.75 [3, 22]
Total reward: 13.7528 [8.57375, 18.5738]
Policy after 4 simulations
MCTS Policy:
a=4 : 18.5738 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 15.1284 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
a=6 : 12.7353 (1)
Sample

# # # # 
# * 1X#
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=3 o=0 a=1 o=0 a=3 o=0 a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.13342 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.13342 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=1 o=0 a=5 o=1 a=3 o=0 a=1 o=0 a=3 o=0 a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 11.7522
Total reward = 10.6063
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.45442 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 11.1646 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.13342 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 13 [9, 17]
Total reward: 6.79392 [5.13342, 10.6063]
Policy after 4 simulations
MCTS Policy:
a=1 : 8.45442 (2)
a=1 o=0 a=2 : 11.1646 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.45442 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 11.1646 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.13342 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=3 o=0 a=1 o=0 a=3 o=0 a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 8.57375 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 8.57375 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 8.57375 (1)
a=4 : 19.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1.66667 [1, 3]
Total reward: 11.8934 [8.57375, 19.5]
Policy after 4 simulations
MCTS Policy:
a=4 : 19.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 8.57375 (1)
a=4 : 19.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=3 o=0 a=1 o=0 a=3 o=0 a=4 o=0 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 3.5 [1, 6]
Total reward: 9.21273 [7.35092, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 7.38864, average = 13.4952
Undiscounted return = 10, average = 16.6667
Starting run 7 with 4 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.4036 (1)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.4036 (1)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.4036 (1)
a=6 : 8.14506 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6.5 [3, 12]
Total reward: 7.27645 [5.4036, 8.57375]
Policy after 4 simulations
MCTS Policy:
a=1 : 8.57375 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.4036 (1)
a=6 : 8.14506 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward 3.58486
Total reward = 13.4056
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 13.4056 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 21 steps, with total reward 9.88735
Total reward = 9.39299
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.39299 (1)
a=3 : 0 (0)
a=4 : 13.4056 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -2.15769
Total reward = -2.04981
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.39299 (1)
a=3 : -2.04981 (1)
a=4 : 13.4056 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.39299 (1)
a=3 : -2.04981 (1)
a=4 : 13.4056 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 17.6667 [11, 21]
Total reward: 7.6872 [-2.04981, 13.4056]
Policy after 4 simulations
MCTS Policy:
a=4 : 13.4056 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.39299 (1)
a=3 : -2.04981 (1)
a=4 : 13.4056 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Sample

# # # # 
# 0$* #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -3.3658
Total reward = -3.19751
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : -3.19751 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 4 [1, 9]
Total reward: 6.33187 [-3.19751, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : -3.19751 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = -0.475, average = 11.4995
Undiscounted return = 0, average = 14.2857
Starting run 8 with 4 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 20 steps, with total reward 9.76091
Total reward = 9.27286
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.27286 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 9.27286 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 18.1451
Total reward = 17.2378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 9.27286 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 17.2378 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward -0.475
Total reward = -10.4512
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 9.27286 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.4512 (1)
a=5 : 0 (0)
a=6 : 17.2378 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8.75 [3, 20]
Total reward: 5.7607 [-10.4512, 17.2378]
Policy after 4 simulations
MCTS Policy:
a=6 : 17.2378 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 9.27286 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.4512 (1)
a=5 : 0 (0)
a=6 : 17.2378 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 25 steps, with total reward 7.79664
Total reward = -2.59319
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.59319 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.59319 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 23 steps, with total reward -6.76466
Total reward = -6.42643
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.59319 (1)
a=5 : 0 (0)
a=6 : -6.42643 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 3.40562
Total reward = 3.23534
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.59319 (1)
a=5 : 3.23534 (1)
a=6 : -6.42643 (1)
Tree depth: 0 [0, 0]
Rollout depth: 17.75 [1, 25]
Total reward: 0.928928 [-6.42643, 9.5]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.59319 (1)
a=5 : 3.23534 (1)
a=6 : -6.42643 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 20.1059
Total reward = 19.1006
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 19.1006 (1)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5.5 [1, 18]
Total reward: 14.2814 [9.5, 19.1006]
Policy after 4 simulations
MCTS Policy:
a=3 : 19.1006 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 19.1006 (1)
a=4 : 19.025 (1)
a=5 : 0 (0)
a=6 : 9.5 (1)
W

# # # # 
# * 1$#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 16.8509
Total reward = 6.00837
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.00837 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.00837 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward -3.01663
Total reward = -2.8658
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.00837 (1)
a=5 : -2.8658 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward -1.94732
Total reward = -1.84995
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.00837 (1)
a=5 : -2.8658 (1)
a=6 : -1.84995 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8 [4, 13]
Total reward: 2.35942 [-2.8658, 8.14506]
Policy after 4 simulations
MCTS Policy:
a=1 : 8.14506 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 6.00837 (1)
a=5 : -2.8658 (1)
a=6 : -1.84995 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=1 o=0 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -2.14908
Total reward = -2.04163
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -2.04163 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = -0.975
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.975 (1)
a=5 : -2.04163 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -0.33171
Total reward = -0.315125
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.315125 (1)
a=3 : 0 (0)
a=4 : -0.975 (1)
a=5 : -2.04163 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -0.428688
Total reward = -0.407253
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.315125 (1)
a=3 : 0 (0)
a=4 : -0.975 (1)
a=5 : -2.04163 (1)
a=6 : -0.407253 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6 [2, 10]
Total reward: -0.934751 [-2.04163, -0.315125]
Policy after 4 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.315125 (1)
a=3 : 0 (0)
a=4 : -0.975 (1)
a=5 : -2.04163 (1)
a=6 : -0.407253 (1)
W

# # # # 
# * 1$#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 6.30249 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 17.5987
Total reward = 16.7188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 16.7188 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 6.30249 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 16.3759
Total reward = 5.55712
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 16.7188 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.55712 (1)
a=5 : 0 (0)
a=6 : 6.30249 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5.75 [3, 9]
Total reward: 9.28804 [5.55712, 16.7188]
Policy after 4 simulations
MCTS Policy:
a=2 : 16.7188 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 16.7188 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.55712 (1)
a=5 : 0 (0)
a=6 : 6.30249 (1)
S

# # # # 
# 0$1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 16.7628
MCTS Values:
a=0 : 16.7628 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 16.7628 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 6.6342 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4.75 [1, 8]
Total reward: 10.1587 [6.6342, 16.7628]
Policy after 4 simulations
MCTS Policy:
a=0 : 16.7628 (1)
Values after 4 simulations
MCTS Values:
a=0 : 16.7628 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
a=6 : 6.6342 (1)
N

# # # # 
# * 1$#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward -0.975
Total reward = -0.92625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.92625 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 24 steps, with total reward -6.42643
Total reward = -6.10511
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -6.10511 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.92625 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = -1.42625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -6.10511 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : -0.92625 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -6.10511 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : -0.92625 (1)
a=6 : 4.87675 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11 [3, 24]
Total reward: -0.895215 [-6.10511, 4.87675]
Policy after 4 simulations
MCTS Policy:
a=6 : 4.87675 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -6.10511 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : -0.92625 (1)
a=6 : 4.87675 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 28 steps, with total reward 9.85436
Total reward = 9.36164
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.36164 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 15.8829
Total reward = 15.0887
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.36164 (1)
a=2 : 15.0887 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 24.2212
Total reward = 23.0101
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.36164 (1)
a=2 : 15.0887 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 23.0101 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 16.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.36164 (1)
a=2 : 15.0887 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : 0 (0)
a=6 : 23.0101 (1)
Tree depth: 0 [0, 0]
Rollout depth: 12.5 [6, 28]
Total reward: 16.0237 [9.36164, 23.0101]
Policy after 4 simulations
MCTS Policy:
a=6 : 23.0101 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.36164 (1)
a=2 : 15.0887 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.6342 (1)
a=5 : 0 (0)
a=6 : 23.0101 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 2 attempts
Expanding node: a=6 o=1 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=6 o=1 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 22 steps, with total reward 3.40562
Total reward = 3.23534
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.23534 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 17.5987
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.23534 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 17.5987 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 19.5
Total reward = 18.525
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 18.525 (1)
a=2 : 3.23534 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 17.5987 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = -1.85494
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 18.525 (1)
a=2 : 3.23534 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.85494 (1)
a=5 : 17.5987 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7.75 [2, 22]
Total reward: 9.37604 [-1.85494, 18.525]
Policy after 4 simulations
MCTS Policy:
a=1 : 18.525 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 18.525 (1)
a=2 : 3.23534 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.85494 (1)
a=5 : 17.5987 (1)
a=6 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=6 o=1 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=6 o=1 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 19.025
Total reward = 18.0737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 18.0737 (1)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 24 steps, with total reward 16.3089
Total reward = 15.4935
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 15.4935 (1)
a=4 : 0 (0)
a=5 : 18.0737 (1)
a=6 : 9.025 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9.66667 [2, 24]
Total reward: 13.1481 [9.025, 18.0737]
Policy after 4 simulations
MCTS Policy:
a=5 : 18.0737 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 15.4935 (1)
a=4 : 0 (0)
a=5 : 18.0737 (1)
a=6 : 9.025 (1)
Check 1

# # # # 
# 0$* #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 4 attempts
Expanding node: a=6 o=1 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 8.57375 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 8.57375 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = -0.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 8.57375 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 8.57375 (1)
a=6 : 5.13342 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5.66667 [1, 13]
Total reward: 5.80179 [-0.5, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 8.57375 (1)
a=6 : 5.13342 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 5.688, average = 10.773
Undiscounted return = 10, average = 13.75
Starting run 9 with 4 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 25 steps, with total reward 9.90326
Total reward = 9.4081
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.4081 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 9.4081 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 16 steps, with total reward -2.71801
Total reward = -2.58211
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 9.4081 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -2.58211 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward -0.256671
Total reward = -0.243837
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 9.4081 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.243837 (1)
a=6 : -2.58211 (1)
Tree depth: 0 [0, 0]
Rollout depth: 16.75 [11, 25]
Total reward: 3.06754 [-2.58211, 9.4081]
Policy after 4 simulations
MCTS Policy:
a=2 : 9.4081 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.688 (1)
a=2 : 9.4081 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.243837 (1)
a=6 : -2.58211 (1)
S

# # # # 
# 0X1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.35092 (1)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : 0 (0)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.35092 (1)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 18.5737
Total reward = 17.6451
MCTS Values:
a=0 : 17.6451 (1)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.35092 (1)
a=6 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5 [3, 7]
Total reward: 10.1383 [6.98337, 17.6451]
Policy after 4 simulations
MCTS Policy:
a=0 : 17.6451 (1)
Values after 4 simulations
MCTS Values:
a=0 : 17.6451 (1)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.35092 (1)
a=6 : 8.57375 (1)
N

# # # # 
# * 1$#
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = -2.26219
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 27 steps, with total reward -6.47036
Total reward = -6.14684
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -6.14684 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -6.14684 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 21 steps, with total reward -2.10314
Total reward = -1.99798
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -6.14684 (1)
a=2 : -1.99798 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 14.25 [4, 27]
Total reward: -0.565489 [-6.14684, 8.14506]
Policy after 4 simulations
MCTS Policy:
a=5 : 8.14506 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -6.14684 (1)
a=2 : -1.99798 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Check 1

# # # # 
# * 1$#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=0 o=0 a=5 o=2 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 16.7628
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.7628 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.7628 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward -1.93955
Total reward = -1.84257
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : -1.84257 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.7628 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = -1.85494
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : -1.84257 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.85494 (1)
a=5 : 16.7628 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4.75 [1, 9]
Total reward: 5.64133 [-1.85494, 16.7628]
Policy after 4 simulations
MCTS Policy:
a=5 : 16.7628 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : -1.84257 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.85494 (1)
a=5 : 16.7628 (1)
a=6 : 0 (0)
Check 1

# # # # 
# * 1$#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=0 o=0 a=5 o=2 a=5 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.98337 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = -4.5964
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.5964 (1)
a=5 : 6.98337 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward -2.51663
Total reward = -2.3908
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.5964 (1)
a=5 : 6.98337 (1)
a=6 : -2.3908 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.5964 (1)
a=5 : 6.98337 (1)
a=6 : -2.3908 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8.75 [7, 12]
Total reward: 1.6576 [-4.5964, 6.98337]
Policy after 4 simulations
MCTS Policy:
a=5 : 6.98337 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.5964 (1)
a=5 : 6.98337 (1)
a=6 : -2.3908 (1)
Check 1

# # # # 
# * 1$#
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=0 o=0 a=5 o=2 a=5 o=2 a=5 o=2 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 7.59875
Total reward = 7.21881
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.21881 (1)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -0.794144
Total reward = -0.754436
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.754436 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.21881 (1)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.63291
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.63291 (1)
a=2 : -0.754436 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.21881 (1)
a=6 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7.75 [4, 15]
Total reward: 4.70877 [-0.754436, 7.73781]
Policy after 4 simulations
MCTS Policy:
a=6 : 7.73781 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.63291 (1)
a=2 : -0.754436 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.21881 (1)
a=6 : 7.73781 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 3 attempts
Expanding node: a=2 o=0 a=0 o=0 a=5 o=2 a=5 o=2 a=5 o=2 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 5.98737 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 15.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 0 (0)
a=6 : 5.98737 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 16.9834
Total reward = 16.1342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 16.1342 (1)
a=6 : 5.98737 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 20.9607
Total reward = 19.9127
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 19.9127 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 16.1342 (1)
a=6 : 5.98737 (1)
Tree depth: 0 [0, 0]
Rollout depth: 10.75 [8, 13]
Total reward: 14.3595 [5.98737, 19.9127]
Policy after 4 simulations
MCTS Policy:
a=2 : 19.9127 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 19.9127 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 16.1342 (1)
a=6 : 5.98737 (1)
S

# # # # 
# 0X1$#
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=0 o=0 a=5 o=2 a=5 o=2 a=5 o=2 a=6 o=1 a=2 o=0 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 4.89386
Total reward = 4.64916
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.64916 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.64916 (1)
a=6 : 7.35092 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 26 steps, with total reward 2.7739
Total reward = 2.6352
MCTS Values:
a=0 : 2.6352 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.64916 (1)
a=6 : 7.35092 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 2.6352 (1)
a=1 : 8.57375 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.64916 (1)
a=6 : 7.35092 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11.75 [3, 26]
Total reward: 5.80226 [2.6352, 8.57375]
Policy after 4 simulations
MCTS Policy:
a=1 : 8.57375 (1)
Values after 4 simulations
MCTS Values:
a=0 : 2.6352 (1)
a=1 : 8.57375 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.64916 (1)
a=6 : 7.35092 (1)
E

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=0 o=0 a=5 o=2 a=5 o=2 a=5 o=2 a=6 o=1 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -2.58638
Total reward = -2.45706
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -2.45706 (1)
a=6 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6.66667 [4, 11]
Total reward: 5.85645 [-2.45706, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -2.45706 (1)
a=6 : 7.73781 (1)
E

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 6.6342, average = 10.3132
Undiscounted return = 10, average = 13.3333
Starting run 10 with 4 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 4.1812
Total reward = 3.97214
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 26.7628
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : 26.7628 (1)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 8 [4, 18]
Total reward: 11.6545 [3.97214, 26.7628]
Policy after 4 simulations
MCTS Policy:
a=4 : 26.7628 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : 26.7628 (1)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Sample

# # # # 
# * 1X#
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -1.22283
Total reward = -1.16169
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.16169 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward -1.42625
Total reward = -1.35494
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.35494 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.16169 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -1.93955
Total reward = -1.84257
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.35494 (1)
a=2 : -1.84257 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.16169 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward -0.975
Total reward = -0.879937
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.35494 (1)
a=2 : -1.84257 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.02081 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : -0.92625 (1)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 5.75 [3, 9]
Total reward: -1.30978 [-1.84257, -0.879937]
Policy after 4 simulations
MCTS Policy:
a=6 : -1.02081 (2)
a=6 o=2 a=2 : 0 (0)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.35494 (1)
a=2 : -1.84257 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1.02081 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : -0.92625 (1)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
Matched 1 states
Created 1 local transformations out of 3 attempts
Expanding node: a=4 o=0 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -2.14908
Total reward = -2.04163
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.04163 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.04163 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.18796 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 7.73781 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 4.5 [2, 7]
Total reward: 5.61984 [-2.04163, 9.025]
Policy after 4 simulations
MCTS Policy:
a=6 : 8.18796 (2)
a=6 o=2 a=2 : 7.73781 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.04163 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.18796 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 7.73781 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 30 steps, with total reward -2.87407
Total reward = -2.73036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.73036 (1)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.73036 (1)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.56422 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 7.35092 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 13.5 [4, 30]
Total reward: 4.31871 [-2.73036, 8.14506]
Policy after 4 simulations
MCTS Policy:
a=6 : 7.56422 (2)
a=6 o=2 a=2 : 7.35092 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -2.73036 (1)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.56422 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 7.35092 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Check 2

# # # # 
# * 1X#
# . . #
# # # # 
Observed bad
Reward 0
Matched 1 states
Created 1 local transformations out of 2 attempts
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.40127
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.35092 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 a=6 o=2 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 24 steps, with total reward -0.898575
Total reward = -0.810963
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 3.26998 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -0.853646 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 14 [6, 24]
Total reward: 4.23215 [-0.810963, 7.35092]
Policy after 4 simulations
MCTS Policy:
a=1 : 5.98737 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 3.26998 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -0.853646 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=2 a=6 o=2 a=6 o=2 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 9.025 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 9.025 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = -0.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 9.025 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1.33333 [1, 2]
Total reward: 7.00625 [-0.5, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 9.025 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = -2.26219, average = 9.05562
Undiscounted return = 0, average = 12
Simulations = 4
Runs = 10
Undiscounted return = 12 +- 2.75681
Discounted return = 9.05562 +- 2.32989
Time = 0.0055857
Starting run 1 with 8 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 16.7628
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 16.7628 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward -0.475
Total reward = -0.45125
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 16.7628 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -0.45125 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = -4.01263
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 16.7628 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : -0.45125 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward -6.57267
Total reward = -6.24404
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 16.7628 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : -0.45125 (1)
a=6 : -6.24404 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward -0.975
Total reward = -0.92625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : 16.7628 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : -0.45125 (1)
a=6 : -6.24404 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : 12.8939 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : -0.45125 (1)
a=6 : -6.24404 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Adding sample:

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : 10.5917 (3)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 6.30249 (1)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : -0.45125 (1)
a=6 : -6.24404 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 23 steps, with total reward 3.23534
Total reward = 2.91989
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : 10.5917 (3)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 6.30249 (1)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : 1.23432 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 3.07357 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : -6.24404 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 8.5 [1, 23]
Total reward: 2.88261 [-6.24404, 16.7628]
Policy after 8 simulations
MCTS Policy:
a=2 : 10.5917 (3)
a=2 o=0 a=1 : 9.5 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.92625 (1)
a=2 : 10.5917 (3)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 6.30249 (1)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : 1.23432 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 3.07357 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : -6.24404 (1)
S

# # # # 
# 0X1X#
# * . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 0 (0)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 13.5487
Total reward = 12.8712
MCTS Values:
a=0 : 12.8712 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 12.8712 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 11.5321
Total reward = 10.9555
MCTS Values:
a=0 : 12.8712 (1)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 10.9555 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=2 o=0 a=0 o=0 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 23 steps, with total reward 5.51772
Total reward = 4.97974
MCTS Values:
a=0 : 8.92549 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 5.24183 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 10.9555 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=1 
Adding sample:

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 22.4794
Total reward = 20.2876
MCTS Values:
a=0 : 8.92549 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 5.24183 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 15.6216 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 21.3554 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Expanding node: a=2 o=0 a=1 o=0 
Adding sample:

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -0.33171
Total reward = -0.299368
MCTS Values:
a=0 : 8.92549 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 5.24183 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 4.36282 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : -0.315125 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 15.6216 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 21.3554 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 
Adding sample:

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : 8.92549 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 5.24183 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 4.36282 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : -0.315125 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.35941 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 9.025 (1)
a=6 : 15.6216 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 21.3554 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 10 [2, 23]
Total reward: 9.31732 [-0.299368, 20.2876]
Policy after 8 simulations
MCTS Policy:
a=6 : 15.6216 (2)
a=6 o=1 a=6 : 21.3554 (1)
Values after 8 simulations
MCTS Values:
a=0 : 8.92549 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 5.24183 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 4.36282 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : -0.315125 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.35941 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 9.025 (1)
a=6 : 15.6216 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 21.3554 (1)
Check 2

# # # # 
# 0X1X#
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 7.09664
Total reward = 6.74181
MCTS Values:
a=0 : 6.74181 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 6.74181 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 6.74181 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 6.74181 (1)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=5 o=1 
Adding sample:

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 9.09275
Total reward = 8.2062
MCTS Values:
a=0 : 6.74181 (1)
a=1 : 8.14506 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.17563 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 8.63811 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=1 o=0 
Adding sample:

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward -0.475
Total reward = -0.428687
MCTS Values:
a=0 : 6.74181 (1)
a=1 : 3.85819 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : -0.45125 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.17563 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 8.63811 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=0 o=0 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 1.03547
Total reward = 0.934513
MCTS Values:
a=0 : 3.83816 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0.983697 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 3.85819 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : -0.45125 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.17563 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 8.63811 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 
Adding sample:

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 16.8509
Total reward = 15.208
MCTS Values:
a=0 : 3.83816 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0.983697 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 3.85819 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : -0.45125 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.17563 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 8.63811 (1)
a=6 : 10.9211 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : 16.0084 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 7.375 [3, 17]
Total reward: 6.69826 [-0.428687, 15.208]
Policy after 8 simulations
MCTS Policy:
a=6 : 10.9211 (2)
a=6 o=2 a=6 : 16.0084 (1)
Values after 8 simulations
MCTS Values:
a=0 : 3.83816 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0.983697 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 3.85819 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : -0.45125 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.17563 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 8.63811 (1)
a=6 : 10.9211 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : 16.0084 (1)
Check 2

# # # # 
# 0X1X#
# * . #
# # # # 
Observed bad
Reward 0
Matched 1 states
Created 1 local transformations out of 2 attempts
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 22 steps, with total reward -10.2478
Total reward = -9.73541
MCTS Values:
a=0 : 0 (0)
a=1 : -9.73541 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward -4.39209
Total reward = -4.17248
MCTS Values:
a=0 : 0 (0)
a=1 : -9.73541 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -4.17248 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -2.64908
Total reward = -2.51663
MCTS Values:
a=0 : -2.51663 (1)
a=1 : -9.73541 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -4.17248 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -2.51663 (1)
a=1 : -9.73541 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 6.30249 (1)
a=6 : -4.17248 (1)
Starting simulation

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=1 
Adding sample:

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 28 steps, with total reward 2.50344
Total reward = 2.25936
MCTS Values:
a=0 : -2.51663 (1)
a=1 : -9.73541 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.28092 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 2.37827 (1)
a=6 : -4.17248 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=0 o=0 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : 2.41715 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 7.73781 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : -9.73541 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.28092 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 2.37827 (1)
a=6 : -4.17248 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 
Adding sample:

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : 2.41715 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 7.73781 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : -9.73541 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 5.30426 (3)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 2.37827 (1)
a=5 o=2 a=0 : 0 (0)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : 7.73781 (1)
a=5 o=2 a=6 : 0 (0)
a=6 : -4.17248 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=6 o=2 
Adding sample:

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.4036
MCTS Values:
a=0 : 2.41715 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 7.73781 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : -9.73541 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 5.30426 (3)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 2.37827 (1)
a=5 o=2 a=0 : 0 (0)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : 7.73781 (1)
a=5 o=2 a=6 : 0 (0)
a=6 : 0.615559 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : 5.688 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 12.875 [5, 28]
Total reward: 1.53035 [-9.73541, 7.35092]
Policy after 8 simulations
MCTS Policy:
a=5 : 5.30426 (3)
a=5 o=1 a=6 : 2.37827 (1)
a=5 o=2 a=5 : 7.73781 (1)
Values after 8 simulations
MCTS Values:
a=0 : 2.41715 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 7.73781 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : -9.73541 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 5.30426 (3)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 2.37827 (1)
a=5 o=2 a=0 : 0 (0)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : 7.73781 (1)
a=5 o=2 a=6 : 0 (0)
a=6 : 0.615559 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : 5.688 (1)
Check 1

# # # # 
# 0X1X#
# * . #
# # # # 
Observed bad
Reward 0
Matched 1 states
Created 1 local transformations out of 2 attempts
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 4.1812
Total reward = 3.97214
MCTS Values:
a=0 : 0 (0)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward -0.583769
Total reward = -0.55458
MCTS Values:
a=0 : 0 (0)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -0.55458 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : 0 (0)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 6.30249 (1)
a=6 : -0.55458 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 6.30249 (1)
a=6 : -0.55458 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=0 o=0 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : 7.54436 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 8.14506 (1)
a=0 o=0 a=6 : 0 (0)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 6.30249 (1)
a=6 : -0.55458 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=5 o=2 
Adding sample:

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 28 steps, with total reward -2.90016
Total reward = -2.61739
MCTS Values:
a=0 : 7.54436 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 8.14506 (1)
a=0 o=0 a=6 : 0 (0)
a=1 : 3.97214 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 1.84255 (2)
a=5 o=2 a=0 : 0 (0)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : -2.75515 (1)
a=5 o=2 a=6 : 0 (0)
a=6 : -0.55458 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=1 o=0 
Adding sample:

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -1.51086
Total reward = -1.36355
MCTS Values:
a=0 : 7.54436 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 8.14506 (1)
a=0 o=0 a=6 : 0 (0)
a=1 : 1.3043 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : -1.43532 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 1.84255 (2)
a=5 o=2 a=0 : 0 (0)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : -2.75515 (1)
a=5 o=2 a=6 : 0 (0)
a=6 : -0.55458 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=6 o=2 
Adding sample:

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : 7.54436 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 8.14506 (1)
a=0 o=0 a=6 : 0 (0)
a=1 : 1.3043 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : -1.43532 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 1.84255 (2)
a=5 o=2 a=0 : 0 (0)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : -2.75515 (1)
a=5 o=2 a=6 : 0 (0)
a=6 : 3.79524 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 8.57375 (1)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.5 [0, 1]
Rollout depth: 11.25 [3, 28]
Total reward: 3.62161 [-2.61739, 8.14506]
Policy after 8 simulations
MCTS Policy:
a=0 : 7.54436 (2)
a=0 o=0 a=5 : 8.14506 (1)
Values after 8 simulations
MCTS Values:
a=0 : 7.54436 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 8.14506 (1)
a=0 o=0 a=6 : 0 (0)
a=1 : 1.3043 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : -1.43532 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 1.84255 (2)
a=5 o=2 a=0 : 0 (0)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : -2.75515 (1)
a=5 o=2 a=6 : 0 (0)
a=6 : 3.79524 (2)
a=6 o=2 a=0 : 0 (0)
a=6 o=2 a=1 : 8.57375 (1)
a=6 o=2 a=2 : -1e+10 (1000000)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : 0 (0)
N

# # # # 
# * 1X#
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=0 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward -1.43532
Total reward = -1.36355
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.36355 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward -0.45125
Total reward = -10.4287
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.36355 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward -3.69751
Total reward = -3.51263
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.36355 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : -3.51263 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward -0.732154
Total reward = -0.695546
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.36355 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : -3.51263 (1)
a=6 : -0.695546 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : -1.36355 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : -3.51263 (1)
a=6 : -0.695546 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.66914 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 6.30249 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1.36355 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : -3.51263 (1)
a=6 : -0.695546 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward -1.42625
Total reward = -1.28719
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.66914 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 6.30249 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1.36355 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : -3.51263 (1)
a=6 : -0.991369 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : 0 (0)
a=6 o=2 a=5 : -1.35494 (1)
a=6 o=2 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=2 o=0 
Adding sample:

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.1812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.66914 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 6.30249 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 1.40883 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 4.40127 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : -3.51263 (1)
a=6 : -0.991369 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : 0 (0)
a=6 o=2 a=5 : -1.35494 (1)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.375 [0, 1]
Rollout depth: 9.5 [4, 17]
Total reward: 0.0289859 [-10.4287, 7.35092]
Policy after 8 simulations
MCTS Policy:
a=1 : 6.66914 (2)
a=1 o=0 a=3 : 6.30249 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.66914 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 6.30249 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 1.40883 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 4.40127 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.4287 (1)
a=5 : -3.51263 (1)
a=6 : -0.991369 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : 0 (0)
a=6 o=2 a=5 : -1.35494 (1)
a=6 o=2 a=6 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward -1.2306
Total reward = -1.16907
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.16907 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -1.10361
Total reward = -1.04842
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.16907 (1)
a=3 : -1.04842 (1)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.16907 (1)
a=3 : -1.04842 (1)
a=4 : 0 (0)
a=5 : 9.5 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = -1.42625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.16907 (1)
a=3 : -1.04842 (1)
a=4 : -1.42625 (1)
a=5 : 9.5 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward -9.79937
Total reward = -9.3094
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.16907 (1)
a=3 : -1.04842 (1)
a=4 : -1.42625 (1)
a=5 : 9.5 (1)
a=6 : -9.3094 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -1.16907 (1)
a=3 : -1.04842 (1)
a=4 : -1.42625 (1)
a=5 : 9.5 (1)
a=6 : -9.3094 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=1 o=0 a=5 o=2 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 18.525
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -1.16907 (1)
a=3 : -1.04842 (1)
a=4 : -1.42625 (1)
a=5 : 14.0125 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : 0 (0)
a=5 o=2 a=4 : 19.5 (1)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : -9.3094 (1)
Tree depth: 0.125 [0, 1]
Rollout depth: 6.5 [1, 13]
Total reward: 4.38398 [-9.3094, 18.525]
Policy after 8 simulations
MCTS Policy:
a=5 : 14.0125 (2)
a=5 o=2 a=4 : 19.5 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -1.16907 (1)
a=3 : -1.04842 (1)
a=4 : -1.42625 (1)
a=5 : 14.0125 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : 0 (0)
a=5 o=2 a=4 : 19.5 (1)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : -9.3094 (1)
Check 1

# # # # 
# 0X* #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=1 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = -0.975
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.975 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 12.755
Total reward = 12.1172
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 12.1172 (1)
a=4 : -0.975 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 12.1172 (1)
a=4 : -0.975 (1)
a=5 : 0 (0)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 3.77896
Total reward = 3.59002
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.59002 (1)
a=3 : 12.1172 (1)
a=4 : -0.975 (1)
a=5 : 0 (0)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 28 steps, with total reward 6.47558
Total reward = 6.1518
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.59002 (1)
a=3 : 12.1172 (1)
a=4 : -0.975 (1)
a=5 : 6.1518 (1)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 3.59002 (1)
a=3 : 12.1172 (1)
a=4 : -0.975 (1)
a=5 : 6.1518 (1)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=2 o=0 a=6 o=2 a=6 o=2 a=5 o=2 a=0 o=0 a=1 o=0 a=5 o=1 a=3 o=0 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 3.59002 (1)
a=3 : 9.73406 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 7.73781 (1)
a=3 o=0 a=6 : 0 (0)
a=4 : -0.975 (1)
a=5 : 6.1518 (1)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 3.59002 (1)
a=3 : 9.73406 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 7.73781 (1)
a=3 o=0 a=6 : 0 (0)
a=4 : -0.975 (1)
a=5 : 6.1518 (1)
a=6 : 9.5 (1)
Tree depth: 0.125 [0, 1]
Rollout depth: 11.6667 [1, 28]
Total reward: 7.21687 [-0.975, 12.1172]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 3.59002 (1)
a=3 : 9.73406 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 7.73781 (1)
a=3 o=0 a=6 : 0 (0)
a=4 : -0.975 (1)
a=5 : 6.1518 (1)
a=6 : 9.5 (1)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 6.98337, average = 6.98337
Undiscounted return = 10, average = 10
Starting run 2 with 8 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 16.9834
Total reward = 16.1342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.1342 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.1342 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 16.1342 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 16.1342 (1)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 16.1342 (1)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 17.3509
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.9623 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 7.73781 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 16.1342 (1)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=5 o=1 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.9623 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 7.73781 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 11.2183 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 6.6342 (1)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 17.3509
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.7585 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 7.73781 (1)
a=4 o=0 a=2 : 7.73781 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 11.2183 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 6.6342 (1)
a=6 : 7.73781 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 4.75 [2, 8]
Total reward: 12.6875 [6.30249, 18.5738]
Policy after 8 simulations
MCTS Policy:
a=4 : 17.7585 (3)
a=4 o=0 a=1 : 7.73781 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.7585 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 7.73781 (1)
a=4 o=0 a=2 : 7.73781 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 11.2183 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 6.6342 (1)
a=6 : 7.73781 (1)
Sample

# # # # 
# * 1$#
# . . #
# # # # 
Reward -10
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 21 steps, with total reward 3.58486
Total reward = 3.40562
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.40562 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.40562 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.40562 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.35941 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 9.025 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.40562 (1)
a=2 : 7.54436 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.35941 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 9.025 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.14118 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 5.13342 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 7.54436 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.35941 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 9.025 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.14118 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 5.13342 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 7.54436 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.15221 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 8.14506 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 9.025 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.14118 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 5.13342 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.03791 (3)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.15221 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 8.14506 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 9.025 (1)
Tree depth: 0.625 [0, 1]
Rollout depth: 6.875 [1, 21]
Total reward: 7.10659 [3.40562, 9.025]
Policy after 8 simulations
MCTS Policy:
a=6 : 8.15221 (3)
a=6 o=1 a=6 : 9.025 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.14118 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 5.13342 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.03791 (3)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.15221 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 8.14506 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 9.025 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.688 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.688 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.688 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.03688 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.688 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 11.7061
Total reward = 10.5648
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.03688 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.27606 (2)
a=2 o=0 a=0 : 11.1208 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 5.688 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 16.7188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.03688 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.27606 (2)
a=2 o=0 a=0 : 11.1208 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.2034 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.03688 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.27606 (2)
a=2 o=0 a=0 : 11.1208 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.0482 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 8.14506 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.19125 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.27606 (2)
a=2 o=0 a=0 : 11.1208 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.0482 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 8.14506 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.5987 (1)
Tree depth: 0.625 [0, 1]
Rollout depth: 6.28571 [1, 13]
Total reward: 9.28381 [5.688, 16.7188]
Policy after 8 simulations
MCTS Policy:
a=6 : 10.0482 (3)
a=6 o=1 a=6 : 17.5987 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.19125 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.27606 (2)
a=2 o=0 a=0 : 11.1208 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.0482 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 8.14506 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.5987 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
Matched 2 states
Created 1 local transformations out of 2 attempts
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 17.5987
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 18.5737
Total reward = 17.6451
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.6451 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.6451 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 16.4834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.0642 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 17.3509 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.0642 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 17.3509 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.9506 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 6.6342 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.2341 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 17.3509 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 9.025 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.9506 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 6.6342 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.2341 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 17.3509 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 9.025 (1)
a=2 : 8.58503 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.9506 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 6.6342 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 12.3222
Total reward = 11.1208
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.2341 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 17.3509 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 9.025 (1)
a=2 : 8.58503 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.674 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 6.6342 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 11.7061 (1)
Tree depth: 0.625 [0, 1]
Rollout depth: 5 [1, 12]
Total reward: 11.8618 [6.30249, 17.6451]
Policy after 8 simulations
MCTS Policy:
a=1 : 14.2341 (3)
a=1 o=0 a=4 : 17.3509 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.2341 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 17.3509 (1)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 9.025 (1)
a=2 : 8.58503 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.674 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 6.6342 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 11.7061 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 5.98737 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 18.0737
Total reward = 17.1701
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 17.1701 (1)
a=3 : 5.98737 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 17.3509
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 17.1701 (1)
a=3 : 5.98737 (1)
a=4 : 17.3509 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 17.1701 (1)
a=3 : 5.98737 (1)
a=4 : 17.3509 (1)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 17.1701 (1)
a=3 : 5.98737 (1)
a=4 : 17.3509 (1)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=4 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 17.7378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 17.1701 (1)
a=3 : 5.98737 (1)
a=4 : 17.5444 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 8.14506 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 13.335 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : 5.98737 (1)
a=4 : 17.5444 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 8.14506 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 13.335 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : 5.98737 (1)
a=4 : 17.5444 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 8.14506 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 5 [1, 10]
Total reward: 12.1558 [5.98737, 17.7378]
Policy after 8 simulations
MCTS Policy:
a=4 : 17.5444 (2)
a=4 o=0 a=2 : 8.14506 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 13.335 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : 5.98737 (1)
a=4 : 17.5444 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 8.14506 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward 3.58486
Total reward = 3.23534
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 6.36767 (2)
a=2 o=0 a=0 : 3.40562 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=4 o=0 a=3 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 6.36767 (2)
a=2 o=0 a=0 : 3.40562 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 8.14506 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 8.57375 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 6.36767 (2)
a=2 o=0 a=0 : 3.40562 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 8.14506 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 8.57375 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 6.36767 (2)
a=2 o=0 a=0 : 3.40562 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 8.00931 (3)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 8.14506 (1)
a=3 o=0 a=2 : 8.57375 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0.375 [0, 1]
Rollout depth: 6.6 [1, 21]
Total reward: 8.34541 [3.23534, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 6.36767 (2)
a=2 o=0 a=0 : 3.40562 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 8.00931 (3)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 8.14506 (1)
a=3 o=0 a=2 : 8.57375 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 5.88287, average = 6.43312
Undiscounted return = 10, average = 10
Starting run 3 with 8 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 24.5887
Total reward = 23.3593
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 23.3593 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward -10.2844
Total reward = -9.77018
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -9.77018 (1)
a=6 : 23.3593 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 15.9874
Total reward = 15.188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.188 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -9.77018 (1)
a=6 : 23.3593 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 9.09275
Total reward = 8.63811
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.188 (1)
a=2 : 8.63811 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -9.77018 (1)
a=6 : 23.3593 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 16.3025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.188 (1)
a=2 : 8.63811 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.3025 (1)
a=5 : -9.77018 (1)
a=6 : 23.3593 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.188 (1)
a=2 : 8.63811 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.3025 (1)
a=5 : -9.77018 (1)
a=6 : 14.5236 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 5.98737 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 16.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.188 (1)
a=2 : 8.63811 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.4683 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 6.98337 (1)
a=5 : -9.77018 (1)
a=6 : 14.5236 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 5.98737 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.344 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.63811 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.4683 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 6.98337 (1)
a=5 : -9.77018 (1)
a=6 : 14.5236 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 5.98737 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.375 [0, 1]
Rollout depth: 9 [6, 13]
Total reward: 10.6925 [-9.77018, 23.3593]
Policy after 8 simulations
MCTS Policy:
a=4 : 16.4683 (2)
a=4 o=0 a=6 : 6.98337 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 12.344 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 8.63811 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.4683 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 6.98337 (1)
a=5 : -9.77018 (1)
a=6 : 14.5236 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 5.98737 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Sample

# # # # 
# * 1X#
# . . #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 17.1701
Total reward = 16.3116
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 16.3116 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 16.3116 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 14.0403
Total reward = 13.3383
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.3383 (1)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 16.3116 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 31 steps, with total reward -3.54161
Total reward = -3.19631
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.3383 (1)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.55763 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -3.36453 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.7417 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 8.57375 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.55763 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -3.36453 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward -1.94732
Total reward = -1.75745
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.7417 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 8.57375 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 1.55965 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -1.84995 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.55763 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -3.36453 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.48891 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 8.57375 (1)
a=1 o=0 a=3 : 7.35092 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 1.55965 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -1.84995 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.55763 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -3.36453 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.48891 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 8.57375 (1)
a=1 o=0 a=3 : 7.35092 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 1.55965 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -1.84995 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.58315 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 6.98337 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -3.36453 (1)
Tree depth: 0.625 [0, 1]
Rollout depth: 11.125 [3, 31]
Total reward: 6.41693 [-3.19631, 16.3116]
Policy after 8 simulations
MCTS Policy:
a=1 : 9.48891 (3)
a=1 o=0 a=2 : 8.57375 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.48891 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 8.57375 (1)
a=1 o=0 a=3 : 7.35092 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 1.55965 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -1.84995 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.58315 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 6.98337 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : -3.36453 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward -1.35446
Total reward = -1.28673
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : -1.28673 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = -0.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : -1.28673 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : -1.28673 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : -1.28673 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 4.87675 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : -1.28673 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 4.87675 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1.28673 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 4.87675 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=6 o=2 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1.28673 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 6.72525 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : 0 (0)
a=6 o=2 a=4 : 0 (0)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 9.025 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 6.8 [1, 16]
Total reward: 6.33297 [-1.28673, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1.28673 (1)
a=4 : -0.5 (1)
a=5 : -1e+10 (1000000)
a=6 : 6.72525 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : 0 (0)
a=6 o=2 a=4 : 0 (0)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 9.025 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 19.025, average = 10.6304
Undiscounted return = 20, average = 13.3333
Starting run 4 with 8 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = -5.8188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -1.85494
Total reward = -1.76219
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 0 (0)
a=6 : -1.76219 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 4.1812
Total reward = 3.97214
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 0 (0)
a=6 : -1.76219 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 0 (0)
a=6 : -1.76219 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 8.33831
Total reward = 7.92139
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 7.92139 (1)
a=6 : -1.76219 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -1.43532
Total reward = 8.20463
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.61481 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 8.63645 (1)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 7.92139 (1)
a=6 : -1.76219 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.61481 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 8.63645 (1)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 3.97214 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 7.45238 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 7.35092 (1)
a=6 : -1.76219 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 16.7628
Total reward = 15.1284
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.61481 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 8.63645 (1)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 9.55029 (2)
a=2 o=0 a=0 : 15.9247 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 7.45238 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 7.35092 (1)
a=6 : -1.76219 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 9 [2, 18]
Total reward: 5.45675 [-5.8188, 15.1284]
Policy after 8 simulations
MCTS Policy:
a=2 : 9.55029 (2)
a=2 o=0 a=0 : 15.9247 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.61481 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 8.63645 (1)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 9.55029 (2)
a=2 o=0 a=0 : 15.9247 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 7.45238 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 7.35092 (1)
a=6 : -1.76219 (1)
S

# # # # 
# 0X1$#
# * . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.35092 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : 5.688 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.35092 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 15 steps, with total reward -2.47417
Total reward = -2.35046
MCTS Values:
a=0 : 5.688 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -2.35046 (1)
a=6 : 7.35092 (1)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 32 steps, with total reward -0.880822
Total reward = -0.836781
MCTS Values:
a=0 : 5.688 (1)
a=1 : -0.836781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -2.35046 (1)
a=6 : 7.35092 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=1 
Adding sample:

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : 5.688 (1)
a=1 : -0.836781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -2.35046 (1)
a=6 : 7.74799 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 8.57375 (1)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=2 o=0 a=0 o=0 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward -2.51663
Total reward = -11.7713
MCTS Values:
a=0 : -3.04163 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -12.3908 (1)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : -0.836781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -2.35046 (1)
a=6 : 7.74799 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 8.57375 (1)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Adding sample:

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 27 steps, with total reward -3.35217
Total reward = -3.02533
MCTS Values:
a=0 : -3.04163 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -12.3908 (1)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : -0.836781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -2.35046 (1)
a=6 : 4.15688 (3)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 8.57375 (1)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -3.18456 (1)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Expanding node: a=2 o=0 a=1 o=0 
Adding sample:

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -3.04163 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -12.3908 (1)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 3.25707 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 7.73781 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -2.35046 (1)
a=6 : 4.15688 (3)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 8.57375 (1)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -3.18456 (1)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.5 [0, 1]
Rollout depth: 13.375 [3, 32]
Total reward: 1.31888 [-11.7713, 8.14506]
Policy after 8 simulations
MCTS Policy:
a=6 : 4.15688 (3)
a=6 o=1 a=1 : 8.57375 (1)
Values after 8 simulations
MCTS Values:
a=0 : -3.04163 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -12.3908 (1)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 3.25707 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 7.73781 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -2.35046 (1)
a=6 : 4.15688 (3)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 8.57375 (1)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -3.18456 (1)
a=6 o=1 a=6 : 0 (0)
Check 2

# # # # 
# 0X1$#
# * . #
# # # # 
Observed good
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 25 steps, with total reward 7.79664
Total reward = 7.40681
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.40681 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.40681 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 7.40681 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 7.40681 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Expanding node: a=2 o=0 a=6 o=1 a=1 o=0 
Adding sample:

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 17.1701
Total reward = 15.496
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 12.498 (2)
a=1 o=0 a=0 : 16.3116 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
a=6 : 7.40681 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=1 a=5 o=1 
Adding sample:

# # # # 
# 0$1$#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 12.498 (2)
a=1 o=0 a=0 : 16.3116 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.35941 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 8.57375 (1)
a=5 o=1 a=6 : 0 (0)
a=6 : 7.40681 (1)
Starting simulation

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=2 o=0 a=6 o=1 a=0 o=0 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 16.7188
MCTS Values:
a=0 : 12.2283 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 17.5987 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 12.498 (2)
a=1 o=0 a=0 : 16.3116 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.35941 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 8.57375 (1)
a=5 o=1 a=6 : 0 (0)
a=6 : 7.40681 (1)
Starting simulation

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Expanding node: a=2 o=0 a=6 o=1 a=6 o=1 
Adding sample:

# # # # 
# 0$1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : 12.2283 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 17.5987 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 12.498 (2)
a=1 o=0 a=0 : 16.3116 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.35941 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 8.57375 (1)
a=5 o=1 a=6 : 0 (0)
a=6 : 7.99028 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 9.025 (1)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.5 [0, 1]
Rollout depth: 5.875 [1, 25]
Total reward: 10.269 [7.40681, 16.7188]
Policy after 8 simulations
MCTS Policy:
a=1 : 12.498 (2)
a=1 o=0 a=0 : 16.3116 (1)
Values after 8 simulations
MCTS Values:
a=0 : 12.2283 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 17.5987 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 0 (0)
a=1 : 12.498 (2)
a=1 o=0 a=0 : 16.3116 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.35941 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 8.57375 (1)
a=5 o=1 a=6 : 0 (0)
a=6 : 7.99028 (2)
a=6 o=1 a=0 : 0 (0)
a=6 o=1 a=1 : 9.025 (1)
a=6 o=1 a=2 : -1e+10 (1000000)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
E

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 9.025 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 9.025 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 9.025 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=2 o=0 a=6 o=1 a=1 o=0 a=0 o=0 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : 8.79938 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : 0 (0)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 9.025 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
a=6 : 6.6342 (1)
Starting simulation

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Expanding node: a=2 o=0 a=6 o=1 a=1 o=0 a=5 o=1 
Adding sample:

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : 8.79938 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : 0 (0)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 9.025 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.82253 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 10 (1)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : 0 (0)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=6 : 6.6342 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 4.2 [2, 8]
Total reward: 8.70198 [6.6342, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : 8.79938 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : 0 (0)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=0 o=0 a=6 : 9.025 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.82253 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 10 (1)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : 0 (0)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=6 : 6.6342 (1)
E

# # # # 
# 0X1$#
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 8.57375, average = 10.1162
Undiscounted return = 10, average = 12.5
Starting run 5 with 8 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 17.5987
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 5.4036 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 5.4036 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1$#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.6342 (1)
a=6 : 5.4036 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = -0.975
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 6.6342 (1)
a=6 : 5.4036 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.5494 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 6.6342 (1)
a=6 : 5.4036 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.1812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.5494 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.6031 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 6.6342 (1)
a=6 : 5.4036 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward -3.16972
Total reward = -2.86067
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.5494 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.6031 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 1.88677 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : -3.01123 (1)
a=6 : 5.4036 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 8.71429 [2, 18]
Total reward: 6.06339 [-2.86067, 17.5987]
Policy after 8 simulations
MCTS Policy:
a=1 : 13.5494 (2)
a=1 o=0 a=1 : 10 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.5494 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.6031 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 1.88677 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : -3.01123 (1)
a=6 : 5.4036 (1)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = -3.69751
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -3.69751 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -3.69751 (1)
a=5 : 0 (0)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 30 steps, with total reward -0.51454
Total reward = -0.488813
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.488813 (1)
a=3 : 0 (0)
a=4 : -3.69751 (1)
a=5 : 0 (0)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.488813 (1)
a=3 : 0 (0)
a=4 : -3.69751 (1)
a=5 : 9.5 (1)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -0.488813 (1)
a=3 : 0 (0)
a=4 : -3.69751 (1)
a=5 : 9.5 (1)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward -2.64908
Total reward = -2.51663
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -0.488813 (1)
a=3 : -2.51663 (1)
a=4 : -3.69751 (1)
a=5 : 9.5 (1)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -0.488813 (1)
a=3 : -2.51663 (1)
a=4 : -3.69751 (1)
a=5 : 9.5 (1)
a=6 : 8.14506 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=1 o=0 a=5 o=2 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -0.488813 (1)
a=3 : -2.51663 (1)
a=4 : -3.69751 (1)
a=5 : 9.5 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 10 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : 0 (0)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : 8.14506 (1)
Tree depth: 0.125 [0, 1]
Rollout depth: 10.2 [1, 30]
Total reward: 5.05526 [-3.69751, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -0.488813 (1)
a=3 : -2.51663 (1)
a=4 : -3.69751 (1)
a=5 : 9.5 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 10 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : 0 (0)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : 8.14506 (1)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.5, average = 9.993
Undiscounted return = 10, average = 12
Starting run 6 with 8 simulations... 
Expanding node: 

# # # # 
# * 1X#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 15.9874
Total reward = 15.188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 15.188 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 13.7072
Total reward = 13.0218
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 13.0218 (1)
a=6 : 15.188 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward -0.45125
Total reward = -0.428688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.428688 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 13.0218 (1)
a=6 : 15.188 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -2.14908
Total reward = -12.0416
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.428688 (1)
a=3 : -1e+10 (1000000)
a=4 : -12.0416 (1)
a=5 : 13.0218 (1)
a=6 : 15.188 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 19.5
Total reward = 18.525
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 18.525 (1)
a=2 : -0.428688 (1)
a=3 : -1e+10 (1000000)
a=4 : -12.0416 (1)
a=5 : 13.0218 (1)
a=6 : 15.188 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.0125 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -0.428688 (1)
a=3 : -1e+10 (1000000)
a=4 : -12.0416 (1)
a=5 : 13.0218 (1)
a=6 : 15.188 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward -2.04981
Total reward = -1.84995
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.0125 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -0.428688 (1)
a=3 : -1e+10 (1000000)
a=4 : -12.0416 (1)
a=5 : 13.0218 (1)
a=6 : 6.66902 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1.94732 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=5 o=1 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.0125 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -0.428688 (1)
a=3 : -1e+10 (1000000)
a=4 : -12.0416 (1)
a=5 : 10.7978 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 9.025 (1)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=6 : 6.66902 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1.94732 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.375 [0, 1]
Rollout depth: 7.42857 [2, 14]
Total reward: 6.31104 [-12.0416, 18.525]
Policy after 8 simulations
MCTS Policy:
a=1 : 14.0125 (2)
a=1 o=0 a=1 : 10 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.0125 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : -0.428688 (1)
a=3 : -1e+10 (1000000)
a=4 : -12.0416 (1)
a=5 : 10.7978 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 9.025 (1)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=6 : 6.66902 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : -1.94732 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 9.025 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = -0.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 9.025 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward -1.66958
Total reward = -1.5861
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 9.025 (1)
a=6 : -1.5861 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -0.5 (1)
a=5 : 9.025 (1)
a=6 : -1.5861 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 27 steps, with total reward -13.0528
Total reward = -12.4002
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -12.4002 (1)
a=4 : -0.5 (1)
a=5 : 9.025 (1)
a=6 : -1.5861 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 23 steps, with total reward -5.78966
Total reward = -5.50018
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -5.50018 (1)
a=3 : -12.4002 (1)
a=4 : -0.5 (1)
a=5 : 9.025 (1)
a=6 : -1.5861 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -5.50018 (1)
a=3 : -12.4002 (1)
a=4 : -0.5 (1)
a=5 : 9.025 (1)
a=6 : -1.5861 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=1 o=0 a=5 o=2 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -5.50018 (1)
a=3 : -12.4002 (1)
a=4 : -0.5 (1)
a=5 : 8.58503 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 8.57375 (1)
a=5 o=2 a=3 : 0 (0)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : -1.5861 (1)
Tree depth: 0.125 [0, 1]
Rollout depth: 12 [1, 27]
Total reward: 2.14795 [-12.4002, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -5.50018 (1)
a=3 : -12.4002 (1)
a=4 : -0.5 (1)
a=5 : 8.58503 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 8.57375 (1)
a=5 o=2 a=3 : 0 (0)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : -1.5861 (1)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.5, average = 9.91083
Undiscounted return = 10, average = 11.6667
Starting run 7 with 8 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1X#
# . * #
# # # # 
Ending rollout after 20 steps, with total reward 3.77354
Total reward = 3.58486
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 3.58486 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = 14.1812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 14.1812 (1)
a=5 : 0 (0)
a=6 : 3.58486 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 19.025
Total reward = 18.0737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 14.1812 (1)
a=5 : 18.0737 (1)
a=6 : 3.58486 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -3.19751
Total reward = -3.03763
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -3.03763 (1)
a=3 : -1e+10 (1000000)
a=4 : 14.1812 (1)
a=5 : 18.0737 (1)
a=6 : 3.58486 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 14 steps, with total reward -3.44033
Total reward = -3.26831
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -3.26831 (1)
a=2 : -3.03763 (1)
a=3 : -1e+10 (1000000)
a=4 : 14.1812 (1)
a=5 : 18.0737 (1)
a=6 : 3.58486 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 25.4247
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -3.26831 (1)
a=2 : -3.03763 (1)
a=3 : -1e+10 (1000000)
a=4 : 14.1812 (1)
a=5 : 21.7492 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 26.7628 (1)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=6 : 3.58486 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = -5.36709
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -3.26831 (1)
a=2 : -3.03763 (1)
a=3 : -1e+10 (1000000)
a=4 : 4.40706 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 4.87675 (1)
a=5 : 21.7492 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 26.7628 (1)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=6 : 3.58486 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -3.26831 (1)
a=2 : -3.03763 (1)
a=3 : -1e+10 (1000000)
a=4 : 4.40706 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 4.87675 (1)
a=5 : 17.0787 (3)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 26.7628 (1)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 8.14506 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : 3.58486 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 10.875 [3, 20]
Total reward: 7.16616 [-5.36709, 25.4247]
Policy after 8 simulations
MCTS Policy:
a=5 : 17.0787 (3)
a=5 o=1 a=4 : 26.7628 (1)
a=5 o=2 a=1 : 8.14506 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -3.26831 (1)
a=2 : -3.03763 (1)
a=3 : -1e+10 (1000000)
a=4 : 4.40706 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 4.87675 (1)
a=5 : 17.0787 (3)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 26.7628 (1)
a=5 o=1 a=5 : 0 (0)
a=5 o=1 a=6 : 0 (0)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 8.14506 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : 3.58486 (1)
Check 1

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 48 steps, with total reward 5.53036
Total reward = 15.2538
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.2538 (1)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 24.0403
Total reward = 22.8383
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.2538 (1)
a=5 : 0 (0)
a=6 : 22.8383 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.2538 (1)
a=5 : 8.14506 (1)
a=6 : 22.8383 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 25 steps, with total reward 16.5047
Total reward = 15.6795
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.6795 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.2538 (1)
a=5 : 8.14506 (1)
a=6 : 22.8383 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.6795 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.2538 (1)
a=5 : 8.14506 (1)
a=6 : 22.8383 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 14.6334
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.6795 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.2538 (1)
a=5 : 8.14506 (1)
a=6 : 18.7359 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 15.4036 (1)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=5 o=1 a=1 o=0 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 14.3343
Total reward = 12.9367
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.3081 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 13.6176 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.2538 (1)
a=5 : 8.14506 (1)
a=6 : 18.7359 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 15.4036 (1)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=1 a=4 o=0 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 16.9834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.3081 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 13.6176 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.1186 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 7.35092 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 8.14506 (1)
a=6 : 18.7359 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 15.4036 (1)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.375 [0, 1]
Rollout depth: 15 [4, 48]
Total reward: 14.1817 [6.98337, 22.8383]
Policy after 8 simulations
MCTS Policy:
a=6 : 18.7359 (2)
a=6 o=1 a=4 : 15.4036 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.3081 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 13.6176 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.1186 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 7.35092 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 8.14506 (1)
a=6 : 18.7359 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : 15.4036 (1)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
Matched 1 states
Created 1 local transformations out of 3 attempts
Expanding node: a=5 o=1 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 11.1646
Total reward = 10.6063
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 10.6063 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 5.76482
Total reward = 5.47658
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.47658 (1)
a=6 : 10.6063 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 17.5987
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.47658 (1)
a=6 : 10.6063 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 16.3759
Total reward = 15.5571
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 15.5571 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.47658 (1)
a=6 : 10.6063 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 13.5487
Total reward = 22.8712
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 15.5571 (1)
a=3 : -1e+10 (1000000)
a=4 : 22.8712 (1)
a=5 : 5.47658 (1)
a=6 : 10.6063 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 17.7378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.5987 (1)
a=2 : 15.5571 (1)
a=3 : -1e+10 (1000000)
a=4 : 20.3045 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 8.14506 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 5.47658 (1)
a=6 : 10.6063 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=1 o=0 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.5494 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 15.5571 (1)
a=3 : -1e+10 (1000000)
a=4 : 20.3045 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 8.14506 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 5.47658 (1)
a=6 : 10.6063 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=2 o=0 
Adding sample:

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward -2.58638
Total reward = -2.33421
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.5494 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.61146 (2)
a=2 o=0 a=0 : -2.45706 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 20.3045 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 8.14506 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 5.47658 (1)
a=6 : 10.6063 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 10.4286 [3, 18]
Total reward: 12.1267 [-2.33421, 22.8712]
Policy after 8 simulations
MCTS Policy:
a=4 : 20.3045 (2)
a=4 o=0 a=1 : 8.14506 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.5494 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.61146 (2)
a=2 o=0 a=0 : -2.45706 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 20.3045 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 8.14506 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 0 (0)
a=5 : 5.47658 (1)
a=6 : 10.6063 (1)
Sample

# # # # 
# * 1$#
# . . #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward -1.85494
Total reward = -1.76219
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.76219 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 10.5648
Total reward = 10.0365
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.76219 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.0365 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 3.40562
Total reward = 3.23534
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.76219 (1)
a=2 : 3.23534 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.0365 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.76219 (1)
a=2 : 3.23534 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.33536 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 6.98337 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.76219 (1)
a=2 : 6.13017 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.33536 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 6.98337 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 19.5
Total reward = 17.5987
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.76219 (1)
a=2 : 6.13017 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.4232 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 18.525 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 6.98337 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward -2.26219
Total reward = -2.04163
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.76219 (1)
a=2 : 6.13017 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.05696 (4)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 18.525 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : -2.14908 (1)
a=6 o=2 a=2 : 6.98337 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward -0.367546
Total reward = -0.33171
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.04695 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -0.349169 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.13017 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.05696 (4)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 18.525 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : -2.14908 (1)
a=6 o=2 a=2 : 6.98337 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.625 [0, 1]
Rollout depth: 8.25 [1, 22]
Total reward: 5.29928 [-2.04163, 17.5987]
Policy after 8 simulations
MCTS Policy:
a=6 : 8.05696 (4)
a=6 o=1 a=1 : 18.525 (1)
a=6 o=2 a=2 : 6.98337 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.04695 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -0.349169 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.13017 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.05696 (4)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 18.525 (1)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : -2.14908 (1)
a=6 o=2 a=2 : 6.98337 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
Matched 1 states
Created 1 local transformations out of 4 attempts
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.24169 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.35092 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.57375 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 18.525
Total reward = 16.7188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.24169 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.35092 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 12.6463 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.24169 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.35092 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 7.50618 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 12.6463 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 16.7628
Total reward = 15.1284
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.24169 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.35092 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 7.50618 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 13.4737 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 15.9247 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.5987 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.24169 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.35092 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 7.50618 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 12.3615 (4)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 9.5 (1)
a=6 o=1 a=2 : 15.9247 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.5987 (1)
Tree depth: 0.625 [0, 1]
Rollout depth: 3.875 [1, 10]
Total reward: 10.1177 [5.98737, 16.7188]
Policy after 8 simulations
MCTS Policy:
a=6 : 12.3615 (4)
a=6 o=1 a=6 : 17.5987 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.24169 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.35092 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 7.50618 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 12.3615 (4)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 9.5 (1)
a=6 o=1 a=2 : 15.9247 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 17.5987 (1)
Check 2

# # # # 
# * 1$#
# . . #
# # # # 
Observed good
Reward 0
Matched 3 states
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.40127
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 6.48751 (2)
a=2 o=0 a=0 : 4.63291 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.98337 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 6.48751 (2)
a=2 o=0 a=0 : 4.63291 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.36059 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 8.14506 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.22378 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.48751 (2)
a=2 o=0 a=0 : 4.63291 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 7.36059 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 8.14506 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.1812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.22378 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.48751 (2)
a=2 o=0 a=0 : 4.63291 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.3008 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 8.14506 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 4.40127 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 16.8509
Total reward = 15.208
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.88517 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 16.0084 (1)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.48751 (2)
a=2 o=0 a=0 : 4.63291 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.3008 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 8.14506 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 4.40127 (1)
Tree depth: 0.625 [0, 1]
Rollout depth: 8 [3, 16]
Total reward: 7.69161 [4.1812, 15.208]
Policy after 8 simulations
MCTS Policy:
a=1 : 9.88517 (3)
a=1 o=0 a=2 : 16.0084 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.88517 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 16.0084 (1)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.48751 (2)
a=2 o=0 a=0 : 4.63291 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 6.3008 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 8.14506 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 4.40127 (1)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 18.5738 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.73781 (1)
a=4 : 18.5738 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 17.7378
Total reward = 16.8509
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.73781 (1)
a=4 : 18.5738 (1)
a=5 : -1e+10 (1000000)
a=6 : 16.8509 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 13.3383
Total reward = 12.6714
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 12.6714 (1)
a=3 : 7.73781 (1)
a=4 : 18.5738 (1)
a=5 : -1e+10 (1000000)
a=6 : 16.8509 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 12.6714 (1)
a=3 : 7.73781 (1)
a=4 : 18.5738 (1)
a=5 : -1e+10 (1000000)
a=6 : 16.8509 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=4 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = -3.3658
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 12.6714 (1)
a=3 : 7.73781 (1)
a=4 : 7.60398 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 6.98337 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 16.8509 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=6 o=1 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 18.525
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 12.6714 (1)
a=3 : 7.73781 (1)
a=4 : 7.60398 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 6.98337 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.688 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 19.5 (1)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.65279 (2)
a=2 o=0 a=0 : 6.98337 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : 7.73781 (1)
a=4 : 7.60398 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 6.98337 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.688 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 19.5 (1)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.375 [0, 1]
Rollout depth: 5.71429 [1, 11]
Total reward: 10.9534 [-3.3658, 18.5738]
Policy after 8 simulations
MCTS Policy:
a=6 : 17.688 (2)
a=6 o=1 a=4 : 19.5 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.65279 (2)
a=2 o=0 a=0 : 6.98337 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : 7.73781 (1)
a=4 : 7.60398 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 6.98337 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 17.688 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 19.5 (1)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Check 2

# # # # 
# . * #
# . . #
# # # # 
Observed good
Reward 0
Matched 1 states
Created 1 local transformations out of 2 attempts
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=6 o=1 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 12.2899
Total reward = 11.6754
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 11.6754 (1)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 15.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 11.6754 (1)
a=3 : 0 (0)
a=4 : 15.688 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 16.6342
Total reward = 15.8025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 11.6754 (1)
a=3 : 0 (0)
a=4 : 15.688 (1)
a=5 : -1e+10 (1000000)
a=6 : 15.8025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 11.6754 (1)
a=3 : 8.14506 (1)
a=4 : 15.688 (1)
a=5 : -1e+10 (1000000)
a=6 : 15.8025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 11.6754 (1)
a=3 : 8.14506 (1)
a=4 : 15.688 (1)
a=5 : -1e+10 (1000000)
a=6 : 15.8025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=6 o=1 a=6 o=1 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.40127
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 11.6754 (1)
a=3 : 8.14506 (1)
a=4 : 15.688 (1)
a=5 : -1e+10 (1000000)
a=6 : 10.1019 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 4.63291 (1)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=6 o=1 a=4 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 11.6754 (1)
a=3 : 8.14506 (1)
a=4 : 17.3565 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 9.5 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.1019 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 4.63291 (1)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=6 o=1 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 12.975
Total reward = 11.71
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 11.6927 (2)
a=2 o=0 a=0 : 12.3263 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : 8.14506 (1)
a=4 : 17.3565 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 9.5 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.1019 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 4.63291 (1)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.375 [0, 1]
Rollout depth: 9.71429 [1, 17]
Total reward: 12.0559 [4.40127, 19.025]
Policy after 8 simulations
MCTS Policy:
a=4 : 17.3565 (2)
a=4 o=0 a=2 : 9.5 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 11.6927 (2)
a=2 o=0 a=0 : 12.3263 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : 8.14506 (1)
a=4 : 17.3565 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 9.5 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.1019 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 4.63291 (1)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=6 o=1 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 5.98737 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.025 (1)
a=3 : 5.98737 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.025 (1)
a=3 : 5.98737 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=6 o=1 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.3814 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 8.14506 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 5.98737 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=5 o=1 a=6 o=1 a=4 o=0 a=6 o=1 a=6 o=1 a=1 o=0 a=6 o=1 a=4 o=0 a=3 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.3814 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 8.14506 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 7.06622 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 8.57375 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.3814 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 8.14506 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 7.06622 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 8.57375 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.75427 (3)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 8.14506 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 7.06622 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 8.57375 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0.375 [0, 1]
Rollout depth: 4.75 [2, 10]
Total reward: 8.79941 [5.98737, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.75427 (3)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 8.14506 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 7.06622 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 8.57375 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 22.6426, average = 11.7297
Undiscounted return = 30, average = 14.2857
Starting run 8 with 8 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -1.51086
Total reward = -1.43532
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1.43532 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : -1.43532 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 16.7628
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 16.7628 (1)
a=6 : -1.43532 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 17.7378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 16.7628 (1)
a=6 : -1.43532 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 16.7628 (1)
a=6 : -1.43532 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 16.7628
Total reward = 5.12844
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : 11.4331 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 15.9247 (1)
a=5 : 16.7628 (1)
a=6 : -1.43532 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = -3.19751
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : 11.4331 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 15.9247 (1)
a=5 : 6.78265 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -3.3658 (1)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : -1.43532 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : 11.4331 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 15.9247 (1)
a=5 : 6.78265 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -3.3658 (1)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : -1.43532 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 5.625 [1, 10]
Total reward: 7.43858 [-3.19751, 17.7378]
Policy after 8 simulations
MCTS Policy:
a=4 : 11.4331 (2)
a=4 o=0 a=6 : 15.9247 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : 11.4331 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 15.9247 (1)
a=5 : 6.78265 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -3.3658 (1)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : -1.43532 (1)
Sample

# # # # 
# * 1$#
# . . #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward -2.10662
Total reward = -2.00129
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -2.00129 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -2.00129 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 34 steps, with total reward 4.47546
Total reward = 4.25169
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.25169 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -2.00129 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.25169 (1)
a=2 : 7.94144 (2)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -2.00129 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.11953 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.30249 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 7.94144 (2)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -2.00129 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 27 steps, with total reward 2.6352
Total reward = 2.37827
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.11953 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.30249 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.08705 (3)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -2.00129 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1X#
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.11953 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.30249 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.08705 (3)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 2.86826 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 8.14506 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 25 steps, with total reward 11.065
Total reward = 9.98612
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.74173 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.30249 (1)
a=1 o=0 a=3 : 10.5117 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.08705 (3)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 2.86826 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 8.14506 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.625 [0, 1]
Rollout depth: 15.25 [3, 34]
Total reward: 5.52785 [-2.00129, 9.98612]
Policy after 8 simulations
MCTS Policy:
a=1 : 6.74173 (3)
a=1 o=0 a=3 : 10.5117 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.74173 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.30249 (1)
a=1 o=0 a=3 : 10.5117 (1)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 6.08705 (3)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 2.86826 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 0 (0)
a=6 o=2 a=2 : 8.14506 (1)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : -1e+10 (1000000)
a=6 o=2 a=5 : -1e+10 (1000000)
a=6 o=2 a=6 : 0 (0)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 6.98337 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 6.98337 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 6.98337 (1)
a=4 : 0 (0)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 6.98337 (1)
a=4 : 19.025 (1)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.57375 (1)
a=3 : 6.98337 (1)
a=4 : 19.025 (1)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=4 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 19.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.57375 (1)
a=3 : 6.98337 (1)
a=4 : 19.2625 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 16.9834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.57375 (1)
a=3 : 6.98337 (1)
a=4 : 18.5028 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 7.35092 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.57375 (1)
a=3 : 6.98337 (1)
a=4 : 18.5028 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 7.35092 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 4 [2, 7]
Total reward: 12.5113 [6.98337, 19.5]
Policy after 8 simulations
MCTS Policy:
a=4 : 18.5028 (3)
a=4 o=0 a=1 : 10 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.57375 (1)
a=3 : 6.98337 (1)
a=4 : 18.5028 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 7.35092 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.025 (1)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.14506 (1)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.14506 (1)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=4 o=0 a=3 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = 3.97214
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.14506 (1)
a=3 : 6.27295 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 4.1812 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.82253 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 6.27295 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 4.1812 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.82253 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 6.27295 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 4.1812 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.09309 (3)
a=2 o=0 a=0 : 6.98337 (1)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 6.27295 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 4.1812 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0.375 [0, 1]
Rollout depth: 7.75 [3, 17]
Total reward: 8.35314 [3.97214, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.09309 (3)
a=2 o=0 a=0 : 6.98337 (1)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 6.27295 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 4.1812 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 27.5987, average = 13.7133
Undiscounted return = 30, average = 16.25
Starting run 9 with 8 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 14.6334
Total reward = 13.9017
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 13.9017 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward -1.67408
Total reward = -1.59038
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 13.9017 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 20 steps, with total reward -4.37153
Total reward = -4.15295
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -4.15295 (1)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 13.9017 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -4.15295 (1)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 13.9017 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$1$#
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -4.15295 (1)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 13.9017 (1)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = -4.86658
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -4.15295 (1)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : 7.07921 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 5.4036 (1)
a=5 : 13.9017 (1)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 12 steps, with total reward -11.5498
Total reward = -10.4237
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -4.15295 (1)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : 7.07921 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 5.4036 (1)
a=5 : 1.73902 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -10.9723 (1)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : 7.73781 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Expanding node: a=6 o=2 
Adding sample:

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -4.15295 (1)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : 7.07921 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 5.4036 (1)
a=5 : 1.73902 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -10.9723 (1)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : 6.43562 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 5.4036 (1)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : 0 (0)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : 0 (0)
Tree depth: 0.375 [0, 1]
Rollout depth: 10.5 [2, 20]
Total reward: 3.09555 [-10.4237, 19.025]
Policy after 8 simulations
MCTS Policy:
a=4 : 7.07921 (2)
a=4 o=0 a=6 : 5.4036 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -4.15295 (1)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : 7.07921 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : 5.4036 (1)
a=5 : 1.73902 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -10.9723 (1)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : 6.43562 (2)
a=6 o=2 a=0 : -1e+10 (1000000)
a=6 o=2 a=1 : 5.4036 (1)
a=6 o=2 a=2 : 0 (0)
a=6 o=2 a=3 : -1e+10 (1000000)
a=6 o=2 a=4 : 0 (0)
a=6 o=2 a=5 : 0 (0)
a=6 o=2 a=6 : 0 (0)
Sample

# # # # 
# * 1$#
# . . #
# # # # 
Reward -10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 11.391
Total reward = 10.8214
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.8214 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 27 steps, with total reward 2.6352
Total reward = 2.50344
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.8214 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=4 o=0 a=6 o=1 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 11.7061
Total reward = 10.5648
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.6931 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 11.1208 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 10.6931 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 11.1208 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 22 steps, with total reward 3.40562
Total reward = 3.07357
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 2.50344 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.15325 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 3.23534 (1)
a=6 o=1 a=2 : 11.1208 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . 1$#
# * . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.32425 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.15325 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 3.23534 (1)
a=6 o=1 a=2 : 11.1208 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.75427 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 8.14506 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.32425 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.15325 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 3.23534 (1)
a=6 o=1 a=2 : 11.1208 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.625 [0, 1]
Rollout depth: 12 [2, 27]
Total reward: 7.67138 [2.50344, 10.8214]
Policy after 8 simulations
MCTS Policy:
a=1 : 8.75427 (3)
a=1 o=0 a=1 : 10 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.75427 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 8.14506 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : -1e+10 (1000000)
a=1 o=0 a=6 : 0 (0)
a=2 : 5.32425 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 8.15325 (3)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 3.23534 (1)
a=6 o=1 a=2 : 11.1208 (1)
a=6 o=1 a=3 : -1e+10 (1000000)
a=6 o=1 a=4 : -1e+10 (1000000)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 0 (0)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 16.3025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 16.3025 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : 16.3025 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . 1$#
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : 16.3025 (1)
a=5 : -1e+10 (1000000)
a=6 : 0 (0)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : 16.3025 (1)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : 16.3025 (1)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=4 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 19.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : 17.9012 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : 17.9012 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 9.5 (1)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=6 o=1 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 15.0887
Total reward = 13.6176
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : 17.9012 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.5588 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 14.3343 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 4.8 [1, 9]
Total reward: 11.9714 [7.35092, 19.5]
Policy after 8 simulations
MCTS Policy:
a=4 : 17.9012 (2)
a=4 o=0 a=1 : 10 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 7.35092 (1)
a=4 : 17.9012 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 10 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : 11.5588 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 0 (0)
a=6 o=1 a=5 : -1e+10 (1000000)
a=6 o=1 a=6 : 14.3343 (1)
Sample

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.63291
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 4.63291 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 4.63291 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 4.63291 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 4.63291 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.42546 (2)
a=2 o=0 a=0 : 7.73781 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 4.63291 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=4 o=0 a=3 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 20 steps, with total reward 3.77354
Total reward = 3.40562
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.42546 (2)
a=2 o=0 a=0 : 7.73781 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 4.01926 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 3.58486 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.42546 (2)
a=2 o=0 a=0 : 7.73781 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 4.01926 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 3.58486 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.47489 (3)
a=2 o=0 a=0 : 7.73781 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 9.025 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 4.01926 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 3.58486 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
Tree depth: 0.375 [0, 1]
Rollout depth: 8.6 [1, 20]
Total reward: 7.9329 [3.40562, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.47489 (3)
a=2 o=0 a=0 : 7.73781 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 9.025 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 4.01926 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 3.58486 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
a=6 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 7.59875, average = 13.0339
Undiscounted return = 10, average = 15.5556
Starting run 10 with 8 simulations... 
Expanding node: 

# # # # 
# * 1$#
# . . #
# # # # 
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 18.0737
Total reward = 17.1701
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.1701 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.1701 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.73781 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward -0.367546
Total reward = -10.3492
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.1701 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.3492 (1)
a=5 : 7.73781 (1)
a=6 : 0 (0)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 18 steps, with total reward 2.74589
Total reward = 2.60859
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 17.1701 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.3492 (1)
a=5 : 7.73781 (1)
a=6 : 2.60859 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X1X#
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.1517 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 5.4036 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -10.3492 (1)
a=5 : 7.73781 (1)
a=6 : 2.60859 (1)
Starting simulation

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$1X#
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0$1X#
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -0.428688
Total reward = -0.38689
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.1517 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 5.4036 (1)
a=2 : 4.31905 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -0.407253 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.3492 (1)
a=5 : 7.73781 (1)
a=6 : 2.60859 (1)
Starting simulation

# # # # 
# * 1$#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * 1$#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1$#
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
E
Reward 0

# # # # 
# . 1$#
# . * #
# # # # 
W
Reward 0

# # # # 
# . 1$#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1$#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = -3.19751
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.1517 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 5.4036 (1)
a=2 : 4.31905 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -0.407253 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.3492 (1)
a=5 : 2.27015 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -3.3658 (1)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : 2.60859 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 7.75 [2, 18]
Total reward: 3.46767 [-10.3492, 17.1701]
Policy after 8 simulations
MCTS Policy:
a=1 : 11.1517 (2)
a=1 o=0 a=6 : 5.4036 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.1517 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : 0 (0)
a=1 o=0 a=5 : 0 (0)
a=1 o=0 a=6 : 5.4036 (1)
a=2 : 4.31905 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -0.407253 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=2 o=0 a=6 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -10.3492 (1)
a=5 : 2.27015 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -3.3658 (1)
a=5 o=2 a=5 : 0 (0)
a=5 o=2 a=6 : 0 (0)
a=6 : 2.60859 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 16.8509
Total reward = 16.0084
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : 0 (0)
a=6 : 16.0084 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward -1.2306
Total reward = -1.16907
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : 0 (0)
a=5 : -1.16907 (1)
a=6 : 16.0084 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
S
Reward 0

# # # # 
# . 1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward -2.51663
Total reward = -2.3908
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -2.3908 (1)
a=4 : 0 (0)
a=5 : -1.16907 (1)
a=6 : 16.0084 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 17.2378
Total reward = 26.3759
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -2.3908 (1)
a=4 : 26.3759 (1)
a=5 : -1.16907 (1)
a=6 : 16.0084 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -2.3908 (1)
a=4 : 26.3759 (1)
a=5 : -1.16907 (1)
a=6 : 16.0084 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X1X#
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X1X#
# * . #
# # # # 
N
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
Check 2
Observed bad
Reward 0

# # # # 
# * 1X#
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward -1.36355
Total reward = -1.29537
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.29537 (1)
a=3 : -2.3908 (1)
a=4 : 26.3759 (1)
a=5 : -1.16907 (1)
a=6 : 16.0084 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=1 o=0 a=4 o=0 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.29537 (1)
a=3 : -2.3908 (1)
a=4 : 22.7005 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 9.5 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : 0 (0)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1.16907 (1)
a=6 : 16.0084 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 2
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=1 o=0 a=6 o=1 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 18.525
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.29537 (1)
a=3 : -2.3908 (1)
a=4 : 22.7005 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 9.5 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : 0 (0)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1.16907 (1)
a=6 : 17.2667 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 19.5 (1)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 6.71429 [1, 13]
Total reward: 10.6349 [-2.3908, 26.3759]
Policy after 8 simulations
MCTS Policy:
a=4 : 22.7005 (2)
a=4 o=0 a=2 : 9.5 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -1.29537 (1)
a=3 : -2.3908 (1)
a=4 : 22.7005 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 9.5 (1)
a=4 o=0 a=3 : 0 (0)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : 0 (0)
a=4 o=0 a=6 : -1e+10 (1000000)
a=5 : -1.16907 (1)
a=6 : 17.2667 (2)
a=6 o=1 a=0 : -1e+10 (1000000)
a=6 o=1 a=1 : 0 (0)
a=6 o=1 a=2 : 0 (0)
a=6 o=1 a=3 : 0 (0)
a=6 o=1 a=4 : 19.5 (1)
a=6 o=1 a=5 : 0 (0)
a=6 o=1 a=6 : 0 (0)
Sample

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=4 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.57375 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 20 steps, with total reward -4.37153
Total reward = -4.15295
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.57375 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -4.15295 (1)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.57375 (1)
a=3 : 9.025 (1)
a=4 : -1e+10 (1000000)
a=5 : -4.15295 (1)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.57375 (1)
a=3 : 9.025 (1)
a=4 : -1e+10 (1000000)
a=5 : -4.15295 (1)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=1 o=0 a=4 o=0 a=3 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.57375 (1)
a=3 : 8.18796 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 7.73781 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 0 (0)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -4.15295 (1)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Expanding node: a=1 o=0 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.79938 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 9.5 (1)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 8.18796 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 7.73781 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 0 (0)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -4.15295 (1)
a=6 : -1e+10 (1000000)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.79938 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 9.5 (1)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 8.18796 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 7.73781 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 0 (0)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -4.15295 (1)
a=6 : -1e+10 (1000000)
Tree depth: 0.25 [0, 1]
Rollout depth: 6.2 [1, 20]
Total reward: 7.47771 [-4.15295, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.79938 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 9.5 (1)
a=2 o=0 a=6 : -1e+10 (1000000)
a=3 : 8.18796 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 7.73781 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 0 (0)
a=3 o=0 a=6 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -4.15295 (1)
a=6 : -1e+10 (1000000)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 18.525, average = 13.583
Undiscounted return = 20, average = 16
Simulations = 8
Runs = 10
Undiscounted return = 16 +- 2.52982
Discounted return = 13.583 +- 2.30141
Time = 0.0070722
