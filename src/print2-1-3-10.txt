Initialising fast UCB table... done
Main runs
Starting run 1 with 1 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = -0.975
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: -0.975 [-0.975, -0.975]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 0 (0)
S

# # # # 
# 0X. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 7.73781 [7.73781, 7.73781]
Policy after 1 simulations
MCTS Policy:
a=1 : 7.73781 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 7.73781 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
E

# # # # 
# 0X. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=3 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
W

# # # # 
# 0X. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 8.14506 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=0 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : 8.14506 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=3 o=0 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.35092 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=2 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.35092 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
S

# # # # 
# 0X. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=3 o=0 a=0 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
E

# # # # 
# 0X. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=3 o=0 a=0 o=0 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
E

# # # # 
# 0X. #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 7.35092, average = 7.35092
Undiscounted return = 10, average = 10
Starting run 2 with 1 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward -2.15769
Total reward = -2.04981
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -2.04981 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11 [11, 11]
Total reward: -2.04981 [-2.04981, -2.04981]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -2.04981 (1)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.5, average = 8.42546
Undiscounted return = 10, average = 10
Starting run 3 with 1 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = -0.975
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: -0.975 [-0.975, -0.975]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 0 (0)
Check 1

# # # # 
# * . #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=2 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
S

# # # # 
# 0$. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=2 o=0 
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 8.14506 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=0 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : 8.14506 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=2 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
S

# # # # 
# 0$. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=2 o=0 a=0 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 20 steps, with total reward 3.77354
Total reward = 3.58486
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 3.58486 (1)
Tree depth: 0 [0, 0]
Rollout depth: 20 [20, 20]
Total reward: 3.58486 [3.58486, 3.58486]
Policy after 1 simulations
MCTS Policy:
a=5 : 3.58486 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 3.58486 (1)
Check 1

# # # # 
# 0$. #
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=2 o=0 a=0 o=0 a=2 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : 0 (0)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: 6.98337 [6.98337, 6.98337]
Policy after 1 simulations
MCTS Policy:
a=1 : 6.98337 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
E

# # # # 
# 0$. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=2 o=0 a=0 o=0 a=2 o=0 a=5 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 9.025 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.025 [9.025, 9.025]
Policy after 1 simulations
MCTS Policy:
a=3 : 9.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 9.025 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
W

# # # # 
# 0$. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=2 o=0 a=0 o=0 a=2 o=0 a=5 o=1 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 7.73781 [7.73781, 7.73781]
Policy after 1 simulations
MCTS Policy:
a=5 : 7.73781 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.73781 (1)
Check 1

# # # # 
# 0$. #
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=2 o=0 a=0 o=0 a=2 o=0 a=5 o=1 a=1 o=0 a=3 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 0 (0)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.025 [9.025, 9.025]
Policy after 1 simulations
MCTS Policy:
a=1 : 9.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 9.025 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
E

# # # # 
# 0$. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=2 o=0 a=0 o=0 a=2 o=0 a=5 o=1 a=1 o=0 a=3 o=0 a=5 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : 5.98737 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 10 [10, 10]
Total reward: 5.98737 [5.98737, 5.98737]
Policy after 1 simulations
MCTS Policy:
a=0 : 5.98737 (1)
Values after 1 simulations
MCTS Values:
a=0 : 5.98737 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
N

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=2 o=0 a=0 o=0 a=2 o=0 a=5 o=1 a=1 o=0 a=3 o=0 a=5 o=2 a=1 o=0 a=0 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=5 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Check 1

# # # # 
# 0$* #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=1 a=2 o=0 a=0 o=0 a=2 o=0 a=5 o=1 a=1 o=0 a=3 o=0 a=5 o=2 a=1 o=0 a=0 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 5.688, average = 7.51297
Undiscounted return = 10, average = 10
Starting run 4 with 1 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=2 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
S

# # # # 
# 0X. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward -0.835941
Total reward = -0.794144
MCTS Values:
a=0 : -0.794144 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: -0.794144 [-0.794144, -0.794144]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -0.794144 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
E

# # # # 
# 0X. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 25 steps, with total reward 7.79664
Total reward = 7.40681
MCTS Values:
a=0 : 7.40681 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 25 [25, 25]
Total reward: 7.40681 [7.40681, 7.40681]
Policy after 1 simulations
MCTS Policy:
a=0 : 7.40681 (1)
Values after 1 simulations
MCTS Values:
a=0 : 7.40681 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
N

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=0 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=3 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
W

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=0 o=0 a=3 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 16.9834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: 16.9834 [16.9834, 16.9834]
Policy after 1 simulations
MCTS Policy:
a=4 : 16.9834 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.9834 (1)
a=5 : 0 (0)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=0 o=0 a=3 o=0 a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 12 [12, 12]
Total reward: 5.4036 [5.4036, 5.4036]
Policy after 1 simulations
MCTS Policy:
a=2 : 5.4036 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
S

# # # # 
# . . #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=0 o=0 a=3 o=0 a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 9.025 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.025 [9.025, 9.025]
Policy after 1 simulations
MCTS Policy:
a=0 : 9.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : 9.025 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=0 o=0 a=3 o=0 a=4 o=0 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.025 [9.025, 9.025]
Policy after 1 simulations
MCTS Policy:
a=1 : 9.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=0 o=0 a=3 o=0 a=4 o=0 a=2 o=0 a=0 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = 4.1812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 4.1812 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 17 [17, 17]
Total reward: 4.1812 [4.1812, 4.1812]
Policy after 1 simulations
MCTS Policy:
a=3 : 4.1812 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 4.1812 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
W

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=0 o=0 a=3 o=0 a=4 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.025 [9.025, 9.025]
Policy after 1 simulations
MCTS Policy:
a=2 : 9.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
S

# # # # 
# . . #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=0 o=0 a=3 o=0 a=4 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 a=2 o=0 
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : 5.98737 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 10 [10, 10]
Total reward: 5.98737 [5.98737, 5.98737]
Policy after 1 simulations
MCTS Policy:
a=0 : 5.98737 (1)
Values after 1 simulations
MCTS Values:
a=0 : 5.98737 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=0 o=0 a=3 o=0 a=4 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=0 o=0 a=3 o=0 a=4 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=2 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
S

# # # # 
# . . #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=0 o=0 a=3 o=0 a=4 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=2 o=0 
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 7.73781 [7.73781, 7.73781]
Policy after 1 simulations
MCTS Policy:
a=0 : 7.73781 (1)
Values after 1 simulations
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
N

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=0 o=0 a=3 o=0 a=4 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.35092 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=2 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.35092 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
S

# # # # 
# . . #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=0 o=0 a=3 o=0 a=4 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=2 o=0 a=0 o=0 a=2 o=0 
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 9.5 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=0 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : 9.5 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
N

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=1 o=0 a=0 o=0 a=3 o=0 a=4 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=2 o=0 a=0 o=0 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = -3.7438, average = 4.69878
Undiscounted return = 0, average = 7.5
Starting run 5 with 1 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 16.7628
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.7628 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 16.7628 [16.7628, 16.7628]
Policy after 1 simulations
MCTS Policy:
a=1 : 16.7628 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.7628 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.5, average = 5.65902
Undiscounted return = 10, average = 8
Starting run 6 with 1 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 7.73781 [7.73781, 7.73781]
Policy after 1 simulations
MCTS Policy:
a=5 : 7.73781 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.73781 (1)
Check 1

# # # # 
# * . #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = -2.64908
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: -2.64908 [-2.64908, -2.64908]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
Check 1

# # # # 
# * . #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 0 local transformations out of 1000 attempts
Out of particles, finishing episode with SelectRandom
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
S

# # # # 
# 0X. #
# . * #
# # # # 
Reward 0
Check 1

# # # # 
# 0X. #
# . * #
# # # # 
Observed bad
Reward 0
W

# # # # 
# 0X. #
# * . #
# # # # 
Reward 0
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward -10
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
W

# # # # 
# * . #
# . . #
# # # # 
Reward 0
S

# # # # 
# . . #
# * . #
# # # # 
Reward 0
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
S

# # # # 
# . . #
# * . #
# # # # 
Reward 0
E

# # # # 
# . . #
# . * #
# # # # 
Reward 0
W

# # # # 
# . . #
# * . #
# # # # 
Reward 0
E

# # # # 
# . . #
# . * #
# # # # 
Reward 0
N

# # # # 
# . * #
# . . #
# # # # 
Reward 0
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = -2.80217, average = 4.24883
Undiscounted return = 0, average = 6.66667
Starting run 7 with 1 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = -0.975
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: -0.975 [-0.975, -0.975]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.35092 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=2 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.35092 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
S

# # # # 
# 0$. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 6.6342 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 8 [8, 8]
Total reward: 6.6342 [6.6342, 6.6342]
Policy after 1 simulations
MCTS Policy:
a=3 : 6.6342 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 6.6342 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
W

# # # # 
# 0$. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=2 o=0 a=3 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
E

# # # # 
# 0$. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=2 o=0 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 1 simulations
MCTS Policy:
a=5 : 8.57375 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
Check 1

# # # # 
# 0$. #
# . * #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=2 o=0 a=3 o=0 a=1 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
E

# # # # 
# 0$. #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 7.73781, average = 4.74725
Undiscounted return = 10, average = 7.14286
Starting run 8 with 1 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 13.6812
Total reward = 12.9971
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 12.9971 (1)
Tree depth: 0 [0, 0]
Rollout depth: 18 [18, 18]
Total reward: 12.9971 [12.9971, 12.9971]
Policy after 1 simulations
MCTS Policy:
a=5 : 12.9971 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 12.9971 (1)
Check 1

# # # # 
# * . #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.025 [9.025, 9.025]
Policy after 1 simulations
MCTS Policy:
a=2 : 9.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
S

# # # # 
# 0X. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.35092 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=5 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.35092 (1)
Check 1

# # # # 
# 0X. #
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=2 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 6.98337 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: 6.98337 [6.98337, 6.98337]
Policy after 1 simulations
MCTS Policy:
a=5 : 6.98337 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 6.98337 (1)
Check 1

# # # # 
# 0X. #
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 8 attempts
Expanding node: a=5 o=2 a=2 o=0 a=5 o=2 a=5 o=2 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 15.188
Total reward = 14.4286
MCTS Values:
a=0 : 14.4286 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 12 [12, 12]
Total reward: 14.4286 [14.4286, 14.4286]
Policy after 1 simulations
MCTS Policy:
a=0 : 14.4286 (1)
Values after 1 simulations
MCTS Values:
a=0 : 14.4286 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 8 [8, 8]
Total reward: 6.6342 [6.6342, 6.6342]
Policy after 1 simulations
MCTS Policy:
a=2 : 6.6342 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
S

# # # # 
# 0X. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 6.98337 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: 6.98337 [6.98337, 6.98337]
Policy after 1 simulations
MCTS Policy:
a=5 : 6.98337 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 6.98337 (1)
Check 1

# # # # 
# 0X. #
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 a=2 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=5 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
Check 1

# # # # 
# 0X. #
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 172 attempts
Expanding node: a=5 o=2 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=5 o=2 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=0 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : 7.35092 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.40127
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 16 [16, 16]
Total reward: 4.40127 [4.40127, 4.40127]
Policy after 1 simulations
MCTS Policy:
a=2 : 4.40127 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
S

# # # # 
# 0X. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 5.98737 (1)
Tree depth: 0 [0, 0]
Rollout depth: 10 [10, 10]
Total reward: 5.98737 [5.98737, 5.98737]
Policy after 1 simulations
MCTS Policy:
a=5 : 5.98737 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 5.98737 (1)
Check 1

# # # # 
# 0X. #
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 a=2 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward -1.28719
Total reward = -1.22283
MCTS Values:
a=0 : 0 (0)
a=1 : -1.22283 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: -1.22283 [-1.22283, -1.22283]
Policy after 1 simulations
MCTS Policy:
a=0 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : -1.22283 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Check 1

# # # # 
# 0X. #
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 44 attempts
Expanding node: a=5 o=2 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=5 o=2 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.40127
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.40127 (1)
Tree depth: 0 [0, 0]
Rollout depth: 16 [16, 16]
Total reward: 4.40127 [4.40127, 4.40127]
Policy after 1 simulations
MCTS Policy:
a=5 : 4.40127 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 4.40127 (1)
Check 1

# # # # 
# 0X. #
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=5 o=2 a=5 o=2 
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=5 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
Check 1

# # # # 
# 0X. #
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 19 attempts
Expanding node: a=5 o=2 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=5 o=2 a=5 o=2 a=5 o=2 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 13.6534
Total reward = 12.9707
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 12.9707 (1)
Tree depth: 0 [0, 0]
Rollout depth: 10 [10, 10]
Total reward: 12.9707 [12.9707, 12.9707]
Policy after 1 simulations
MCTS Policy:
a=5 : 12.9707 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 12.9707 (1)
Check 1

# # # # 
# 0X. #
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=5 o=2 a=5 o=2 a=5 o=2 a=5 o=2 
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
E

# # # # 
# 0X. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=5 o=2 a=0 o=0 a=2 o=0 a=5 o=2 a=5 o=2 a=5 o=2 a=5 o=2 a=5 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
E

# # # # 
# 0X. #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 4.40127, average = 4.704
Undiscounted return = 10, average = 7.5
Starting run 9 with 1 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = -2.26219
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: -2.26219 [-2.26219, -2.26219]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 0 (0)
S

# # # # 
# 0$. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 19.025
Total reward = 18.0737
MCTS Values:
a=0 : 18.0737 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 18.0737 [18.0737, 18.0737]
Policy after 1 simulations
MCTS Policy:
a=0 : 18.0737 (1)
Values after 1 simulations
MCTS Values:
a=0 : 18.0737 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward -4.01263
Total reward = -3.812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -3.812 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11 [11, 11]
Total reward: -3.812 [-3.812, -3.812]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -3.812 (1)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=0 o=0 a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: 6.98337 [6.98337, 6.98337]
Policy after 1 simulations
MCTS Policy:
a=2 : 6.98337 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
S

# # # # 
# . . #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=0 o=0 a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 5.13342
MCTS Values:
a=0 : 0 (0)
a=1 : 5.13342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 13 [13, 13]
Total reward: 5.13342 [5.13342, 5.13342]
Policy after 1 simulations
MCTS Policy:
a=1 : 5.13342 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 5.13342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . . #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=0 o=0 a=4 o=0 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . . #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 16.7628, average = 6.04387
Undiscounted return = 20, average = 8.88889
Starting run 10 with 1 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.025 [9.025, 9.025]
Policy after 1 simulations
MCTS Policy:
a=1 : 9.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 16.7628
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 16.7628 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 16.7628 [16.7628, 16.7628]
Policy after 1 simulations
MCTS Policy:
a=5 : 16.7628 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 16.7628 (1)
Check 1

# # # # 
# 0$* #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=5 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Check 1

# # # # 
# 0$* #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=3 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
W

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = -1.85494
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.85494 (1)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: -1.85494 [-1.85494, -1.85494]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.85494 (1)
a=5 : 0 (0)
Check 1

# # # # 
# * . #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.025 [9.025, 9.025]
Policy after 1 simulations
MCTS Policy:
a=2 : 9.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
S

# # # # 
# 0$. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 a=2 o=0 
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 7.73781 [7.73781, 7.73781]
Policy after 1 simulations
MCTS Policy:
a=0 : 7.73781 (1)
Values after 1 simulations
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 7.73781 [7.73781, 7.73781]
Policy after 1 simulations
MCTS Policy:
a=1 : 7.73781 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 a=2 o=0 a=0 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -0.879938
Total reward = -0.835941
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -0.835941 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: -0.835941 [-0.835941, -0.835941]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -0.835941 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Check 1

# # # # 
# 0$* #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 a=2 o=0 a=0 o=0 a=1 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=3 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
W

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 a=2 o=0 a=0 o=0 a=1 o=0 a=5 o=1 a=3 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = -1.42625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: -1.42625 [-1.42625, -1.42625]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 0 (0)
S

# # # # 
# 0$. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 a=2 o=0 a=0 o=0 a=1 o=0 a=5 o=1 a=3 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : 6.30249 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 9 [9, 9]
Total reward: 6.30249 [6.30249, 6.30249]
Policy after 1 simulations
MCTS Policy:
a=0 : 6.30249 (1)
Values after 1 simulations
MCTS Values:
a=0 : 6.30249 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 a=2 o=0 a=0 o=0 a=1 o=0 a=5 o=1 a=3 o=0 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward -0.92625
Total reward = -0.879938
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.879938 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: -0.879938 [-0.879938, -0.879938]
Policy after 1 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.879938 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 a=2 o=0 a=0 o=0 a=1 o=0 a=5 o=1 a=3 o=0 a=2 o=0 a=0 o=0 a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 7.35092 [7.35092, 7.35092]
Policy after 1 simulations
MCTS Policy:
a=1 : 7.35092 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 a=2 o=0 a=0 o=0 a=1 o=0 a=5 o=1 a=3 o=0 a=2 o=0 a=0 o=0 a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 25 steps, with total reward 2.91989
Total reward = 2.7739
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 2.7739 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 25 [25, 25]
Total reward: 2.7739 [2.7739, 2.7739]
Policy after 1 simulations
MCTS Policy:
a=3 : 2.7739 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 2.7739 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
W

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 a=2 o=0 a=0 o=0 a=1 o=0 a=5 o=1 a=3 o=0 a=2 o=0 a=0 o=0 a=4 o=0 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = 4.1812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.1812 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 17 [17, 17]
Total reward: 4.1812 [4.1812, 4.1812]
Policy after 1 simulations
MCTS Policy:
a=2 : 4.1812 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.1812 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
S

# # # # 
# . . #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 a=2 o=0 a=0 o=0 a=1 o=0 a=5 o=1 a=3 o=0 a=2 o=0 a=0 o=0 a=4 o=0 a=1 o=0 a=3 o=0 a=2 o=0 
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 5.13342
MCTS Values:
a=0 : 5.13342 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 13 [13, 13]
Total reward: 5.13342 [5.13342, 5.13342]
Policy after 1 simulations
MCTS Policy:
a=0 : 5.13342 (1)
Values after 1 simulations
MCTS Values:
a=0 : 5.13342 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 a=2 o=0 a=0 o=0 a=1 o=0 a=5 o=1 a=3 o=0 a=2 o=0 a=0 o=0 a=4 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.5 [9.5, 9.5]
Policy after 1 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 a=2 o=0 a=0 o=0 a=1 o=0 a=5 o=1 a=3 o=0 a=2 o=0 a=0 o=0 a=4 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=3 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.14506 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
W

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 a=2 o=0 a=0 o=0 a=1 o=0 a=5 o=1 a=3 o=0 a=2 o=0 a=0 o=0 a=4 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 4 [4, 4]
Total reward: 8.14506 [8.14506, 8.14506]
Policy after 1 simulations
MCTS Policy:
a=2 : 8.14506 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
S

# # # # 
# . . #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 a=2 o=0 a=0 o=0 a=1 o=0 a=5 o=1 a=3 o=0 a=2 o=0 a=0 o=0 a=4 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 a=2 o=0 
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : 5.4036 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 12 [12, 12]
Total reward: 5.4036 [5.4036, 5.4036]
Policy after 1 simulations
MCTS Policy:
a=0 : 5.4036 (1)
Values after 1 simulations
MCTS Values:
a=0 : 5.4036 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 a=2 o=0 a=0 o=0 a=1 o=0 a=5 o=1 a=3 o=0 a=2 o=0 a=0 o=0 a=4 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 7 [7, 7]
Total reward: 6.98337 [6.98337, 6.98337]
Policy after 1 simulations
MCTS Policy:
a=1 : 6.98337 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 a=2 o=0 a=0 o=0 a=1 o=0 a=5 o=1 a=3 o=0 a=2 o=0 a=0 o=0 a=4 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.025 [9.025, 9.025]
Policy after 1 simulations
MCTS Policy:
a=2 : 9.025 (1)
Values after 1 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
S

# # # # 
# . . #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 a=5 o=1 a=3 o=0 a=5 o=1 a=2 o=0 a=0 o=0 a=1 o=0 a=5 o=1 a=3 o=0 a=2 o=0 a=0 o=0 a=4 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=3 o=0 a=2 o=0 a=0 o=0 a=1 o=0 a=2 o=0 
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 0 [1e+10, -1e+10]
Total reward: 10 [10, 10]
Policy after 1 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 1 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . . #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 8.47717, average = 6.2872
Undiscounted return = 20, average = 10
Simulations = 1
Runs = 10
Undiscounted return = 10 +- 2
Discounted return = 6.2872 +- 1.80435
Time = 0.0020692
Starting run 1 with 2 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 18.1451
Total reward = 17.2378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 17.2378 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 14.6329
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 14.6329 (1)
a=5 : 17.2378 (1)
Tree depth: 0 [0, 0]
Rollout depth: 10 [5, 15]
Total reward: 15.9354 [14.6329, 17.2378]
Policy after 2 simulations
MCTS Policy:
a=5 : 17.2378 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 14.6329 (1)
a=5 : 17.2378 (1)
Check 1

# # # # 
# * . #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward -2.51663
Total reward = -2.3908
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -2.3908 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = -4.312
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.312 (1)
a=5 : -2.3908 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9.5 [8, 11]
Total reward: -3.3514 [-4.312, -2.3908]
Policy after 2 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.312 (1)
a=5 : -2.3908 (1)
S

# # # # 
# 0X. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 0 (0)
a=1 : 6.6342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 23 steps, with total reward 3.23534
Total reward = 3.07357
MCTS Values:
a=0 : 0 (0)
a=1 : 6.6342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 3.07357 (1)
Tree depth: 0 [0, 0]
Rollout depth: 15.5 [8, 23]
Total reward: 4.85389 [3.07357, 6.6342]
Policy after 2 simulations
MCTS Policy:
a=1 : 6.6342 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 6.6342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 3.07357 (1)
E

# # # # 
# 0X. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 4 [3, 5]
Total reward: 8.15578 [7.73781, 8.57375]
Policy after 2 simulations
MCTS Policy:
a=3 : 8.57375 (1)
Values after 2 simulations
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
W

# # # # 
# 0X. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=2 o=0 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 15.4874
Total reward = 14.713
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 14.713 (1)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 20 steps, with total reward 11.1245
Total reward = 10.5682
MCTS Values:
a=0 : 0 (0)
a=1 : 10.5682 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 14.713 (1)
Tree depth: 0 [0, 0]
Rollout depth: 15.5 [11, 20]
Total reward: 12.6406 [10.5682, 14.713]
Policy after 2 simulations
MCTS Policy:
a=5 : 14.713 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10.5682 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 14.713 (1)
Check 1

# # # # 
# 0X. #
# * . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=2 o=0 a=1 o=0 a=3 o=0 a=5 o=2 
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -1.85494
Total reward = -1.76219
MCTS Values:
a=0 : -1.76219 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1.76219 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3 [1, 5]
Total reward: 3.8689 [-1.76219, 9.5]
Policy after 2 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1.76219 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
E

# # # # 
# 0X. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=2 o=0 a=1 o=0 a=3 o=0 a=5 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 9.28688 [8.57375, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
E

# # # # 
# 0X. #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 7.35092, average = 7.35092
Undiscounted return = 10, average = 10
Starting run 2 with 2 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward -3.69751
Total reward = -3.51263
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -3.51263 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7 [4, 10]
Total reward: 2.31622 [-3.51263, 8.14506]
Policy after 2 simulations
MCTS Policy:
a=2 : 8.14506 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -3.51263 (1)
S

# # # # 
# 0$. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward -0.835941
Total reward = -0.794144
MCTS Values:
a=0 : 0 (0)
a=1 : -0.794144 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4 [2, 6]
Total reward: 4.11543 [-0.794144, 9.025]
Policy after 2 simulations
MCTS Policy:
a=5 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : 0 (0)
a=1 : -0.794144 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
Check 1

# # # # 
# 0$. #
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 19 steps, with total reward 9.95951
Total reward = 9.46154
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.46154 (1)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 8.14506 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.46154 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11.5 [4, 19]
Total reward: 8.8033 [8.14506, 9.46154]
Policy after 2 simulations
MCTS Policy:
a=5 : 9.46154 (1)
Values after 2 simulations
MCTS Values:
a=0 : 8.14506 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.46154 (1)
Check 1

# # # # 
# 0$. #
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 139 attempts
Expanding node: a=2 o=0 a=5 o=1 a=5 o=1 
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward -2.04981
Total reward = -1.94732
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1.94732 (1)
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward -3.01663
Total reward = -2.8658
MCTS Values:
a=0 : -2.8658 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1.94732 (1)
Tree depth: 0 [0, 0]
Rollout depth: 10 [8, 12]
Total reward: -2.40656 [-2.8658, -1.94732]
Policy after 2 simulations
MCTS Policy:
a=1 : 0 (0)
Values after 2 simulations
MCTS Values:
a=0 : -2.8658 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1.94732 (1)
E

# # # # 
# 0$. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=5 o=1 a=1 o=0 
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 14.7793
Total reward = 14.0403
MCTS Values:
a=0 : 14.0403 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
Tree depth: 0 [0, 0]
Rollout depth: 6.5 [4, 9]
Total reward: 11.0927 [8.14506, 14.0403]
Policy after 2 simulations
MCTS Policy:
a=0 : 14.0403 (1)
Values after 2 simulations
MCTS Values:
a=0 : 14.0403 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.14506 (1)
N

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=5 o=1 a=1 o=0 a=0 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 6.30249 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 5.5 [2, 9]
Total reward: 7.66375 [6.30249, 9.025]
Policy after 2 simulations
MCTS Policy:
a=2 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : 6.30249 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
S

# # # # 
# 0$. #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=5 o=1 a=1 o=0 a=0 o=0 a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 6.30249 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [3, 9]
Total reward: 7.43812 [6.30249, 8.57375]
Policy after 2 simulations
MCTS Policy:
a=0 : 8.57375 (1)
Values after 2 simulations
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 6.30249 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
N

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=5 o=1 a=1 o=0 a=0 o=0 a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward -1.28719
Total reward = -1.22283
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1.22283 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 9.025 (1)
a=4 : -1e+10 (1000000)
a=5 : -1.22283 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4 [2, 6]
Total reward: 3.90108 [-1.22283, 9.025]
Policy after 2 simulations
MCTS Policy:
a=3 : 9.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 9.025 (1)
a=4 : -1e+10 (1000000)
a=5 : -1.22283 (1)
W

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=5 o=1 a=1 o=0 a=0 o=0 a=2 o=0 a=0 o=0 a=3 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.73781 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 15.188
Total reward = 14.4286
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.4286 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8.5 [5, 12]
Total reward: 11.0832 [7.73781, 14.4286]
Policy after 2 simulations
MCTS Policy:
a=1 : 14.4286 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 14.4286 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.73781 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=5 o=1 a=1 o=0 a=0 o=0 a=2 o=0 a=0 o=0 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -0.794144
Total reward = -0.754436
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -0.754436 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4 [1, 7]
Total reward: 4.37278 [-0.754436, 9.5]
Policy after 2 simulations
MCTS Policy:
a=5 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -0.754436 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Check 1

# # # # 
# 0$* #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=5 o=1 a=1 o=0 a=0 o=0 a=2 o=0 a=0 o=0 a=3 o=0 a=1 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.75 [9.5, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 5.98737, average = 6.66914
Undiscounted return = 10, average = 10
Starting run 3 with 2 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = -2.64908
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 3.5 [1, 6]
Total reward: 3.42546 [-2.64908, 9.5]
Policy after 2 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 6.30249 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -2.64908
Total reward = -2.51663
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -2.51663 (1)
a=4 : -1e+10 (1000000)
a=5 : 6.30249 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8 [7, 9]
Total reward: 1.89293 [-2.51663, 6.30249]
Policy after 2 simulations
MCTS Policy:
a=5 : 6.30249 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -2.51663 (1)
a=4 : -1e+10 (1000000)
a=5 : 6.30249 (1)
Check 1

# # # # 
# 0$* #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.75 [9.5, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.025, average = 7.45443
Undiscounted return = 10, average = 10
Starting run 4 with 2 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 26 steps, with total reward 6.74604
Total reward = 6.40874
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 6.40874 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 15.1334
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.1334 (1)
a=5 : 6.40874 (1)
Tree depth: 0 [0, 0]
Rollout depth: 19.5 [13, 26]
Total reward: 10.7711 [6.40874, 15.1334]
Policy after 2 simulations
MCTS Policy:
a=4 : 15.1334 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.1334 (1)
a=5 : 6.40874 (1)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 3 [3, 3]
Total reward: 8.57375 [8.57375, 8.57375]
Policy after 2 simulations
MCTS Policy:
a=1 : 8.57375 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 1 [1, 1]
Total reward: 9.75 [9.5, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 19.025, average = 10.3471
Undiscounted return = 20, average = 12.5
Starting run 5 with 2 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2.5 [2, 3]
Total reward: 13.7994 [9.025, 18.5738]
Policy after 2 simulations
MCTS Policy:
a=4 : 18.5738 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 1.5 [1, 2]
Total reward: 9.2625 [9.025, 9.5]
Policy after 2 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.6342 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 6.6342 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 8 [8, 8]
Total reward: 8.3171 [6.6342, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 6.6342 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 19.025, average = 12.0827
Undiscounted return = 20, average = 14
Starting run 6 with 2 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = -1.85494
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.85494 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.6342 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.85494 (1)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [4, 8]
Total reward: 2.38963 [-1.85494, 6.6342]
Policy after 2 simulations
MCTS Policy:
a=1 : 6.6342 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.6342 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.85494 (1)
a=5 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 5.13342 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 5.13342 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 13 [13, 13]
Total reward: 7.56671 [5.13342, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 5.13342 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.5, average = 11.6522
Undiscounted return = 10, average = 13.3333
Starting run 7 with 2 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 18.5738
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [3, 9]
Total reward: 12.4381 [6.30249, 18.5738]
Policy after 2 simulations
MCTS Policy:
a=4 : 18.5738 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.5738 (1)
a=5 : 0 (0)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 1.5 [1, 2]
Total reward: 9.2625 [9.025, 9.5]
Policy after 2 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.35092 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.35092 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 8.67546 [7.35092, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.35092 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 19.025, average = 12.7055
Undiscounted return = 20, average = 14.2857
Starting run 8 with 2 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 14.025 [9.025, 19.025]
Policy after 2 simulations
MCTS Policy:
a=4 : 19.025 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 0 (0)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 5.5 [5, 6]
Total reward: 7.54436 [7.35092, 7.73781]
Policy after 2 simulations
MCTS Policy:
a=2 : 7.73781 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.35092 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
S

# # # # 
# . . #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.40127
MCTS Values:
a=0 : 4.40127 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 8.5 [1, 16]
Total reward: 6.95063 [4.40127, 9.5]
Policy after 2 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : 4.40127 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . . #
# . * #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 9.025 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 9.025 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 2 [2, 2]
Total reward: 9.5125 [9.025, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : 9.025 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . . #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 18.5737, average = 13.439
Undiscounted return = 20, average = 15
Starting run 9 with 2 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.14506 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7 [4, 10]
Total reward: 7.06622 [5.98737, 8.14506]
Policy after 2 simulations
MCTS Policy:
a=5 : 8.14506 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.14506 (1)
Check 1

# # # # 
# * . #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 0 local transformations out of 1000 attempts
Out of particles, finishing episode with SelectRandom
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward 10
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
S

# # # # 
# . . #
# . * #
# # # # 
Reward 0
E

# # # # 
# . . #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 17.6451, average = 13.9063
Undiscounted return = 20, average = 15.5556
Starting run 10 with 2 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = -1.42625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 2 [1, 3]
Total reward: 4.03688 [-1.42625, 9.5]
Policy after 2 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Tree depth: 0 [0, 0]
Rollout depth: 6 [6, 6]
Total reward: 8.67546 [7.35092, 10]
Policy after 2 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 2 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.5, average = 13.4657
Undiscounted return = 10, average = 15
Simulations = 2
Runs = 10
Undiscounted return = 15 +- 1.58114
Discounted return = 13.4657 +- 1.67561
Time = 0.0013417
Starting run 1 with 4 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 56 steps, with total reward -0.689736
Total reward = -0.655249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : -0.655249 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -1.85494
Total reward = -1.76219
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : -0.655249 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1.76219 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 17.7378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : -0.655249 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : -1.76219 (1)
Tree depth: 0 [0, 0]
Rollout depth: 16.75 [1, 56]
Total reward: 6.20509 [-1.76219, 17.7378]
Policy after 4 simulations
MCTS Policy:
a=4 : 17.7378 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : -0.655249 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : -1.76219 (1)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.63291
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.94144 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.14506 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.94144 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.14506 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 6.38899 (2)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0.5 [0, 1]
Rollout depth: 6.5 [3, 15]
Total reward: 7.16521 [4.63291, 8.14506]
Policy after 4 simulations
MCTS Policy:
a=1 : 7.94144 (2)
a=1 o=0 a=3 : 8.14506 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.94144 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.14506 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 6.38899 (2)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 19 steps, with total reward 3.97214
Total reward = 3.77354
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 3.77354 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 3.77354 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 3.77354 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 10 [1, 19]
Total reward: 8.31838 [3.77354, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 3.77354 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = -0.975, average = -0.975
Undiscounted return = 0, average = 0
Starting run 2 with 4 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 18.5737
Total reward = 17.6451
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 17.6451 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 17.2378
Total reward = 16.3759
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.3759 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 17.6451 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 16.7628
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.3759 (1)
a=2 : 16.7628 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 17.6451 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = -2.64908
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.3759 (1)
a=2 : 16.7628 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 17.6451 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5.25 [4, 6]
Total reward: 12.0337 [-2.64908, 17.6451]
Policy after 4 simulations
MCTS Policy:
a=5 : 17.6451 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.3759 (1)
a=2 : 16.7628 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 17.6451 (1)
Check 1

# # # # 
# * . #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -1.43532
Total reward = -1.36355
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1.36355 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = -1.42625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : -1.36355 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : -1.36355 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7.5 [3, 13]
Total reward: 2.62217 [-1.42625, 8.14506]
Policy after 4 simulations
MCTS Policy:
a=1 : 8.14506 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : -1.36355 (1)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 19.025
Total reward = 18.0737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 18.0737 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 6.6342 (1)
a=3 : 18.0737 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 6.6342 (1)
a=3 : 18.0737 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4 [1, 8]
Total reward: 11.052 [6.6342, 18.0737]
Policy after 4 simulations
MCTS Policy:
a=3 : 18.0737 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 6.6342 (1)
a=3 : 18.0737 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
W

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = -2.64908
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -0.794144
Total reward = -0.754436
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : -0.754436 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : -0.754436 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : -0.754436 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7.5 [3, 14]
Total reward: 2.51175 [-2.64908, 8.57375]
Policy after 4 simulations
MCTS Policy:
a=1 : 8.57375 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : -0.754436 (1)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=1 o=0 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 15.9247
Total reward = 15.1284
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 15.1284 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 16.8509
Total reward = 16.0084
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 16.0084 (1)
a=4 : -1e+10 (1000000)
a=5 : 15.1284 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 16.0084 (1)
a=4 : -1e+10 (1000000)
a=5 : 15.1284 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5 [1, 7]
Total reward: 12.6592 [9.5, 16.0084]
Policy after 4 simulations
MCTS Policy:
a=3 : 16.0084 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 16.0084 (1)
a=4 : -1e+10 (1000000)
a=5 : 15.1284 (1)
W

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward -3.19751
Total reward = -3.03763
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -3.03763 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = -4.01263
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : -3.03763 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : -3.03763 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8.25 [1, 12]
Total reward: 1.96333 [-4.01263, 9.5]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.01263 (1)
a=5 : -3.03763 (1)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 16.6342
Total reward = 15.8025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 15.8025 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 15.8025 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 24 steps, with total reward 9.37606
Total reward = 8.90726
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 15.8025 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.90726 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 15.8025 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.90726 (1)
Tree depth: 0 [0, 0]
Rollout depth: 11.3333 [1, 24]
Total reward: 11.0524 [8.90726, 15.8025]
Policy after 4 simulations
MCTS Policy:
a=3 : 15.8025 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 15.8025 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.90726 (1)
W

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = -1.42625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 7.73781 (1)
Tree depth: 0 [0, 0]
Rollout depth: 5 [3, 9]
Total reward: 5.29695 [-1.42625, 8.57375]
Policy after 4 simulations
MCTS Policy:
a=1 : 8.57375 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 6.30249 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 7.73781 (1)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 a=1 o=0 a=3 o=0 a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.63291
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.63291 (1)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 4.63291 (1)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8 [3, 15]
Total reward: 7.6394 [4.63291, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 4.63291 (1)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 6.6342, average = 2.8296
Undiscounted return = 10, average = 5
Starting run 3 with 4 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.57375 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward 10.2191
Total reward = 9.70811
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.70811 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.57375 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 20 steps, with total reward 3.77354
Total reward = -6.41514
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.70811 (1)
a=3 : -1e+10 (1000000)
a=4 : -6.41514 (1)
a=5 : 8.57375 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward -1.76219
Total reward = -1.67408
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.67408 (1)
a=2 : 9.70811 (1)
a=3 : -1e+10 (1000000)
a=4 : -6.41514 (1)
a=5 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 12.5 [3, 21]
Total reward: 2.54816 [-6.41514, 9.70811]
Policy after 4 simulations
MCTS Policy:
a=2 : 9.70811 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -1.67408 (1)
a=2 : 9.70811 (1)
a=3 : -1e+10 (1000000)
a=4 : -6.41514 (1)
a=5 : 8.57375 (1)
S

# # # # 
# 0X. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 16.3116
Total reward = 15.496
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 15.496 (1)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 15.4036
Total reward = 14.6334
MCTS Values:
a=0 : 14.6334 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 15.496 (1)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 
Adding sample:

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 28 steps, with total reward 11.0772
Total reward = 9.99716
MCTS Values:
a=0 : 14.6334 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 12.7466 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 10.5233 (1)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 12 [1, 28]
Total reward: 12.4066 [9.5, 15.496]
Policy after 4 simulations
MCTS Policy:
a=0 : 14.6334 (1)
Values after 4 simulations
MCTS Values:
a=0 : 14.6334 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 12.7466 (2)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 10.5233 (1)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = -5.8188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward -0.754436
Total reward = -0.716715
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.716715 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.716715 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8.25 [3, 17]
Total reward: 2.44401 [-5.8188, 8.57375]
Policy after 4 simulations
MCTS Policy:
a=5 : 8.57375 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.716715 (1)
a=2 : 7.73781 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 8.57375 (1)
Check 1

# # # # 
# * . #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 0 local transformations out of 1000 attempts
Out of particles, finishing episode with SelectRandom
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward -10
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
W

# # # # 
# * . #
# . . #
# # # # 
Reward 0
S

# # # # 
# . . #
# * . #
# # # # 
Reward 0
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
S

# # # # 
# . . #
# . * #
# # # # 
Reward 0
W

# # # # 
# . . #
# * . #
# # # # 
Reward 0
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
S

# # # # 
# . . #
# * . #
# # # # 
Reward 0
E

# # # # 
# . . #
# . * #
# # # # 
Reward 0
E

# # # # 
# . . #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = -3.697, average = 0.654068
Undiscounted return = 0, average = 3.33333
Starting run 4 with 4 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 15.4036
Total reward = 14.6334
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 14.6334 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 14.6334 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 15.9874
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.9874 (1)
a=5 : 14.6334 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.9874 (1)
a=5 : 14.6334 (1)
Tree depth: 0 [0, 0]
Rollout depth: 7 [1, 13]
Total reward: 12.0665 [8.14506, 15.9874]
Policy after 4 simulations
MCTS Policy:
a=4 : 15.9874 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.9874 (1)
a=5 : 14.6334 (1)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 9.025 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.57375 (1)
a=2 : 8.3814 (2)
a=2 o=0 a=0 : 8.14506 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98868 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 5.688 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 8.3814 (2)
a=2 o=0 a=0 : 8.14506 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0.5 [0, 1]
Rollout depth: 5 [2, 11]
Total reward: 7.68504 [5.4036, 9.025]
Policy after 4 simulations
MCTS Policy:
a=2 : 8.3814 (2)
a=2 o=0 a=0 : 8.14506 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98868 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 5.688 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 8.3814 (2)
a=2 o=0 a=0 : 8.14506 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
S

# # # # 
# . . #
# * . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 a=1 o=0 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : 9.5 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 a=0 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : 8.57375 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 9.025 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : 9.5 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0.5 [0, 1]
Rollout depth: 1.75 [1, 3]
Total reward: 8.91812 [8.57375, 9.5]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.2625 (2)
a=1 o=0 a=0 : 9.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : 8.57375 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 9.025 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : 9.5 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . . #
# . * #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 0 (0)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 5 [5, 5]
Total reward: 8.8689 [7.73781, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 4 simulations
MCTS Values:
a=0 : 7.73781 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . . #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 18.5737, average = 5.13399
Undiscounted return = 20, average = 7.5
Starting run 5 with 4 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.13342 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 16.3025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.3025 (1)
a=5 : 5.13342 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.3025 (1)
a=5 : 5.13342 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.3025 (1)
a=5 : 5.13342 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9 [4, 13]
Total reward: 8.89209 [5.13342, 16.3025]
Policy after 4 simulations
MCTS Policy:
a=4 : 16.3025 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.98737 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.3025 (1)
a=5 : 5.13342 (1)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 42 steps, with total reward 1.22087
Total reward = 1.15982
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 1.15982 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.4036 (1)
a=2 : 1.15982 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.77433 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 1.15982 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.77433 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 5.09241 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0.5 [0, 1]
Rollout depth: 14.5 [1, 42]
Total reward: 5.93337 [1.15982, 9.025]
Policy after 4 simulations
MCTS Policy:
a=1 : 6.77433 (2)
a=1 o=0 a=3 : 8.57375 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.77433 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 5.09241 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 9.025 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.57375 (1)
a=3 : 9.025 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.57375 (1)
a=3 : 9.025 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 2.5 [2, 3]
Total reward: 9.39969 [8.57375, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.57375 (1)
a=3 : 9.025 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 19.025, average = 7.91219
Undiscounted return = 20, average = 10
Starting run 6 with 4 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = -4.86658
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.86658 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.86658 (1)
a=5 : 5.688 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward -1.04842
Total reward = -0.996004
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.996004 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -4.86658 (1)
a=5 : 5.688 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.996004 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.86658 (1)
a=5 : 5.688 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9.25 [3, 13]
Total reward: 2.09979 [-4.86658, 8.57375]
Policy after 4 simulations
MCTS Policy:
a=2 : 8.57375 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : -0.996004 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -4.86658 (1)
a=5 : 5.688 (1)
S

# # # # 
# 0$. #
# * . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.63291
MCTS Values:
a=0 : 4.63291 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : 4.63291 (1)
a=1 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward -2.04981
Total reward = -1.94732
MCTS Values:
a=0 : 4.63291 (1)
a=1 : -1.94732 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.025 (1)
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Expanding node: a=2 o=0 a=5 o=2 
Adding sample:

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.30249
MCTS Values:
a=0 : 4.63291 (1)
a=1 : -1.94732 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.66375 (2)
a=5 o=2 a=0 : 6.6342 (1)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 9.25 [2, 15]
Total reward: 4.50327 [-1.94732, 9.025]
Policy after 4 simulations
MCTS Policy:
a=5 : 7.66375 (2)
a=5 o=2 a=0 : 6.6342 (1)
Values after 4 simulations
MCTS Values:
a=0 : 4.63291 (1)
a=1 : -1.94732 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.66375 (2)
a=5 o=2 a=0 : 6.6342 (1)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : 0 (0)
Check 1

# # # # 
# 0$. #
# * . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 19.025
Total reward = 18.0737
MCTS Values:
a=0 : 18.0737 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : 18.0737 (1)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 5.4036 (1)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = 13.4721
MCTS Values:
a=0 : 15.7729 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 14.1812 (1)
a=0 o=0 a=5 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 5.4036 (1)
Tree depth: 0.25 [0, 1]
Rollout depth: 8.25 [1, 17]
Total reward: 11.6124 [5.4036, 18.0737]
Policy after 4 simulations
MCTS Policy:
a=0 : 15.7729 (2)
a=0 o=0 a=4 : 14.1812 (1)
Values after 4 simulations
MCTS Values:
a=0 : 15.7729 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 14.1812 (1)
a=0 o=0 a=5 : 0 (0)
a=1 : 9.5 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 5.4036 (1)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 15.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 18.5737
Total reward = 17.6451
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 17.6451 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 14.372
Total reward = 13.6534
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.6534 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 17.6451 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8 [4, 12]
Total reward: 13.4214 [6.98337, 17.6451]
Policy after 4 simulations
MCTS Policy:
a=5 : 17.6451 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.6534 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.4036 (1)
a=5 : 17.6451 (1)
Check 1

# # # # 
# * . #
# . . #
# # # # 
Observed good
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.57375 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward 11.7299
Total reward = 11.1434
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.1434 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.57375 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.1434 (1)
a=2 : 7.35092 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 8.57375 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 18.1451
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.1434 (1)
a=2 : 7.35092 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.1451 (1)
a=5 : 8.57375 (1)
Tree depth: 0 [0, 0]
Rollout depth: 8.5 [3, 21]
Total reward: 11.3033 [7.35092, 18.1451]
Policy after 4 simulations
MCTS Policy:
a=4 : 18.1451 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 11.1434 (1)
a=2 : 7.35092 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.1451 (1)
a=5 : 8.57375 (1)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.63291
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.16715 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 7.73781 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 4.63291 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.16715 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 7.73781 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 6.38899 (2)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0.5 [0, 1]
Rollout depth: 7.5 [3, 15]
Total reward: 6.77807 [4.63291, 8.14506]
Policy after 4 simulations
MCTS Policy:
a=1 : 7.16715 (2)
a=1 o=0 a=3 : 7.73781 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.16715 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 7.73781 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 6.38899 (2)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 4.1812
Total reward = 3.97214
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 3.97214 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 3.97214 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.73781 (1)
a=3 : 3.97214 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 7.73781 (1)
a=3 : 3.97214 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 11.5 [5, 18]
Total reward: 7.92749 [3.97214, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 7.73781 (1)
a=3 : 3.97214 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 15.496, average = 9.17616
Undiscounted return = 20, average = 11.6667
Starting run 7 with 4 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward -2.72251
Total reward = -2.58638
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -2.58638 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 15.9874
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.9874 (1)
a=5 : -2.58638 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.9874 (1)
a=5 : -2.58638 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9.75 [9, 10]
Total reward: 6.42271 [-2.58638, 15.9874]
Policy after 4 simulations
MCTS Policy:
a=4 : 15.9874 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.30249 (1)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.9874 (1)
a=5 : -2.58638 (1)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward -10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.6189 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.14506 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 5.4036 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.6189 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.14506 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 6.0189 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 6.98337 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0.5 [0, 1]
Rollout depth: 6 [1, 12]
Total reward: 7.3189 [5.4036, 9.5]
Policy after 4 simulations
MCTS Policy:
a=1 : 8.6189 (2)
a=1 o=0 a=3 : 8.14506 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.6189 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.14506 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 6.0189 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 6.98337 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 4 [1, 7]
Total reward: 9.12084 [6.98337, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 6.98337 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = -0.975, average = 7.72599
Undiscounted return = 0, average = 10
Starting run 8 with 4 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 15.5571
Total reward = 14.7793
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 14.7793 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 14.7793 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 9.025 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 14.7793 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 9.025 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 14.7793 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 9.025 (1)
Tree depth: 0 [0, 0]
Rollout depth: 3.25 [1, 8]
Total reward: 13.0823 [9.025, 19.025]
Policy after 4 simulations
MCTS Policy:
a=4 : 19.025 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 14.7793 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 9.025 (1)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.00419 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 5.98737 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.00419 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 6.14493 (2)
a=2 o=0 a=0 : 6.6342 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0.5 [0, 1]
Rollout depth: 6.5 [1, 10]
Total reward: 7.07456 [5.98737, 9.025]
Policy after 4 simulations
MCTS Policy:
a=1 : 8.00419 (2)
a=1 o=0 a=2 : 9.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.00419 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 6.14493 (2)
a=2 o=0 a=0 : 6.6342 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 25 steps, with total reward 2.91989
Total reward = 2.7739
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 2.7739 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 2.7739 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 4.87675 (1)
a=3 : 2.7739 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 4.87675 (1)
a=3 : 2.7739 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 19.5 [14, 25]
Total reward: 6.91266 [2.7739, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 4.87675 (1)
a=3 : 2.7739 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 19.025, average = 9.13837
Undiscounted return = 20, average = 11.25
Starting run 9 with 4 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 17.7378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 8.14506 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 15.5571
Total reward = 14.7793
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 14.7793 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 8.14506 (1)
Tree depth: 0 [0, 0]
Rollout depth: 4.5 [1, 8]
Total reward: 12.5405 [8.14506, 17.7378]
Policy after 4 simulations
MCTS Policy:
a=4 : 17.7378 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 14.7793 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 8.14506 (1)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward 10
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 5.88381 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 6.98337 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0.5 [0, 1]
Rollout depth: 5.75 [1, 13]
Total reward: 7.45441 [5.13342, 9.025]
Policy after 4 simulations
MCTS Policy:
a=1 : 9.025 (2)
a=1 o=0 a=2 : 9.5 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 5.88381 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 6.98337 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = 4.1812
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 4.1812 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 4.1812 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.40127
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 4.40127 (1)
a=3 : 4.1812 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 4.40127 (1)
a=3 : 4.1812 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0 [0, 0]
Rollout depth: 16.5 [16, 17]
Total reward: 7.14562 [4.1812, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 4.40127 (1)
a=3 : 4.1812 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 19.025, average = 10.2369
Undiscounted return = 20, average = 12.2222
Starting run 10 with 4 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.98737 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 17.6451
Total reward = 16.7628
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.7628 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 5.98737 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = -2.64908
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.7628 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 5.98737 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 16 steps, with total reward 11.2671
Total reward = 10.7038
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.7628 (1)
a=2 : 10.7038 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 5.98737 (1)
Tree depth: 0 [0, 0]
Rollout depth: 9.25 [5, 16]
Total reward: 7.70121 [-2.64908, 16.7628]
Policy after 4 simulations
MCTS Policy:
a=1 : 16.7628 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 16.7628 (1)
a=2 : 10.7038 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.64908 (1)
a=5 : 5.98737 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 23 steps, with total reward -4.11558
Total reward = -3.9098
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -3.9098 (1)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -3.9098 (1)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Tree depth: 0 [0, 0]
Rollout depth: 10 [1, 23]
Total reward: 5.73528 [-3.9098, 10]
Policy after 4 simulations
MCTS Policy:
a=1 : 10 (1)
Values after 4 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -3.9098 (1)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.5, average = 10.1632
Undiscounted return = 10, average = 12
Simulations = 4
Runs = 10
Undiscounted return = 12 +- 2.75681
Discounted return = 10.1632 +- 2.80693
Time = 0.0028911
Starting run 1 with 8 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward -1.66958
Total reward = -1.5861
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1.5861 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : -1.5861 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = -3.01663
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.01663 (1)
a=5 : -1.5861 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 23 steps, with total reward -1.39758
Total reward = -1.26131
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 3.65622 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1.3277 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.01663 (1)
a=5 : -1.5861 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward -0.879938
Total reward = -0.794144
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.67546 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : -0.835941 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 3.65622 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1.3277 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.01663 (1)
a=5 : -1.5861 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = -1.76219
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.67546 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : -0.835941 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 3.65622 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1.3277 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.01663 (1)
a=5 : -1.67415 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1.85494 (1)
a=5 o=2 a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 17.3509
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.67546 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : -0.835941 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 3.65622 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1.3277 (1)
a=3 : -1e+10 (1000000)
a=4 : 7.16715 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 7.73781 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : -1.67415 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1.85494 (1)
a=5 o=2 a=5 : 0 (0)
Tree depth: 0.5 [0, 1]
Rollout depth: 8.375 [3, 23]
Total reward: 3.20617 [-3.01663, 17.3509]
Policy after 8 simulations
MCTS Policy:
a=4 : 7.16715 (2)
a=4 o=0 a=1 : 7.73781 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 3.67546 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : -0.835941 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 3.65622 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1.3277 (1)
a=3 : -1e+10 (1000000)
a=4 : 7.16715 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 7.73781 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : -1.67415 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1.85494 (1)
a=5 o=2 a=5 : 0 (0)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 8.00419 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.9762 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 5.688 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 8.00419 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.9762 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 5.688 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 8.05115 (3)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 a=1 o=0 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.9762 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 5.688 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 7.38926 (4)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 7.594 (2)
a=2 o=0 a=1 o=0 a=0 : 5.98737 (1)
a=2 o=0 a=1 o=0 a=1 : 0 (0)
a=2 o=0 a=1 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=3 : 0 (0)
a=2 o=0 a=1 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.35715 (4)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 5.688 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 7.38926 (4)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 7.594 (2)
a=2 o=0 a=1 o=0 a=0 : 5.98737 (1)
a=2 o=0 a=1 o=0 a=1 : 0 (0)
a=2 o=0 a=1 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=3 : 0 (0)
a=2 o=0 a=1 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0.875 [0, 2]
Rollout depth: 4.85714 [1, 11]
Total reward: 7.8732 [5.4036, 9.5]
Policy after 8 simulations
MCTS Policy:
a=1 : 8.35715 (4)
a=1 o=0 a=1 : 10 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.35715 (4)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 5.688 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 7.38926 (4)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 7.594 (2)
a=2 o=0 a=1 o=0 a=0 : 5.98737 (1)
a=2 o=0 a=1 o=0 a=1 : 0 (0)
a=2 o=0 a=1 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=3 : 0 (0)
a=2 o=0 a=1 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 3 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 5.688 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 5.688 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : 5.688 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=3 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : 5.99525 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 6.6342 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 9.5 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : 5.99525 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 6.6342 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.54473 (3)
a=2 o=0 a=0 : 6.98337 (1)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : 5.99525 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 6.6342 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0.375 [0, 1]
Rollout depth: 6.75 [1, 11]
Total reward: 8.45309 [5.688, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.54473 (3)
a=2 o=0 a=0 : 6.98337 (1)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : 5.99525 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 6.6342 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 19.025, average = 19.025
Undiscounted return = 20, average = 20
Starting run 2 with 8 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 14.3767
Total reward = 13.6579
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 13.6579 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 16.1342
Total reward = 15.3275
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.3275 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 13.6579 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = -1.42625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 15.3275 (1)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 13.6579 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward 3.58486
Total reward = 3.23534
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.28141 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 3.40562 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 13.6579 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward -1.36355
Total reward = -1.2306
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.28141 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 3.40562 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 6.98337 (1)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 6.21365 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1.29537 (1)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.28141 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 3.40562 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 7.77856 (2)
a=2 o=0 a=0 : 9.025 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 6.21365 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1.29537 (1)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.63792 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.73781 (1)
a=1 o=0 a=3 : 3.40562 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 7.77856 (2)
a=2 o=0 a=0 : 9.025 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 6.21365 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1.29537 (1)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
Tree depth: 0.5 [0, 1]
Rollout depth: 9.125 [2, 21]
Total reward: 6.55899 [-1.42625, 15.3275]
Policy after 8 simulations
MCTS Policy:
a=1 : 8.63792 (3)
a=1 o=0 a=2 : 7.73781 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.63792 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.73781 (1)
a=1 o=0 a=3 : 3.40562 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 7.77856 (2)
a=2 o=0 a=0 : 9.025 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1.42625 (1)
a=5 : 6.21365 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -1.29537 (1)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward -1.76219
Total reward = -1.67408
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1.67408 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1.67408 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -1.67408 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.025 (1)
a=3 : -1.67408 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.025 (1)
a=3 : -1.67408 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=1 o=0 a=5 o=1 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.025 (1)
a=3 : -1.67408 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.82253 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 8.57375 (1)
a=5 o=1 a=3 : 0 (0)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Expanding node: a=1 o=0 a=2 o=0 
Adding sample:

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.58503 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 8.57375 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1.67408 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.82253 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 8.57375 (1)
a=5 o=1 a=3 : 0 (0)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.58503 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 8.57375 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1.67408 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.82253 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 8.57375 (1)
a=5 o=1 a=3 : 0 (0)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 3 [1, 6]
Total reward: 7.89263 [-1.67408, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.58503 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 8.57375 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1.67408 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.82253 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 8.57375 (1)
a=5 o=1 a=3 : 0 (0)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.5, average = 14.2625
Undiscounted return = 10, average = 15
Starting run 3 with 8 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 16.7628
Total reward = 15.9247
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 15.9247 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 15.9247 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 31 steps, with total reward 12.1464
Total reward = 11.5391
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 15.9247 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 11.5391 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = -1.42625
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 15.9247 (1)
a=3 : -1e+10 (1000000)
a=4 : 8.79937 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.025 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 11.5391 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 13.5487
Total reward = 12.2277
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 14.0762 (2)
a=2 o=0 a=0 : 12.8712 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 8.79937 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.025 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 11.5391 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=5 o=1 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 15.8025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.73781 (1)
a=2 : 14.0762 (2)
a=2 o=0 a=0 : 12.8712 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 8.79937 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.025 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 13.6708 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 16.6342 (1)
a=5 o=1 a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 26 steps, with total reward 8.76127
Total reward = 7.90704
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.82243 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 8.3232 (1)
a=2 : 14.0762 (2)
a=2 o=0 a=0 : 12.8712 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 8.79937 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.025 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 13.6708 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 16.6342 (1)
a=5 o=1 a=5 : 0 (0)
Tree depth: 0.5 [0, 1]
Rollout depth: 11.625 [2, 31]
Total reward: 11.0922 [-1.42625, 19.025]
Policy after 8 simulations
MCTS Policy:
a=2 : 14.0762 (2)
a=2 o=0 a=0 : 12.8712 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.82243 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 8.3232 (1)
a=2 : 14.0762 (2)
a=2 o=0 a=0 : 12.8712 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 8.79937 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.025 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 13.6708 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 16.6342 (1)
a=5 o=1 a=5 : 0 (0)
S

# # # # 
# 0$. #
# * . #
# # # # 
Reward 0
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 13.2067
Total reward = 12.5463
MCTS Values:
a=0 : 0 (0)
a=1 : 12.5463 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 16.3025
Total reward = 15.4874
MCTS Values:
a=0 : 15.4874 (1)
a=1 : 12.5463 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : 15.4874 (1)
a=1 : 12.5463 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 6.30249 (1)
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=2 o=0 a=0 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = -3.51263
MCTS Values:
a=0 : 5.98737 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -3.69751 (1)
a=0 o=0 a=5 : 0 (0)
a=1 : 12.5463 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 6.30249 (1)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Expanding node: a=2 o=0 a=1 o=0 
Adding sample:

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.63291
MCTS Values:
a=0 : 5.98737 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -3.69751 (1)
a=0 o=0 a=5 : 0 (0)
a=1 : 8.58962 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 4.87675 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 6.30249 (1)
Starting simulation

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Expanding node: a=2 o=0 a=5 o=2 
Adding sample:

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : 5.98737 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -3.69751 (1)
a=0 o=0 a=5 : 0 (0)
a=1 : 8.58962 (2)
a=1 o=0 a=0 : 0 (0)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 4.87675 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.43812 (2)
a=5 o=2 a=0 : 0 (0)
a=5 o=2 a=1 : 9.025 (1)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : 0 (0)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Adding sample:

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : 5.98737 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -3.69751 (1)
a=0 o=0 a=5 : 0 (0)
a=1 : 8.73475 (3)
a=1 o=0 a=0 : 9.5 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 4.87675 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 7.43812 (2)
a=5 o=2 a=0 : 0 (0)
a=5 o=2 a=1 : 9.025 (1)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : 0 (0)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 
Adding sample:

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 14.7793
Total reward = 13.3383
MCTS Values:
a=0 : 5.98737 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -3.69751 (1)
a=0 o=0 a=5 : 0 (0)
a=1 : 8.73475 (3)
a=1 o=0 a=0 : 9.5 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 4.87675 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.40484 (3)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 14.0403 (1)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=2 a=0 : 0 (0)
a=5 o=2 a=1 : 9.025 (1)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : 0 (0)
Tree depth: 0.625 [0, 1]
Rollout depth: 8.75 [1, 16]
Total reward: 8.29919 [-3.51263, 15.4874]
Policy after 8 simulations
MCTS Policy:
a=5 : 9.40484 (3)
a=5 o=1 a=1 : 14.0403 (1)
a=5 o=2 a=1 : 9.025 (1)
Values after 8 simulations
MCTS Values:
a=0 : 5.98737 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -3.69751 (1)
a=0 o=0 a=5 : 0 (0)
a=1 : 8.73475 (3)
a=1 o=0 a=0 : 9.5 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 4.87675 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.40484 (3)
a=5 o=1 a=0 : 0 (0)
a=5 o=1 a=1 : 14.0403 (1)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
a=5 o=2 a=0 : 0 (0)
a=5 o=2 a=1 : 9.025 (1)
a=5 o=2 a=2 : -1e+10 (1000000)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : -1e+10 (1000000)
a=5 o=2 a=5 : 0 (0)
Check 1

# # # # 
# 0$. #
# * . #
# # # # 
Observed good
Reward 0
Matched 1 states
Created 1 local transformations out of 2 attempts
Expanding node: a=2 o=0 a=5 o=1 
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : 0 (0)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward 16.3759
Total reward = 15.5571
MCTS Values:
a=0 : 15.5571 (1)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 17 steps, with total reward 10.3886
Total reward = 9.8692
MCTS Values:
a=0 : 15.5571 (1)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.8692 (1)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : 12.0654 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 9.025 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 9.8692 (1)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=5 o=1 
Adding sample:

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward 13.9017
Total reward = 12.5463
MCTS Values:
a=0 : 12.0654 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 9.025 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=1 : 6.98337 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 11.2078 (2)
a=5 o=1 a=0 : 13.2067 (1)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=1 o=0 
Adding sample:

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : 12.0654 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 9.025 (1)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=1 : 8.00419 (2)
a=1 o=0 a=0 : 9.5 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 11.2078 (2)
a=5 o=1 a=0 : 13.2067 (1)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 12.8712
Total reward = 11.6163
MCTS Values:
a=0 : 11.9157 (3)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 9.025 (1)
a=0 o=0 a=2 : 12.2277 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=1 : 8.00419 (2)
a=1 o=0 a=0 : 9.5 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 11.2078 (2)
a=5 o=1 a=0 : 13.2067 (1)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
Starting simulation

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Adding sample:

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 21 steps, with total reward 10.2191
Total reward = 9.2227
MCTS Values:
a=0 : 11.9157 (3)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 9.025 (1)
a=0 o=0 a=2 : 12.2277 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=1 : 8.00419 (2)
a=1 o=0 a=0 : 9.5 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 10.5461 (3)
a=5 o=1 a=0 : 13.2067 (1)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 9.70811 (1)
Tree depth: 0.625 [0, 1]
Rollout depth: 10.5 [1, 21]
Total reward: 10.4242 [6.98337, 15.5571]
Policy after 8 simulations
MCTS Policy:
a=0 : 11.9157 (3)
a=0 o=0 a=2 : 12.2277 (1)
Values after 8 simulations
MCTS Values:
a=0 : 11.9157 (3)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 9.025 (1)
a=0 o=0 a=2 : 12.2277 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : 0 (0)
a=0 o=0 a=5 : 0 (0)
a=1 : 8.00419 (2)
a=1 o=0 a=0 : 9.5 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : 10.5461 (3)
a=5 o=1 a=0 : 13.2067 (1)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : -1e+10 (1000000)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 9.70811 (1)
N

# # # # 
# * . #
# . . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward 7.35092
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 18.1451
Total reward = 17.2378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 17.2378 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 43 steps, with total reward 4.74468
Total reward = 4.50745
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 4.50745 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 17.2378 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=4 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 4.50745 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.5 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 17.2378 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 4.50745 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.5 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 12.2944 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 7.73781 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = -4.01263
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (1)
a=2 : 4.50745 (1)
a=3 : -1e+10 (1000000)
a=4 : 11.3458 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.5 (1)
a=4 o=0 a=2 : 6.30249 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 12.2944 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 7.73781 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=1 o=0 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 7.35092 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 4.50745 (1)
a=3 : -1e+10 (1000000)
a=4 : 11.3458 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.5 (1)
a=4 o=0 a=2 : 6.30249 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 12.2944 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 7.73781 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 9.75 [1, 43]
Total reward: 9.63754 [-4.01263, 19.025]
Policy after 8 simulations
MCTS Policy:
a=5 : 12.2944 (2)
a=5 o=1 a=5 : 7.73781 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.98337 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 7.35092 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 4.50745 (1)
a=3 : -1e+10 (1000000)
a=4 : 11.3458 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.5 (1)
a=4 o=0 a=2 : 6.30249 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 12.2944 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 7.73781 (1)
Check 1

# # # # 
# * . #
# . . #
# # # # 
Observed good
Reward 0
Matched 1 states
Created 1 local transformations out of 4 attempts
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 7.35092 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 17.7378
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.7378 (1)
a=5 : 7.35092 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=4 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.3814 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.5 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 7.35092 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 15.1334
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.2987 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.5 (1)
a=4 o=0 a=2 : 5.4036 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 7.35092 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=1 o=0 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : 17.2987 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.5 (1)
a=4 o=0 a=2 : 5.4036 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 7.35092 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=2 o=0 
Adding sample:

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 7.22378 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 6.6342 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 17.2987 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.5 (1)
a=4 o=0 a=2 : 5.4036 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 7.35092 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 5.28571 [1, 12]
Total reward: 11.5868 [6.30249, 19.025]
Policy after 8 simulations
MCTS Policy:
a=4 : 17.2987 (3)
a=4 o=0 a=1 : 9.5 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 7.22378 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 6.6342 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 17.2987 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.5 (1)
a=4 o=0 a=2 : 5.4036 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 7.35092 (1)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward 10
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 29 steps, with total reward 2.37827
Total reward = 2.25936
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 2.25936 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 2.25936 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 2.25936 (1)
a=2 : 8.35941 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 8.57375 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.28092 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 6.6342 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 8.35941 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 8.57375 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.28092 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 6.6342 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 8.02324 (3)
a=2 o=0 a=0 : 7.73781 (1)
a=2 o=0 a=1 : 8.57375 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=4 o=0 a=2 o=0 a=1 o=0 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.28092 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 6.6342 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 8.27368 (4)
a=2 o=0 a=0 : 7.73781 (1)
a=2 o=0 a=1 : 9.03688 (2)
a=2 o=0 a=1 o=0 a=0 : 0 (0)
a=2 o=0 a=1 o=0 a=1 : 10 (1)
a=2 o=0 a=1 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=3 : 0 (0)
a=2 o=0 a=1 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.86228 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 6.6342 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 8.27368 (4)
a=2 o=0 a=0 : 7.73781 (1)
a=2 o=0 a=1 : 9.03688 (2)
a=2 o=0 a=1 o=0 a=0 : 0 (0)
a=2 o=0 a=1 o=0 a=1 : 10 (1)
a=2 o=0 a=1 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=3 : 0 (0)
a=2 o=0 a=1 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=4 o=0 a=2 o=0 a=0 o=0 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.86228 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 6.6342 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 7.75655 (5)
a=2 o=0 a=0 : 6.86259 (2)
a=2 o=0 a=0 o=0 a=0 : -1e+10 (1000000)
a=2 o=0 a=0 o=0 a=1 : 6.30249 (1)
a=2 o=0 a=0 o=0 a=2 : 0 (0)
a=2 o=0 a=0 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=0 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=0 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=1 : 9.03688 (2)
a=2 o=0 a=1 o=0 a=0 : 0 (0)
a=2 o=0 a=1 o=0 a=1 : 10 (1)
a=2 o=0 a=1 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=3 : 0 (0)
a=2 o=0 a=1 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 1 [0, 2]
Rollout depth: 8.28571 [1, 29]
Total reward: 7.0462 [2.25936, 9.025]
Policy after 8 simulations
MCTS Policy:
a=2 : 7.75655 (5)
a=2 o=0 a=1 : 9.03688 (2)
a=2 o=0 a=1 o=0 a=1 : 10 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.86228 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 6.6342 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 7.75655 (5)
a=2 o=0 a=0 : 6.86259 (2)
a=2 o=0 a=0 o=0 a=0 : -1e+10 (1000000)
a=2 o=0 a=0 o=0 a=1 : 6.30249 (1)
a=2 o=0 a=0 o=0 a=2 : 0 (0)
a=2 o=0 a=0 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=0 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=0 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=1 : 9.03688 (2)
a=2 o=0 a=1 o=0 a=0 : 0 (0)
a=2 o=0 a=1 o=0 a=1 : 10 (1)
a=2 o=0 a=1 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=3 : 0 (0)
a=2 o=0 a=1 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
S

# # # # 
# . . #
# * . #
# # # # 
Reward 0
Matched 4 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=4 o=0 a=2 o=0 
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : 0 (0)
a=1 : 6.6342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 20 steps, with total reward 3.77354
Total reward = 3.58486
MCTS Values:
a=0 : 3.58486 (1)
a=1 : 6.6342 (1)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=4 o=0 a=2 o=0 a=1 o=0 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : 3.58486 (1)
a=1 : 7.60398 (2)
a=1 o=0 a=0 : 9.025 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=4 o=0 a=2 o=0 a=0 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : 6.0793 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 9.025 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=1 : 7.60398 (2)
a=1 o=0 a=0 : 9.025 (1)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : 6.0793 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 9.025 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=1 : 8.23598 (3)
a=1 o=0 a=0 : 9.025 (1)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.4036
MCTS Values:
a=0 : 5.85407 (3)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 5.688 (1)
a=0 o=0 a=2 : 9.025 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=1 : 8.23598 (3)
a=1 o=0 a=0 : 9.025 (1)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.688
MCTS Values:
a=0 : 5.85407 (3)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 5.688 (1)
a=0 o=0 a=2 : 9.025 (1)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=1 : 7.59899 (4)
a=1 o=0 a=0 : 9.025 (1)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 5.98737 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=4 o=0 a=2 o=0 a=0 o=0 a=2 o=0 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.35092
MCTS Values:
a=0 : 6.22828 (4)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 5.688 (1)
a=0 o=0 a=2 : 8.3814 (2)
a=0 o=0 a=2 o=0 a=0 : 0 (0)
a=0 o=0 a=2 o=0 a=1 : 8.14506 (1)
a=0 o=0 a=2 o=0 a=2 : -1e+10 (1000000)
a=0 o=0 a=2 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=2 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=2 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=1 : 7.59899 (4)
a=1 o=0 a=0 : 9.025 (1)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 5.98737 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0.875 [0, 2]
Rollout depth: 8.14286 [2, 20]
Total reward: 6.91364 [3.58486, 9.5]
Policy after 8 simulations
MCTS Policy:
a=1 : 7.59899 (4)
a=1 o=0 a=1 : 10 (1)
Values after 8 simulations
MCTS Values:
a=0 : 6.22828 (4)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 5.688 (1)
a=0 o=0 a=2 : 8.3814 (2)
a=0 o=0 a=2 o=0 a=0 : 0 (0)
a=0 o=0 a=2 o=0 a=1 : 8.14506 (1)
a=0 o=0 a=2 o=0 a=2 : -1e+10 (1000000)
a=0 o=0 a=2 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=2 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=2 o=0 a=5 : -1e+10 (1000000)
a=0 o=0 a=3 : -1e+10 (1000000)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=1 : 7.59899 (4)
a=1 o=0 a=0 : 9.025 (1)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : -1e+10 (1000000)
a=1 o=0 a=3 : 5.98737 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . . #
# . * #
# # # # 
Reward 0
Matched 3 states
Created 1 local transformations out of 1 attempts
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=4 o=0 a=2 o=0 a=1 o=0 
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : 0 (0)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 10 (1)
a=2 : -1e+10 (1000000)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 8.57375 (1)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=4 o=0 a=2 o=0 a=1 o=0 a=0 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = 3.97214
MCTS Values:
a=0 : 6.27295 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : 4.1812 (1)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 7.35092 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Expanding node: a=2 o=0 a=5 o=1 a=0 o=0 a=5 o=1 a=4 o=0 a=2 o=0 a=1 o=0 a=3 o=0 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : 6.27295 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : 4.1812 (1)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -1e+10 (1000000)
a=3 : 7.35092 (2)
a=3 o=0 a=0 : 7.73781 (1)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : -1e+10 (1000000)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 10
MCTS Values:
a=0 : 6.27295 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : 4.1812 (1)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : -1e+10 (1000000)
a=3 : 7.35092 (2)
a=3 o=0 a=0 : 7.73781 (1)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : -1e+10 (1000000)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.13342
MCTS Values:
a=0 : 6.27295 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : 4.1812 (1)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : -1e+10 (1000000)
a=3 : 6.61175 (3)
a=3 o=0 a=0 : 7.73781 (1)
a=3 o=0 a=1 : 5.4036 (1)
a=3 o=0 a=2 : -1e+10 (1000000)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0.375 [0, 1]
Rollout depth: 8.6 [3, 17]
Total reward: 7.79764 [3.97214, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : 6.27295 (2)
a=0 o=0 a=0 : -1e+10 (1000000)
a=0 o=0 a=1 : 0 (0)
a=0 o=0 a=2 : 0 (0)
a=0 o=0 a=3 : 4.1812 (1)
a=0 o=0 a=4 : -1e+10 (1000000)
a=0 o=0 a=5 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : -1e+10 (1000000)
a=3 : 6.61175 (3)
a=3 o=0 a=0 : 7.73781 (1)
a=3 o=0 a=1 : 5.4036 (1)
a=3 o=0 a=2 : -1e+10 (1000000)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . . #
# . * #
# # # # 
Reward 10
Terminated
Discounted return = 15.1284, average = 14.5511
Undiscounted return = 20, average = 16.6667
Starting run 4 with 8 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward 10.8214
Total reward = 10.2804
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 10.2804 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = -2.26219
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 10.2804 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 14 steps, with total reward -0.55458
Total reward = -0.526851
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -0.526851 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 10.2804 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : -0.526851 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 10.2804 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 9 steps, with total reward -2.3908
Total reward = -2.15769
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : -0.526851 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 4.06133 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -2.27126 (1)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.3814 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 8.14506 (1)
a=2 : -0.526851 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 4.06133 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -2.27126 (1)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward -0.975
Total reward = -0.879937
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.29429 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : -0.92625 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 8.14506 (1)
a=2 : -0.526851 (1)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 4.06133 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -2.27126 (1)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -1.67408
Total reward = -1.51086
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.29429 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : -0.92625 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 8.14506 (1)
a=2 : -1.01885 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -1.59038 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 4.06133 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -2.27126 (1)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
Tree depth: 0.5 [0, 1]
Rollout depth: 7.25 [2, 14]
Total reward: 2.4632 [-2.26219, 10.2804]
Policy after 8 simulations
MCTS Policy:
a=1 : 5.29429 (3)
a=1 o=0 a=5 : 8.14506 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 5.29429 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : -0.92625 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 8.14506 (1)
a=2 : -1.01885 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -1.59038 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -2.26219 (1)
a=5 : 4.06133 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : -2.27126 (1)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 7 steps, with total reward -1.22283
Total reward = -1.16169
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -1.16169 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.35092 (1)
a=3 : -1.16169 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 7.35092 (1)
a=3 : -1.16169 (1)
a=4 : -1e+10 (1000000)
a=5 : 9.5 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=1 o=0 a=5 o=1 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 7.35092 (1)
a=3 : -1.16169 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.6189 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : 8.14506 (1)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Expanding node: a=1 o=0 a=2 o=0 
Adding sample:

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward -1.11062
Total reward = -1.00233
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 3.17429 (2)
a=2 o=0 a=0 : -1.05509 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1.16169 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.6189 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : 8.14506 (1)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 3.17429 (2)
a=2 o=0 a=0 : -1.05509 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1.16169 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.6189 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : 8.14506 (1)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 6.6 [1, 15]
Total reward: 6.55309 [-1.16169, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 3.17429 (2)
a=2 o=0 a=0 : -1.05509 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1.16169 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.6189 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : 8.14506 (1)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.5, average = 13.2884
Undiscounted return = 10, average = 15
Starting run 5 with 8 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward 4.1812
Total reward = 13.9721
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : 13.9721 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : 13.9721 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward -2.8658
Total reward = -2.72251
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : 13.9721 (1)
a=5 : -2.72251 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = -2.26219
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.85498 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 8.14506 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : -2.72251 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 4.87675 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.85498 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 8.14506 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : -2.72251 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward -2.04163
Total reward = -1.84257
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 1.51709 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1.93955 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.85498 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 8.14506 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : -2.72251 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 27 steps, with total reward 2.6352
Total reward = 2.37827
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.96776 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 2.50344 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 1.51709 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1.93955 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.85498 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 8.14506 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : -2.72251 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 10.25 [1, 27]
Total reward: 4.11561 [-2.72251, 13.9721]
Policy after 8 simulations
MCTS Policy:
a=1 : 6.96776 (3)
a=1 o=0 a=2 : 9.5 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 6.96776 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 2.50344 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 1.51709 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1.93955 (1)
a=3 : -1e+10 (1000000)
a=4 : 5.85498 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 8.14506 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : -2.72251 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.87675 (1)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 4.87675 (1)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 4.87675 (1)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 4.87675 (1)
a=3 : 8.57375 (1)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=1 o=0 a=3 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward -0.835941
Total reward = -0.754436
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 4.87675 (1)
a=3 : 3.90966 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : -0.794144 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.57375 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=1 o=0 a=5 o=1 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 4.87675 (1)
a=3 : 3.90966 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : -0.794144 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.79938 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 9.5 (1)
a=5 o=1 a=3 : 0 (0)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Expanding node: a=1 o=0 a=2 o=0 
Adding sample:

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 7.18837 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : 3.90966 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : -0.794144 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.79938 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 9.5 (1)
a=5 o=1 a=3 : 0 (0)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
Tree depth: 0.375 [0, 1]
Rollout depth: 5.4 [1, 14]
Total reward: 7.47435 [-0.754436, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 7.18837 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : 3.90966 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : -0.794144 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 8.79938 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 9.5 (1)
a=5 o=1 a=3 : 0 (0)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.5, average = 12.5307
Undiscounted return = 10, average = 14
Starting run 6 with 8 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = -0.975
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 9.025 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 9.025 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward -1.75044
Total reward = -1.57977
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 6.6342 (1)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 3.72261 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : -1.66292 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 7.18601 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 3.72261 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : -1.66292 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.75427 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 8.14506 (1)
a=2 : 7.18601 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 3.72261 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : -1.66292 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
Tree depth: 0.5 [0, 1]
Rollout depth: 4.125 [1, 11]
Total reward: 5.88813 [-1.57977, 9.5]
Policy after 8 simulations
MCTS Policy:
a=1 : 8.75427 (3)
a=1 o=0 a=2 : 9.5 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.75427 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 8.14506 (1)
a=2 : 7.18601 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 8.14506 (1)
a=3 : -1e+10 (1000000)
a=4 : -0.975 (1)
a=5 : 3.72261 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : -1.66292 (1)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 0 (0)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward -1.04842
Total reward = -0.996004
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -0.996004 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 12 steps, with total reward 5.688
Total reward = 5.4036
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : 5.4036 (1)
a=4 : -1e+10 (1000000)
a=5 : -0.996004 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 5.4036 (1)
a=4 : -1e+10 (1000000)
a=5 : -0.996004 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 5.4036 (1)
a=4 : -1e+10 (1000000)
a=5 : -0.996004 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 5.4036 (1)
a=4 : -1e+10 (1000000)
a=5 : -0.996004 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Expanding node: a=1 o=0 a=2 o=0 
Adding sample:

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : 5.4036 (1)
a=4 : -1e+10 (1000000)
a=5 : -0.996004 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=1 o=0 a=3 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 19.025
Total reward = 17.1701
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : 11.2868 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 18.0737 (1)
a=4 : -1e+10 (1000000)
a=5 : -0.996004 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 18 steps, with total reward -2.12129
Total reward = -1.91446
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : 6.8864 (3)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : -2.01523 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 18.0737 (1)
a=4 : -1e+10 (1000000)
a=5 : -0.996004 (1)
Tree depth: 0.375 [0, 1]
Rollout depth: 8.8 [1, 18]
Total reward: 7.3329 [-1.91446, 17.1701]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : 6.8864 (3)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 0 (0)
a=3 o=0 a=2 : -2.01523 (1)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 18.0737 (1)
a=4 : -1e+10 (1000000)
a=5 : -0.996004 (1)
E

# # # # 
# 0$* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.5, average = 12.0256
Undiscounted return = 10, average = 13.3333
Starting run 7 with 8 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 9.025 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 15 steps, with total reward -2.10662
Total reward = -2.00129
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : -2.00129 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 9.025 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 18.1451
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : -2.00129 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.1451 (1)
a=5 : 9.025 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : -2.00129 (1)
a=3 : -1e+10 (1000000)
a=4 : 18.585 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.5 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 9.025 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 14 steps, with total reward 5.13342
Total reward = -5.36709
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : -2.00129 (1)
a=3 : -1e+10 (1000000)
a=4 : 10.601 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.5 (1)
a=4 o=0 a=2 : 4.87675 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 9.025 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 15 steps, with total reward -1.11062
Total reward = -1.00233
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.24883 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1.05509 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : -2.00129 (1)
a=3 : -1e+10 (1000000)
a=4 : 10.601 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.5 (1)
a=4 o=0 a=2 : 4.87675 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 9.025 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -2.64908
Total reward = -2.3908
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.24883 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1.05509 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : -2.00129 (1)
a=3 : -1e+10 (1000000)
a=4 : 10.601 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.5 (1)
a=4 o=0 a=2 : 4.87675 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 3.3171 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : -2.51663 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 7.375 [1, 15]
Total reward: 5.61669 [-5.36709, 19.025]
Policy after 8 simulations
MCTS Policy:
a=4 : 10.601 (3)
a=4 o=0 a=1 : 9.5 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.24883 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : -1.05509 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : -2.00129 (1)
a=3 : -1e+10 (1000000)
a=4 : 10.601 (3)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 9.5 (1)
a=4 o=0 a=2 : 4.87675 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 3.3171 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : -2.51663 (1)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward 10
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 11 steps, with total reward 5.98737
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 5.688 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.5 (1)
a=2 : 5.688 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 5.688 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.2625 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 7.13088 (2)
a=2 o=0 a=0 : 9.025 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.03292 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 7.13088 (2)
a=2 o=0 a=0 : 9.025 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 5.98737
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.03292 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 6.74971 (3)
a=2 o=0 a=0 : 9.025 (1)
a=2 o=0 a=1 : 6.30249 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.14969 (4)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 6.74971 (3)
a=2 o=0 a=0 : 9.025 (1)
a=2 o=0 a=1 : 6.30249 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.21975 (5)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (2)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 6.74971 (3)
a=2 o=0 a=0 : 9.025 (1)
a=2 o=0 a=1 : 6.30249 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0.75 [0, 1]
Rollout depth: 4.33333 [1, 11]
Total reward: 8.29348 [5.688, 9.5]
Policy after 8 simulations
MCTS Policy:
a=1 : 9.21975 (5)
a=1 o=0 a=1 : 10 (2)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.21975 (5)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (2)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 6.74971 (3)
a=2 o=0 a=0 : 9.025 (1)
a=2 o=0 a=1 : 6.30249 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 4 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 9.5 (1)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 9.5 (1)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.5 (1)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.03688 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 9.025 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=3 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.03688 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 9.025 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : 8.3814 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 9.5 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 9.03688 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 9.025 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : 8.3814 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 9.5 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.7396 (3)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 9.025 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : 8.3814 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 9.5 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0.375 [0, 1]
Rollout depth: 2.4 [1, 5]
Total reward: 9.1227 [7.73781, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.7396 (3)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 9.025 (1)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : 8.3814 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 9.5 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 19.025, average = 13.0255
Undiscounted return = 20, average = 14.2857
Starting run 8 with 8 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 11 steps, with total reward 14.5611
Total reward = 13.8331
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.8331 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 15.9874
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.8331 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 15.9874 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 46 steps, with total reward 8.73221
Total reward = 8.2956
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.8331 (1)
a=2 : 8.2956 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.9874 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 18.5737
Total reward = 17.6451
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.8331 (1)
a=2 : 8.2956 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.9874 (1)
a=5 : 17.6451 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=5 o=1 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 10 steps, with total reward 6.30249
Total reward = 5.688
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.8331 (1)
a=2 : 8.2956 (1)
a=3 : -1e+10 (1000000)
a=4 : 15.9874 (1)
a=5 : 11.6665 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 5.98737 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 16.9834
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 13.8331 (1)
a=2 : 8.2956 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.4854 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 7.35092 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 11.6665 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 5.98737 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.592 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.73781 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 8.2956 (1)
a=3 : -1e+10 (1000000)
a=4 : 16.4854 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 7.35092 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 11.6665 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 5.98737 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.592 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.73781 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 8.6603 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.4854 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 7.35092 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 11.6665 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 5.98737 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 11.625 [1, 46]
Total reward: 11.851 [5.688, 17.6451]
Policy after 8 simulations
MCTS Policy:
a=4 : 16.4854 (2)
a=4 o=0 a=1 : 7.35092 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10.592 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.73781 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 8.6603 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 16.4854 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 7.35092 (1)
a=4 o=0 a=2 : 0 (0)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 11.6665 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 0 (0)
a=5 o=1 a=5 : 5.98737 (1)
Sample

# # # # 
# * . #
# . . #
# # # # 
Reward 10
Matched 1 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 5.13342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.14506 (1)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.22378 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.6342 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 5.13342 (1)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.22378 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.6342 (1)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 7.07921 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.35092
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.26616 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.6342 (1)
a=1 o=0 a=3 : 7.73781 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 7.07921 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.26616 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.6342 (1)
a=1 o=0 a=3 : 7.73781 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 7.29874 (3)
a=2 o=0 a=0 : 8.14506 (1)
a=2 o=0 a=1 : 9.5 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Adding sample:

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Expanding node: a=4 o=0 a=2 o=0 a=1 o=0 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.26616 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 6.6342 (1)
a=1 o=0 a=3 : 7.73781 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 7.13261 (4)
a=2 o=0 a=0 : 8.14506 (1)
a=2 o=0 a=1 : 8.24169 (2)
a=2 o=0 a=1 o=0 a=0 : 0 (0)
a=2 o=0 a=1 o=0 a=1 : 0 (0)
a=2 o=0 a=1 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=3 : 7.35092 (1)
a=2 o=0 a=1 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Adding sample:

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.82462 (4)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 6.6342 (1)
a=1 o=0 a=3 : 7.73781 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 7.13261 (4)
a=2 o=0 a=0 : 8.14506 (1)
a=2 o=0 a=1 : 8.24169 (2)
a=2 o=0 a=1 o=0 a=0 : 0 (0)
a=2 o=0 a=1 o=0 a=1 : 0 (0)
a=2 o=0 a=1 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=3 : 7.35092 (1)
a=2 o=0 a=1 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0.875 [0, 2]
Rollout depth: 5.85714 [1, 13]
Total reward: 7.47861 [5.13342, 9.5]
Policy after 8 simulations
MCTS Policy:
a=1 : 7.82462 (4)
a=1 o=0 a=1 : 10 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 7.82462 (4)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 10 (1)
a=1 o=0 a=2 : 6.6342 (1)
a=1 o=0 a=3 : 7.73781 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : -1e+10 (1000000)
a=2 : 7.13261 (4)
a=2 o=0 a=0 : 8.14506 (1)
a=2 o=0 a=1 : 8.24169 (2)
a=2 o=0 a=1 o=0 a=0 : 0 (0)
a=2 o=0 a=1 o=0 a=1 : 0 (0)
a=2 o=0 a=1 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=3 : 7.35092 (1)
a=2 o=0 a=1 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=1 o=0 a=5 : -1e+10 (1000000)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 0
Matched 3 states
Created 1 local transformations out of 1 attempts
Expanding node: a=4 o=0 a=1 o=0 
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.57375 (1)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 8.57375 (1)
a=3 : 6.6342 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 8.57375 (1)
a=3 : 6.6342 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=2 o=0 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.03688 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : 6.6342 (1)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=4 o=0 a=1 o=0 a=3 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 9.03688 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : 7.8296 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 9.5 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 9.03688 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : 7.8296 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 9.5 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Starting simulation

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
Adding sample:

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.7396 (3)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : 7.8296 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 9.5 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
Tree depth: 0.375 [0, 1]
Rollout depth: 3.75 [1, 8]
Total reward: 8.98475 [6.6342, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 8.7396 (3)
a=2 o=0 a=0 : 8.57375 (1)
a=2 o=0 a=1 : 10 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -1e+10 (1000000)
a=3 : 7.8296 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 9.5 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : -1e+10 (1000000)
a=3 o=0 a=5 : -1e+10 (1000000)
a=4 : -1e+10 (1000000)
a=5 : -1e+10 (1000000)
E

# # # # 
# . * #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 19.025, average = 13.7754
Undiscounted return = 20, average = 15
Starting run 9 with 8 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = -3.3658
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -3.3658 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -3.3658 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 16 steps, with total reward 4.63291
Total reward = 4.40127
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.3658 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.3658 (1)
a=5 : 9.025 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.58503 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.3658 (1)
a=5 : 9.025 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=5 o=2 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.58503 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 4.40127 (1)
a=3 : -1e+10 (1000000)
a=4 : -3.3658 (1)
a=5 : 8.00419 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 7.35092 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 15 steps, with total reward -3.697
Total reward = -3.33654
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.58503 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 0.532362 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -3.51215 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -3.3658 (1)
a=5 : 8.00419 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 7.35092 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 1 steps, with total reward 10
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.73169 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 0.532362 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -3.51215 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -3.3658 (1)
a=5 : 8.00419 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 7.35092 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 6.625 [1, 16]
Total reward: 4.9878 [-3.3658, 9.025]
Policy after 8 simulations
MCTS Policy:
a=1 : 8.73169 (3)
a=1 o=0 a=2 : 9.5 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.73169 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 9.5 (1)
a=1 o=0 a=3 : 8.57375 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 0 (0)
a=2 : 0.532362 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : -3.51215 (1)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : -3.3658 (1)
a=5 : 8.00419 (2)
a=5 o=2 a=0 : -1e+10 (1000000)
a=5 o=2 a=1 : 0 (0)
a=5 o=2 a=2 : 0 (0)
a=5 o=2 a=3 : -1e+10 (1000000)
a=5 o=2 a=4 : 0 (0)
a=5 o=2 a=5 : 7.35092 (1)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
Matched 2 states
Created 1 local transformations out of 1 attempts
Expanding node: a=1 o=0 
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 6.30249
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 6.30249 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 24 steps, with total reward -4.27735
Total reward = -4.06348
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : -4.06348 (1)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 6.30249 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -4.06348 (1)
a=3 : 7.73781 (1)
a=4 : -1e+10 (1000000)
a=5 : 6.30249 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=1 o=0 a=3 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 11.1792
Total reward = 10.0893
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -4.06348 (1)
a=3 : 8.91354 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 10.6203 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 6.30249 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=1 o=0 a=5 o=1 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Total reward = 9.5
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : -4.06348 (1)
a=3 : 8.91354 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 10.6203 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 7.90125 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 10 (1)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : 0 (0)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : -4.06348 (1)
a=3 : 8.91354 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 10.6203 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 7.90125 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 10 (1)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : 0 (0)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 13.25 [5, 24]
Total reward: 7.44576 [-4.06348, 10.0893]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : -4.06348 (1)
a=3 : 8.91354 (2)
a=3 o=0 a=0 : -1e+10 (1000000)
a=3 o=0 a=1 : 10.6203 (1)
a=3 o=0 a=2 : 0 (0)
a=3 o=0 a=3 : -1e+10 (1000000)
a=3 o=0 a=4 : 0 (0)
a=3 o=0 a=5 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 7.90125 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 10 (1)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : 0 (0)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.5, average = 13.3004
Undiscounted return = 10, average = 14.4444
Starting run 10 with 8 simulations... 
Expanding node: 

# # # # 
# * . #
# . . #
# # # # 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward 18.5737
Total reward = 17.6451
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 17.6451 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 17.6451 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 15 steps, with total reward 4.87675
Total reward = 4.63291
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.63291 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 17.6451 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 19.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.63291 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 19.025 (1)
a=5 : 17.6451 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=4 o=0 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = -3.01663
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.63291 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 8.00419 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 7.35092 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 17.6451 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Expanding node: a=5 o=1 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward 10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 9 steps, with total reward 6.6342
Total reward = 15.4874
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.63291 (1)
a=2 : 8.57375 (1)
a=3 : -1e+10 (1000000)
a=4 : 8.00419 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 7.35092 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 16.5662 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 16.3025 (1)
a=5 o=1 a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Expanding node: a=2 o=0 
Adding sample:

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 18 steps, with total reward -5.8188
Total reward = -5.25146
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.63291 (1)
a=2 : 1.66114 (2)
a=2 o=0 a=0 : -5.52786 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 8.00419 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 7.35092 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 16.5662 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 0 (0)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 16.3025 (1)
a=5 o=1 a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Adding sample:

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.63291 (1)
a=2 : 1.66114 (2)
a=2 o=0 a=0 : -5.52786 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 8.00419 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 7.35092 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 13.3719 (3)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 7.35092 (1)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 16.3025 (1)
a=5 o=1 a=5 : 0 (0)
Tree depth: 0.5 [0, 1]
Rollout depth: 7.875 [2, 18]
Total reward: 8.00992 [-5.25146, 19.025]
Policy after 8 simulations
MCTS Policy:
a=5 : 13.3719 (3)
a=5 o=1 a=4 : 16.3025 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 4.63291 (1)
a=2 : 1.66114 (2)
a=2 o=0 a=0 : -5.52786 (1)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 0 (0)
a=3 : -1e+10 (1000000)
a=4 : 8.00419 (2)
a=4 o=0 a=0 : -1e+10 (1000000)
a=4 o=0 a=1 : 0 (0)
a=4 o=0 a=2 : 7.35092 (1)
a=4 o=0 a=3 : -1e+10 (1000000)
a=4 o=0 a=4 : -1e+10 (1000000)
a=4 o=0 a=5 : -1e+10 (1000000)
a=5 : 13.3719 (3)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 7.35092 (1)
a=5 o=1 a=3 : -1e+10 (1000000)
a=5 o=1 a=4 : 16.3025 (1)
a=5 o=1 a=5 : 0 (0)
Check 1

# # # # 
# * . #
# . . #
# # # # 
Observed bad
Reward 0
No matching node found
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 7 steps, with total reward -1.67408
Total reward = -1.59038
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : 0 (0)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 17 steps, with total reward 4.40127
Total reward = -5.8188
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 0 (0)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 9.025
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : 0 (0)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
N
Reward 0

# # # # 
# . * #
# . . #
# # # # 
E
Reward 10

# # # # 
# . * #
# . . #
# # # # 
Ending rollout after 19 steps, with total reward -6.02786
Total reward = -5.72646
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 9.025 (1)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : -5.72646 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Expanding node: a=5 o=2 a=1 o=0 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 3 steps, with total reward 9.025
Total reward = 8.14506
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.58503 (2)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 0 (0)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 8.57375 (1)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : -5.72646 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Ending rollout after 2 steps, with total reward 9.5
Total reward = 8.57375
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.58127 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 8.57375 (1)
a=2 : -1.59038 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : -5.72646 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Expanding node: a=5 o=2 a=2 o=0 
Adding sample:

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Starting rollout
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# . * #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# . * #
# # # # 
W
Reward 0

# # # # 
# . . #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 12 steps, with total reward -3.337
Total reward = -3.01164
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.58127 (3)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 0 (0)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 8.57375 (1)
a=2 : -2.30101 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -3.17015 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : -5.72646 (1)
Starting simulation

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
Adding sample:

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
Check 1
Observed bad
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 6 steps, with total reward 7.73781
Total reward = 6.98337
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.1818 (4)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.35092 (1)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 8.57375 (1)
a=2 : -2.30101 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -3.17015 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : -5.72646 (1)
Tree depth: 0.5 [0, 1]
Rollout depth: 8.5 [2, 19]
Total reward: 2.07249 [-5.8188, 9.025]
Policy after 8 simulations
MCTS Policy:
a=1 : 8.1818 (4)
a=1 o=0 a=3 : 9.025 (1)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 8.1818 (4)
a=1 o=0 a=0 : -1e+10 (1000000)
a=1 o=0 a=1 : 0 (0)
a=1 o=0 a=2 : 7.35092 (1)
a=1 o=0 a=3 : 9.025 (1)
a=1 o=0 a=4 : -1e+10 (1000000)
a=1 o=0 a=5 : 8.57375 (1)
a=2 : -2.30101 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : -1e+10 (1000000)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : -3.17015 (1)
a=3 : -1e+10 (1000000)
a=4 : -5.8188 (1)
a=5 : -5.72646 (1)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 0
Matched 3 states
Created 1 local transformations out of 1 attempts
Expanding node: a=5 o=2 a=1 o=0 
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 0 (0)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Starting rollout
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 8 steps, with total reward 6.98337
Total reward = 6.6342
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : 0 (0)
a=4 : -1e+10 (1000000)
a=5 : 6.6342 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
Starting rollout
Sample
Reward -10

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# . . #
# * . #
# # # # 
E
Reward 0

# # # # 
# . . #
# . * #
# # # # 
E
Reward 10

# # # # 
# . . #
# . * #
# # # # 
Ending rollout after 4 steps, with total reward -1.42625
Total reward = -1.35494
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 0 (0)
a=3 : -1.35494 (1)
a=4 : -1e+10 (1000000)
a=5 : 6.6342 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0X* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
W
Reward 0

# # # # 
# 0X. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0X. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0X. #
# . * #
# # # # 
Ending rollout after 5 steps, with total reward 8.14506
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (1)
a=2 : 7.73781 (1)
a=3 : -1.35494 (1)
a=4 : -1e+10 (1000000)
a=5 : 6.6342 (1)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 7.73781 (1)
a=3 : -1.35494 (1)
a=4 : -1e+10 (1000000)
a=5 : 6.6342 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Expanding node: a=5 o=2 a=1 o=0 a=2 o=0 
Adding sample:

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
E
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
W
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
N
Reward 0

# # # # 
# * . #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# * . #
# # # # 
E
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
E
Reward 10

# # # # 
# 0$. #
# . * #
# # # # 
Ending rollout after 13 steps, with total reward 5.4036
Total reward = 4.87675
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 6.30728 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 5.13342 (1)
a=3 : -1.35494 (1)
a=4 : -1e+10 (1000000)
a=5 : 6.6342 (1)
Starting simulation

# # # # 
# 0$* #
# . . #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
Expanding node: a=5 o=2 a=1 o=0 a=5 o=1 
Adding sample:

# # # # 
# 0$* #
# . . #
# # # # 
S
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Starting rollout
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
Check 1
Observed good
Reward 0

# # # # 
# 0$. #
# . * #
# # # # 
N
Reward 0

# # # # 
# 0$* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0$* #
# . . #
# # # # 
Ending rollout after 4 steps, with total reward 8.57375
Total reward = 7.73781
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (2)
a=2 : 6.30728 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 5.13342 (1)
a=3 : -1.35494 (1)
a=4 : -1e+10 (1000000)
a=5 : 7.18601 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 8.14506 (1)
a=5 o=1 a=3 : 0 (0)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
Starting simulation

# # # # 
# 0X* #
# . . #
# # # # 
E
Reward 10

# # # # 
# 0X* #
# . . #
# # # # 
Total reward = 10
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 6.30728 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 5.13342 (1)
a=3 : -1.35494 (1)
a=4 : -1e+10 (1000000)
a=5 : 7.18601 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 8.14506 (1)
a=5 o=1 a=3 : 0 (0)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
Tree depth: 0.25 [0, 1]
Rollout depth: 6.8 [4, 13]
Total reward: 6.95395 [-1.35494, 10]
Policy after 8 simulations
MCTS Policy:
a=1 : 10 (3)
Values after 8 simulations
MCTS Values:
a=0 : -1e+10 (1000000)
a=1 : 10 (3)
a=2 : 6.30728 (2)
a=2 o=0 a=0 : 0 (0)
a=2 o=0 a=1 : 0 (0)
a=2 o=0 a=2 : -1e+10 (1000000)
a=2 o=0 a=3 : 0 (0)
a=2 o=0 a=4 : -1e+10 (1000000)
a=2 o=0 a=5 : 5.13342 (1)
a=3 : -1.35494 (1)
a=4 : -1e+10 (1000000)
a=5 : 7.18601 (2)
a=5 o=1 a=0 : -1e+10 (1000000)
a=5 o=1 a=1 : 0 (0)
a=5 o=1 a=2 : 8.14506 (1)
a=5 o=1 a=3 : 0 (0)
a=5 o=1 a=4 : -1e+10 (1000000)
a=5 o=1 a=5 : 0 (0)
E

# # # # 
# 0X* #
# . . #
# # # # 
Reward 10
Terminated
Discounted return = 9.025, average = 12.8728
Undiscounted return = 10, average = 14
Simulations = 8
Runs = 10
Undiscounted return = 14 +- 1.54919
Discounted return = 12.8728 +- 1.37957
Time = 0.0043025
